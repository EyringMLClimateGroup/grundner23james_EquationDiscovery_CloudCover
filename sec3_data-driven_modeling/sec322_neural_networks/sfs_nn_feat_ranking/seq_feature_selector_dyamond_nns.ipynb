{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c55372-d418-4307-8e49-31662bf7c54c",
   "metadata": {},
   "source": [
    "We use mlxtend here becaue it does not need a scikit-learn estimator as its argument.\n",
    "\n",
    "Source: https://github.com/rasbt/mlxtend/discussions/777 <br>\n",
    "See also: http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/\n",
    "\n",
    "There's no early stopping implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52d18388-f366-47d0-b747-b17736292f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150GB could be enough\n",
    "# Won't run on a Levante GPU node\n",
    "\n",
    "# Executed via /home/b/b309170/scripts/run_sfs_nns.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f4430f7-25de-4de0-b701-715a69e0e661",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-924eb7f50ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequentialFeatureSelector\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSFS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_sequential_feature_selection\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplot_sfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/python/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/python/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtraining_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork_serialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss_scale_optimizer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlso\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mscipy_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0mpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m from pandas.core.api import (\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;31m# dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mInt8Dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/pandas/core/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringDtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedAgg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m from pandas.core.indexes.api import (\n\u001b[1;32m     31\u001b[0m     \u001b[0mCategoricalIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/pandas/core/groupby/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedAgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroupBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ = [\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_shared_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m from pandas.core.groupby.groupby import (\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mGroupBy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0m_apply_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import mlxtend\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow import nn \n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, '/home/b/b309170' + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "\n",
    "# Reloading custom file to incorporate changes dynamically\n",
    "import my_classes\n",
    "from my_classes import read_mean_and_std\n",
    "\n",
    "seed = int(sys.argv[1]) # [10, 20, ...]\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "matplotlib.use('PDF')\n",
    "# hour_min = '%d_%d'%(datetime.datetime.now().hour, datetime.datetime.now().minute)\n",
    "hour_min = str(seed) # Just use the seed instead..!\n",
    "\n",
    "# output_var = sys.argv[1] # 'cl_volume' or 'cl_area'\n",
    "# subset_size = int(sys.argv[2]) # Tried 100000 and 150000\n",
    "\n",
    "# floating_bool = bool(int(sys.argv[3]))\n",
    "\n",
    "# split_by_cloud_regime = bool(int(sys.argv[4]))\n",
    "# # Is only relevant if split_by_cloud_regime\n",
    "# regime = int(sys.argv[5])\n",
    "\n",
    "output_var = 'cl_area' # 'cl_volume' or 'cl_area'\n",
    "subset_size = 150000 # Tried 100000 and 150000\n",
    "\n",
    "floating_bool = False\n",
    "# Only remove condensate-free cells\n",
    "no_condensate_free_cells = bool(int(sys.argv[2]))\n",
    "\n",
    "split_by_cloud_regime = False\n",
    "# Is only relevant if split_by_cloud_regime\n",
    "regime = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc51ddc-e4c3-4e41-875c-32b040fd032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25 \n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325c1479-fd9f-44e4-af90-6525768da069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "output_path = '/home/b/b309170/workspace_icon-ml/symbolic_regression/finding_symmetries/seq_feature_selector_dyamond_nns'    \n",
    "folder_data = '/home/b/b309170/my_work/icon-ml_data/cloud_cover_parameterization/neighborhood_based_SR_DYAMOND/'\n",
    "\n",
    "input_data = np.load(os.path.join(folder_data, 'cloud_cover_input_dyamond.npy'))\n",
    "if output_var == 'cl_volume':\n",
    "    output_data = np.load(os.path.join(folder_data, 'cloud_cover_output_dyamond.npy'))\n",
    "elif output_var == 'cl_area':\n",
    "    output_data = np.load(os.path.join(folder_data, 'cloud_area_output_dyamond.npy'))\n",
    "\n",
    "features = ['hus', 'clw', 'cli', 'ta', 'pa', 'zg', 'fr_land', 'U', 'rh', 'ps', 'hus_z', 'hus_zz', 'clw_z', 'clw_zz', 'cli_z',\\\n",
    "            'cli_zz', 'ta_z', 'ta_zz', 'pa_z', 'pa_zz', 'U_z', 'U_zz', 'rh_z', 'rh_zz']\n",
    "no_features = len(features)\n",
    "\n",
    "samples_total, no_of_features = input_data.shape\n",
    "\n",
    "# Split into train/valid\n",
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])\n",
    "\n",
    "input_train = input_data[training_folds[1]]\n",
    "input_valid = input_data[validation_folds[1]]\n",
    "output_train = output_data[training_folds[1]]\n",
    "output_valid = output_data[validation_folds[1]]\n",
    "\n",
    "# Remove input_data, output_data\n",
    "del input_data, output_data, training_folds, validation_folds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97063ee-c9cd-42e9-bb22-635b9ed42522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To locate variables\n",
    "loc = {}\n",
    "for i in range(len(features)):\n",
    "    loc[features[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38aec786-7b6b-43ea-9f89-ff927cc118dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190119664, 24)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d7e856-8ce3-44ed-a292-5cfd5af296d8",
   "metadata": {},
   "source": [
    "**Split into cloud regimes**\n",
    "\n",
    "According to both:\n",
    "- a*q_i + q_c\n",
    "- air pressure\n",
    "\n",
    "--> There is no easy way to specify a, so I choose it to be equal to 1 (alternatively one could think about mean(a qi) = mean(qc)). Then I can interpret qi+qc as the condensate mixing ratio.\n",
    "\n",
    "So I have four regimes in total: <br>\n",
    "1) 0 < qi+qc < 1.6e-5 and p < 7.9e4 [High altitude, little condensate]\n",
    "2) 0 < qi+qc < 1.6e-5 and p > 7.9e4 [Low altitude, little condensate]\n",
    "3) qi+qc > 1.6e-5 and p < 7.9e4 [High altitude, high condensate]\n",
    "4) qi+qc > 1.6e-5 and p > 7.9e4 [Low altitude, high condensate]\n",
    "\n",
    "For $qi + qc = 0$ we simply set $C = 0$.\n",
    "\n",
    "In every regime there are more than 2.3e6 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91dc84f3-0c01-49a6-b348-e6c35cbf8b48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38864092, 24)\n",
      "(38864092,)\n",
      "(23611276, 24)\n",
      "(23611276,)\n",
      "(23611277, 24)\n",
      "(23611277,)\n",
      "(38864096, 24)\n",
      "(38864096,)\n"
     ]
    }
   ],
   "source": [
    "# We train SFS NNs per cloud regime if split_by_cloud_regime is True\n",
    "if split_by_cloud_regime:\n",
    "    a = 1\n",
    "\n",
    "    cod_subs = a*input_train[:, loc['cli']] + input_train[:, loc['clw']]\n",
    "    cod_subs_med = np.median(cod_subs[cod_subs != 0])\n",
    "\n",
    "    pa_med = np.median(input_train[cod_subs != 0, loc['pa']])\n",
    "\n",
    "    # For the training data\n",
    "    input_train_reg_1 = input_train[(0 < cod_subs) & (cod_subs < cod_subs_med) & (input_train[:, loc['pa']] < pa_med)]\n",
    "    input_train_reg_2 = input_train[(0 < cod_subs) & (cod_subs < cod_subs_med) & (input_train[:, loc['pa']] > pa_med)]\n",
    "    input_train_reg_3 = input_train[(cod_subs > cod_subs_med) & (input_train[:, loc['pa']] < pa_med)]\n",
    "    input_train_reg_4 = input_train[(cod_subs > cod_subs_med) & (input_train[:, loc['pa']] > pa_med)]\n",
    "\n",
    "    output_train_reg_1 = output_train[(0 < cod_subs) & (cod_subs < cod_subs_med) & (input_train[:, loc['pa']] < pa_med)]\n",
    "    output_train_reg_2 = output_train[(0 < cod_subs) & (cod_subs < cod_subs_med) & (input_train[:, loc['pa']] > pa_med)]\n",
    "    output_train_reg_3 = output_train[(cod_subs > cod_subs_med) & (input_train[:, loc['pa']] < pa_med)]\n",
    "    output_train_reg_4 = output_train[(cod_subs > cod_subs_med) & (input_train[:, loc['pa']] > pa_med)]\n",
    "    \n",
    "    # Do the regimes have a similar size?\n",
    "    for i in range(1, 5):\n",
    "        print(locals()['input_train_reg_%d'%i].shape)\n",
    "        print(locals()['output_train_reg_%d'%i].shape)\n",
    "        \n",
    "if no_condensate_free_cells:\n",
    "    cod_subs = input_train[:, loc['cli']] + input_train[:, loc['clw']]\n",
    "    input_train = input_train[(1e-20 < cod_subs)]\n",
    "    output_train = output_train[(1e-20 < cod_subs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544a076f-797e-4b90-b6b8-2a0d78f17a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19337328, 24)\n",
      "(19337328,)\n",
      "(11581298, 24)\n",
      "(11581298,)\n",
      "(11931635, 24)\n",
      "(11931635,)\n",
      "(19790540, 24)\n",
      "(19790540,)\n"
     ]
    }
   ],
   "source": [
    "# We train SFS NNs per cloud regime if split_by_cloud_regime is True\n",
    "if split_by_cloud_regime:\n",
    "    # Same for the validation data\n",
    "    cod_subs = a*input_valid[:, loc['cli']] + input_valid[:, loc['clw']]\n",
    "\n",
    "    input_valid_reg_1 = input_valid[(0 < cod_subs) & (cod_subs < cod_subs_med) & (input_valid[:, loc['pa']] < pa_med)]\n",
    "    input_valid_reg_2 = input_valid[(0 < cod_subs) & (cod_subs < cod_subs_med) & (input_valid[:, loc['pa']] > pa_med)]\n",
    "    input_valid_reg_3 = input_valid[(cod_subs > cod_subs_med) & (input_valid[:, loc['pa']] < pa_med)]\n",
    "    input_valid_reg_4 = input_valid[(cod_subs > cod_subs_med) & (input_valid[:, loc['pa']] > pa_med)]\n",
    "\n",
    "    output_valid_reg_1 = output_valid[(0 < cod_subs) & (cod_subs < cod_subs_med) & (input_valid[:, loc['pa']] < pa_med)]\n",
    "    output_valid_reg_2 = output_valid[(0 < cod_subs) & (cod_subs < cod_subs_med) & (input_valid[:, loc['pa']] > pa_med)]\n",
    "    output_valid_reg_3 = output_valid[(cod_subs > cod_subs_med) & (input_valid[:, loc['pa']] < pa_med)]\n",
    "    output_valid_reg_4 = output_valid[(cod_subs > cod_subs_med) & (input_valid[:, loc['pa']] > pa_med)]\n",
    "    \n",
    "    # Do the regimes have a similar size?\n",
    "    for i in range(1, 5):\n",
    "        print(locals()['input_valid_reg_%d'%i].shape)\n",
    "        print(locals()['output_valid_reg_%d'%i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9349121-64b4-43d7-be9c-07febd49268e",
   "metadata": {},
   "source": [
    "**Choose the appropriate regime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e7dd64-27a3-43e3-a186-186bb46001c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert string into variable name according to the right regime\n",
    "if split_by_cloud_regime:\n",
    "    input_train = locals()['input_train_reg_%d'%regime].copy()\n",
    "    input_valid = locals()['input_valid_reg_%d'%regime].copy()\n",
    "\n",
    "    output_train = locals()['output_train_reg_%d'%regime].copy()\n",
    "    output_valid = locals()['output_valid_reg_%d'%regime].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f13d3-8ebc-4428-99e5-458b70afa41f",
   "metadata": {},
   "source": [
    "**Normalize the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acd33603-28b3-4b2c-8b0a-f6a8c3ee8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second fold yields the best model\n",
    "# Normalize the data acc. to the mean and std associated with the training data\n",
    "mean, std = read_mean_and_std(os.path.join('/home/b/b309170/workspace_icon-ml/cloud_cover_parameterization/neighborhood_based_SR_DYAMOND/saved_models', \n",
    "                                           'cross_validation_neighborhood_based_sr_%s_fold_2.txt'%output_var))\n",
    "input_train = (input_train - mean)/std\n",
    "input_valid = (input_valid - mean)/std\n",
    "\n",
    "samples_total, no_of_features = input_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87d47e38-a14e-403a-be33-18b527339987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to work with a subset of the data\n",
    "subset = np.random.randint(0, input_train.shape[0], subset_size)\n",
    "\n",
    "input_train = input_train[subset]\n",
    "output_train = output_train[subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17df9d8a-bef1-4ed0-8bec-ff75a50d144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_train):\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # First hidden layer\n",
    "    model.add(Dense(units=64, activation='tanh', input_dim=input_train.shape[1], \n",
    "                    kernel_regularizer=l1_l2(l1=0.004749, l2=0.008732)))\n",
    "\n",
    "    # Second hidden layer\n",
    "    model.add(Dense(units=64, activation=nn.leaky_relu, kernel_regularizer=l1_l2(l1=0.004749, l2=0.008732)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Third hidden layer\n",
    "    model.add(Dense(units=64, activation='tanh', kernel_regularizer=l1_l2(l1=0.004749, l2=0.008732)))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='linear', kernel_regularizer=l1_l2(l1=0.004749, l2=0.008732)))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.000433, epsilon=0.1),\n",
    "        loss=tf.keras.losses.MeanSquaredError()\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ee45d7c-ddc1-4e48-9d63-9d4da123bc69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrap Keras nn and turn it into a scikit-learn estimator\n",
    "class MakeModel(object):\n",
    "    def __init__(self, X=None, y=None):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.model.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        skwrapped_model = KerasRegressor(build_fn=create_model,\n",
    "                                          input_train=X,\n",
    "                                          epochs=EPOCHS,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          verbose=0)\n",
    "        self.model = skwrapped_model\n",
    "        self.model.fit(X, y)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827614f-fb48-4814-82aa-33475ce2b0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   57.9s remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "# Could set floating to either True or False\n",
    "max_features = 10\n",
    "\n",
    "t0 = time.time()\n",
    "sffs = SFS(MakeModel(),\n",
    "           k_features=(1, max_features),\n",
    "           floating=floating_bool, # Adds a check whether it is better to remove a feature from a given subset\n",
    "           clone_estimator=False, # Set to False if the estimator doesn't implement scikit-learn's set_params and get_params methods\n",
    "           cv=0, # Required if clone_estimator=False\n",
    "           n_jobs=1, # Required if clone_estimator=False\n",
    "           verbose=2,\n",
    "           scoring='r2')\n",
    "\n",
    "# Apply SFS to identify best feature subset\n",
    "sffs = sffs.fit(input_train, output_train, custom_feature_names=features)\n",
    "\n",
    "required_time = time.time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a51dc-9df2-496a-b625-75c0c87967c2",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e69018-a3fa-4b5c-815a-164f53b91fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plot_sfs(sffs.get_metric_dict(), kind='std_dev')\n",
    "\n",
    "# plt.ylim([0.8, 1])\n",
    "plt.title('Sequential Forward Selection')\n",
    "plt.grid()\n",
    "plt.ylabel('$R^2$ score on training data')\n",
    "\n",
    "if split_by_cloud_regime:\n",
    "    plt.savefig('/home/b/b309170/workspace_icon-ml/symbolic_regression/finding_symmetries/seq_feature_selector_dyamond_nns/split_by_regime/seq_feat_selector_training_data_regime_%d_%s_%s.pdf'%(regime, output_var, hour_min))\n",
    "elif no_condensate_free_cells:\n",
    "    plt.savefig('/home/b/b309170/workspace_icon-ml/symbolic_regression/finding_symmetries/seq_feature_selector_dyamond_nns/no_condensate_free_cells/seq_feat_selector_training_data_%s_%s.pdf'%(output_var, hour_min))\n",
    "else:\n",
    "    plt.savefig('/home/b/b309170/workspace_icon-ml/symbolic_regression/finding_symmetries/seq_feature_selector_dyamond_nns/seq_feat_selector_training_data_%s_%s.pdf'%(output_var, hour_min))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f617af6-2214-4b91-9105-55f894ac8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {}\n",
    "for k in range(1, max_features + 1):\n",
    "    out_dict['features_%d'%k] = sffs.subsets_[k]['feature_names']\n",
    "    out_dict['r2_score_%d'%k] = sffs.subsets_[k]['avg_score']\n",
    "out_dict['Required time in minutes'] = required_time/60\n",
    "out_dict['Epochs'] = EPOCHS\n",
    "out_dict['Subset size'] = subset_size\n",
    "if floating_bool:\n",
    "    out_dict['Floating'] = 'True'\n",
    "\n",
    "if split_by_cloud_regime:\n",
    "    out_json_path = '/home/b/b309170/workspace_icon-ml/symbolic_regression/finding_symmetries/seq_feature_selector_dyamond_nns/split_by_regime/seq_feat_selector_training_data_regime_%d_%s_%s.json'%(regime, output_var, hour_min)    \n",
    "elif no_condensate_free_cells:\n",
    "    out_json_path = '/home/b/b309170/workspace_icon-ml/symbolic_regression/finding_symmetries/seq_feature_selector_dyamond_nns/no_condensate_free_cells/seq_feat_selector_training_data_%s_%s.json'%(output_var, hour_min)\n",
    "else:\n",
    "    out_json_path = '/home/b/b309170/workspace_icon-ml/symbolic_regression/finding_symmetries/seq_feature_selector_dyamond_nns/seq_feat_selector_training_data_%s_%s.json'%(output_var, hour_min) \n",
    "    \n",
    "with open(out_json_path, 'w') as file:\n",
    "    json.dump(out_dict, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds",
   "language": "python",
   "name": "clouds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07bc643c-098b-4d6d-9eac-5bb4a65da042",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "1. We read the data from the npy files\n",
    "2. We combine the QUBICC and NARVAL data\n",
    "4. Set up cross validation\n",
    "\n",
    "During cross-validation:\n",
    "\n",
    "1. We scale the data, convert to tf data\n",
    "2. Plot training progress, model biases \n",
    "3. Write losses and epochs into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e82aa1a1-85e5-4228-afdd-ac4d88658935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Ran with 800GB (750GB should also be fine)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "#Import sklearn before tensorflow (static Thread-local storage)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# For Leaky_ReLU:\n",
    "from tensorflow import nn \n",
    "\n",
    "t0 = time.time()\n",
    "path = '/home/b/b309170'\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, path + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "\n",
    "# Reloading custom file to incorporate changes dynamically\n",
    "import importlib\n",
    "import my_classes\n",
    "importlib.reload(my_classes)\n",
    "\n",
    "from my_classes import write_infofile\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import TimeOut\n",
    "\n",
    "# We always pick the second fold (fold = 1)\n",
    "fold = 1\n",
    "\n",
    "# Which one of the 10 models to train (no_features in [4, 7])\n",
    "no_features = int(sys.argv[1])\n",
    "\n",
    "# Batch normalization and third layer: Bool\n",
    "bn = bool(int(sys.argv[2]))\n",
    "third_layer = bool(int(sys.argv[3]))\n",
    "\n",
    "# Number of units per layer [16,32,64,128]\n",
    "no_units = int(sys.argv[4])\n",
    "\n",
    "# Minutes per fold\n",
    "timeout = 450 \n",
    "\n",
    "# Maximum amount of epochs for each model\n",
    "epochs = 30 \n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_visible_devices(gpus[3], 'GPU')\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c292f1c8-63ee-4135-b622-1fb26642632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Cover or Cloud Area?\n",
    "output_var = 'cl_area' # Set output_var to one of {'cl_volume', 'cl_area'} \n",
    "\n",
    "path_base = os.path.join(path, 'workspace_icon-ml/cloud_cover_parameterization/neighborhood_based_on_seq_feat_sel_DYAMOND')\n",
    "path_data = os.path.join(path, 'my_work/icon-ml_data/cloud_cover_parameterization/neighborhood_based_SR_DYAMOND')\n",
    "    \n",
    "path_model = os.path.join(path_base, 'saved_models/hyperparameter_tests')\n",
    "path_figures = os.path.join(path_base, 'figures/hyperparameter_tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb692c8a-23f4-4e97-9222-cc86b8e7f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't run on a CPU node\n",
    "try:\n",
    "    # Prevents crashes of the code\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "    # Allow the growth of memory Tensorflow allocates (limits memory usage overall)\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "528d4ec8-bb3e-4ed3-9801-18fbc6a40569",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205faad9-9e44-4026-917a-1633166cbd28",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afaa7b35-5044-4c91-a0fd-b69d47e0870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['hus', 'clw', 'cli', 'ta', 'pa', 'zg', 'fr_land', 'U', 'rh', 'ps', 'hus_z', 'hus_zz', 'clw_z', 'clw_zz', 'cli_z',\\\n",
    "            'cli_zz', 'ta_z', 'ta_zz', 'pa_z', 'pa_zz', 'U_z', 'U_zz', 'rh_z', 'rh_zz']\n",
    "\n",
    "loc = {}\n",
    "for i in range(len(features)):\n",
    "    loc[features[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6498650e-eddb-4a7e-92c3-f399c24a7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "# The 17_15 ran with the largest amount of training data\n",
    "with open('~/workspace_icon-ml/symbolic_regression/finding_symmetries/seq_feature_selector_dyamond_nns/\\\n",
    "seq_feat_selector_training_data_%s_17_15.json'%output_var, 'r') as file:\n",
    "    seq_results = json.load(file)\n",
    "selected_vars = seq_results['features_%d'%no_features]\n",
    "\n",
    "input_data = np.load(path_data + '/cloud_cover_input_dyamond.npy')\n",
    "input_data = np.concatenate([np.expand_dims(input_data[:, loc[sel_var]], axis=1) for sel_var in selected_vars], axis = 1)\n",
    "\n",
    "layers_data = np.load(path_data + '/samples_vertical_layers_dyamond.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53259751-047b-40ec-8628-52fe7ccfd7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285179494, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e785aeb5-5219-464b-801f-848041de6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output data\n",
    "if output_var == 'cl_volume':\n",
    "    output_data = np.load(path_data + '/cloud_cover_output_dyamond.npy')\n",
    "elif output_var == 'cl_area':\n",
    "    output_data = np.load(path_data + '/cloud_area_output_dyamond.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff9916d5-ec8a-4353-9de8-fb9a106fa122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285179494, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_total, _ = input_data.shape\n",
    "(samples_total, no_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd45c51-8c05-45b1-af0f-cf507b8ce7e8",
   "metadata": {},
   "source": [
    "*Temporal cross-validation*\n",
    "\n",
    "Split into 2-weeks increments (when working with 3 months of data). It's 25 day increments with 5 months of data. <br>\n",
    "1.: Validate on increments 1 and 4 <br>\n",
    "2.: Validate on increments 2 and 5 <br>\n",
    "3.: Validate on increments 3 and 6\n",
    "\n",
    "--> 2/3 training data, 1/3 validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a952cab-c1c2-4495-9977-eb70b3ea3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de4abb5-6f61-40df-a9ba-a44df3649e1f",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57ef55-187e-40e5-a825-4046efa6c23a",
   "metadata": {},
   "source": [
    "Activation function for the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87d15e72-19fa-492d-9f40-f75fc09ba412",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {}\n",
    "custom_objects['leaky_relu'] = nn.leaky_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a703d640-c922-487d-bc2f-0a941f05e0c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'neighborhood_based_sfs_%s_no_features_%s_%s_%s_%d.h5'%(output_var, no_features, bn, third_layer, no_units)\n",
    "\n",
    "model = load_model(os.path.join(path_model, model_name), custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a93128-a535-4b22-9319-b486c001d31c",
   "metadata": {},
   "source": [
    "#### The data will need to be scaled according to the training folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c01dbc-f499-46dd-a9dd-566db079cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4150a-8046-4dc5-826c-c7fa64ae4246",
   "metadata": {},
   "source": [
    "#### Useful functions to plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5313775-a958-40f1-b038-528180f91ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_clc_per_vertical_layer(model, input_data, output_data, layers_data, batch_size=2**20):\n",
    "    '''\n",
    "        Input: \n",
    "            model: neural network\n",
    "            input_data: Usually the validation data\n",
    "            output_data: The ground truth output\n",
    "            layers_data: Vector that tells us the vertical layer of a given sample\n",
    "            \n",
    "        Model prediction and the Ground Truth means per vertical layer\n",
    "    '''\n",
    "    # Predicted cloud cover means\n",
    "    # Curiously it works best if we use predict_on_batch on small subsets of the data instead of predict(..., batch_size=...) \n",
    "    for i in range(1 + input_data.shape[0]//batch_size):\n",
    "        if i == 0:\n",
    "            a = model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            a = np.concatenate((a, model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])), axis=0)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "    pred_adj = np.minimum(np.maximum(a, 0), 100) \n",
    "    \n",
    "    # Computing means with the help of layers_data\n",
    "    clc_pred_mean = []; clc_data_mean = [];\n",
    "    for i in range(5, 32):\n",
    "        ind = np.where(layers_data == i)\n",
    "        clc_data_mean.append(np.mean(output_data[ind], dtype=np.float64))\n",
    "        clc_pred_mean.append(np.mean(pred_adj[ind], dtype=np.float64))\n",
    "    \n",
    "    return clc_pred_mean, clc_data_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90fc4b-fdbe-41c9-9e26-8af18c1ad672",
   "metadata": {},
   "source": [
    "#### Evaluate the models on the data\n",
    "\n",
    "Add training and validation losses to the text files. <br>\n",
    "Print results per vertical layer (respective validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71afc196-d928-4e16-b655-a731af7f5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = [] ; valid_losses = [] ; valid_means = [] ; valid_model_predictions = [] ;\n",
    "narval_means = [] ; narval_model_predictions = [] ; qubicc_means = [] ; qubicc_model_predictions = [] ;\n",
    "qubicc_month_0 = [] ; qubicc_model_pred_month_0 = [] ; qubicc_month_1 = [] ; qubicc_model_pred_month_1 = [] ;\n",
    "qubicc_month_2 = [] ; qubicc_model_pred_month_2 = [] ;\n",
    "\n",
    "filename = 'neighborhood_based_sfs_%s_no_features_%s_%s_%s_%d'%(output_var, no_features, bn, third_layer, no_units)\n",
    "\n",
    "#Standardize according to the fold\n",
    "scaler.fit(input_data[training_folds[fold]])\n",
    "\n",
    "#Load the data for the respective fold\n",
    "input_train = scaler.transform(input_data[training_folds[fold]])\n",
    "input_valid = scaler.transform(input_data[validation_folds[fold]])\n",
    "output_train = output_data[training_folds[fold]]\n",
    "output_valid = output_data[validation_folds[fold]]\n",
    "\n",
    "## Training and validation losses\n",
    "train_loss = model.evaluate(input_train, output_train, verbose=2, batch_size=10**5)\n",
    "valid_loss = model.evaluate(input_valid, output_valid, verbose=2, batch_size=10**5)\n",
    "\n",
    "train_losses.append(train_loss)\n",
    "valid_losses.append(valid_loss)\n",
    "\n",
    "with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "    file.write('Unbounded training loss: %.4f\\n'%(train_loss))\n",
    "    file.write('Unbounded validation loss: %.4f\\n'%(valid_loss))\n",
    "\n",
    "## Compute mean cloud cover per vertical layer\n",
    "# On the respective validation sets (QUBICC and NARVAL)\n",
    "try:\n",
    "    clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_valid, output_valid, \n",
    "                                                               layers_data[validation_folds[fold]])\n",
    "except(ResourceExhaustedError):\n",
    "    print('Resource Exhausted Qubicc')\n",
    "    clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_valid, output_valid, \n",
    "                                                               layers_data[validation_folds[fold]], batch_size=2**15)\n",
    "valid_means.append(clc_data_mean)\n",
    "valid_model_predictions.append(clc_pred_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "271bda1e-398d-435f-af9a-dd59a14fc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In case we want to reproduce the plots without running everything again:\n",
    "# with open(os.path.join(path_figures, 'values_for_figures.txt'), 'w') as file:\n",
    "#     file.write('On validation sets\\n')\n",
    "#     file.write(str(valid_means))\n",
    "#     file.write(str(valid_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d67c01-c9fe-416c-af1f-b5d014ace70f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compute bounded losses\n",
    "\n",
    "We also save the scaling parameters for the fold-based models as we haven't done that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54520c10-d1b7-425d-909e-713933e89ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_bounded_loss(model, input_data, output_data, batch_size=2**20):\n",
    "    for i in range(1 + input_data.shape[0]//batch_size):\n",
    "        if i == 0:\n",
    "            a = model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            a = np.concatenate((a, model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])), axis=0)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "    # Bounded output!\n",
    "    pred_adj = np.minimum(np.maximum(a[:,0], 0), 100) \n",
    "    \n",
    "    # Mean Squared Error\n",
    "    return np.mean((pred_adj - output_data)**2, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f651d-99e7-41fb-b06b-f7ad7bf7fe9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss = compute_bounded_loss(model, input_train, output_train, batch_size=2**15)\n",
    "valid_loss = compute_bounded_loss(model, input_valid, output_valid, batch_size=2**15)\n",
    "\n",
    "with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "    file.write('Bounded training loss: %.4f\\n'%(train_loss))\n",
    "    file.write('Bounded validation loss: %.4f\\n'%(valid_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds",
   "language": "python",
   "name": "clouds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

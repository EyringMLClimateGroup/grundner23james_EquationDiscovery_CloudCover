{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate polynomial fit\n",
    "\n",
    "Using the sequential feature selector\n",
    "\n",
    "There are $\\begin{pmatrix} n+N \\\\ N \\end{pmatrix}$ terms in a polynomial of degree $N$ and $n$ input features.\n",
    "\n",
    "In this notebook I want to first split the data into cloud regimes, then give the option to transform the features in a smarter way instead of just normalizing them (CDF-transformed)! For the degree 3 polynomial I work only with the ten best features (from the SFS NNs) (plus p for the regimes). After that I run the SFS. If a CDF-transformed variable is picked, then we could approximate that CDF using PySR.\n",
    "\n",
    "We can use this notebook to:\n",
    "- Run polynomials, linear models\n",
    "- Split the data into regimes (data-driven 2-4 regimes, isccp-based) and train on regimes\n",
    "- Scaling the features: Normalization, No scaling, CDF-scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150GB are enough to run the code (if not even 120GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "1140.0\n",
      "171.0\n",
      "2925.0\n",
      "325.0\n",
      "286.0\n",
      "364.0\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "from scipy import special\n",
    "\n",
    "print(special.binom(1 + 3, 3)) # Degree 3, one feature\n",
    "\n",
    "# qubicc\n",
    "print(special.binom(17 + 3, 3)) # Degree 3\n",
    "print(special.binom(17 + 2, 2)) # Degree 2\n",
    "\n",
    "# dyamond\n",
    "print(special.binom(24 + 3, 3)) # Degree 3\n",
    "print(special.binom(24 + 2, 2)) # Degree 2\n",
    "\n",
    "print(special.binom(10 + 3, 3)) # Degree 3, restricted set of inputs\n",
    "print(special.binom(11 + 3, 3)) # Degree 3, restricted set of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1663138212.73059\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cbab0d709b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# How shall we transform the data (pick 0,1,2,3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mset_transformation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mtransformation_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'normalized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cdf_in_and_out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cdf_in_only'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtransformation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformation_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mset_transformation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '-f'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from tensorflow import nn\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy import interpolate\n",
    "from scipy import misc\n",
    "\n",
    "sys.path.insert(0, '~/workspace_icon-ml/symbolic_regression/')\n",
    "from functions import differentiate\n",
    "from functions import append_dict_to_json\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, '~/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "\n",
    "# Reloading custom file to incorporate changes dynamically\n",
    "import my_classes\n",
    "\n",
    "from my_classes import read_mean_and_std\n",
    "\n",
    "# Set seed (usually set to 10 and False)\n",
    "seed = int(sys.argv[7])\n",
    "np.random.seed(seed)\n",
    "more_seeds = bool(int(sys.argv[8]))\n",
    "\n",
    "# Timing all cells\n",
    "print(time.time())\n",
    "\n",
    "# How shall we transform the data (pick 0,1,2,3)\n",
    "set_transformation = int(sys.argv[1])\n",
    "transformation_options = ['off', 'normalized', 'cdf_in_and_out', 'cdf_in_only']\n",
    "transformation = transformation_options[set_transformation]\n",
    "\n",
    "regime_types = ['off', 'data_driven', 'isccp_based']\n",
    "regime_type = regime_types[int(sys.argv[2])]\n",
    "\n",
    "# Only relevant for regime_type == 'data_driven':\n",
    "no_of_regimes = int(sys.argv[3]) # [2,3,4]\n",
    "dt_basis_nn_predictions = bool(int(sys.argv[4])) # DT decomposition based on NN predictions or not\n",
    "\n",
    "# Which regime should we work with (pick [1, ..., 4] for isccp_based, [1, ..., (no_of_regimes - 1)] for data_driven)\n",
    "regime = int(sys.argv[5])\n",
    "\n",
    "# Non-interactive backend\n",
    "matplotlib.use('PDF')\n",
    "\n",
    "# Should we use qubicc or dyamond data\n",
    "data_source = 'dyamond' \n",
    "\n",
    "# cl_volume or cl_area\n",
    "# output_var = sys.argv[1]\n",
    "output_var = 'cl_area' \n",
    "\n",
    "# Degree of the polynomial (<= 3 is possible)\n",
    "deg = int(sys.argv[6])\n",
    "# deg = 2\n",
    "\n",
    "if transformation == 'off':\n",
    "    subfolder = '%s_data/unnormalized_data'%data_source\n",
    "elif transformation == 'normalized':\n",
    "    subfolder = '%s_data/normalized_data'%data_source\n",
    "elif transformation == 'cdf_in_and_out':\n",
    "    subfolder = '%s_data/cdf_transformed_data'%data_source\n",
    "elif transformation == 'cdf_in_only':\n",
    "    subfolder = '%s_data/cdf_transformed_input_only'%data_source\n",
    "\n",
    "# JSON output file\n",
    "if regime_type == 'off':\n",
    "    outfile = '~/workspace_icon-ml/symbolic_regression/baselines/polynomial_results_v2/%s/without_regimes/polynomial_fit_%s_with_derivatives_degree_%d.json'%(subfolder, output_var, deg)\n",
    "elif regime_type == 'isccp_based':\n",
    "    outfile = '~/workspace_icon-ml/symbolic_regression/baselines/polynomial_results_v2/%s/isccp_based_regimes/polynomial_fit_%s_with_derivatives_degree_%d_regime_%d.json'%(subfolder, output_var, deg, regime)\n",
    "elif regime_type == 'data_driven' and not more_seeds:\n",
    "    outfile = '~/workspace_icon-ml/symbolic_regression/baselines/polynomial_results_v2/%s/data_driven_regimes/polynomial_fit_%s_with_derivatives_degree_%d_dt_basis_%s_no_of_regimes_%d_regime_%d.json'%(subfolder, output_var, deg, dt_basis_nn_predictions, no_of_regimes, regime)\n",
    "elif regime_type == 'data_driven' and more_seeds:\n",
    "    outfile = '~/workspace_icon-ml/symbolic_regression/baselines/polynomial_results_v2/%s/data_driven_regimes/more_seeds/polynomial_fit_%s_with_derivatives_degree_%d_dt_basis_%s_no_of_regimes_%d_regime_%d_seed_%d.json'%(subfolder, output_var, deg, dt_basis_nn_predictions, no_of_regimes, regime, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == 'qubicc':\n",
    "    # Load columns of data\n",
    "    folder_data = '~/workspace_icon-ml/symbolic_regression/data/'\n",
    "\n",
    "    input_train = np.load(os.path.join(folder_data, 'input_train_with_deriv.npy'))\n",
    "    input_valid = np.load(os.path.join(folder_data, 'input_valid_with_deriv.npy'))\n",
    "    output_train = np.load(os.path.join(folder_data, 'output_train_with_deriv.npy'))\n",
    "    output_valid = np.load(os.path.join(folder_data, 'output_valid_with_deriv.npy'))\n",
    "    \n",
    "elif data_source == 'dyamond':\n",
    "    output_path = '~/workspace_icon-ml/symbolic_regression/baselines/linear_results_v2/dyamond_data'    \n",
    "    folder_data = '~/my_work/icon-ml_data/cloud_cover_parameterization/neighborhood_based_SR_DYAMOND/'\n",
    "    \n",
    "    input_data = np.load(os.path.join(folder_data, 'cloud_cover_input_dyamond.npy'))\n",
    "    if output_var == 'cl_volume':\n",
    "        output_data = np.load(os.path.join(folder_data, 'cloud_cover_output_dyamond.npy'))\n",
    "    elif output_var == 'cl_area':\n",
    "        output_data = np.load(os.path.join(folder_data, 'cloud_area_output_dyamond.npy'))\n",
    "        \n",
    "    new_features = ['hus', 'clw', 'cli', 'ta', 'pa', 'zg', 'fr_land', 'U',\\\n",
    "                    'rh', 'ps', 'hus_z', 'hus_zz', 'clw_z', 'clw_zz', 'cli_z',\\\n",
    "                    'cli_zz', 'ta_z', 'ta_zz', 'pa_z', 'pa_zz', 'U_z', 'U_zz',\\\n",
    "                    'rh_z', 'rh_zz']\n",
    "        \n",
    "    # If deg == 3: Only use the 10 most important variables (deemed to be most important by the SFS NNs). Otherwise we run into memory issues.\n",
    "    if deg == 3:\n",
    "        # To locate variables\n",
    "        loc = {}\n",
    "        for i in range(len(new_features)):\n",
    "            loc[new_features[i]] = i \n",
    "        \n",
    "        # Keeping also pa so that we can put the data into regimes\n",
    "        selected_features = ['clw', 'cli', 'ta', 'rh', 'ps', 'ta_z', 'pa_z', 'pa_zz', 'rh_z', 'rh_zz', 'pa']\n",
    "            \n",
    "        A = np.zeros((input_data.shape[0], len(selected_features)))\n",
    "        for k in range(len(selected_features)):\n",
    "            A[:, k] = input_data[:, loc[selected_features[k]]]\n",
    "            \n",
    "        input_data = A.copy()\n",
    "        new_features = selected_features\n",
    "        \n",
    "        del A\n",
    "        gc.collect()\n",
    "    \n",
    "    samples_total, no_of_features = input_data.shape\n",
    "    \n",
    "    # Split into train/valid\n",
    "    training_folds = []\n",
    "    validation_folds = []\n",
    "    two_week_incr = samples_total//6\n",
    "\n",
    "    for i in range(3):\n",
    "        # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "        first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "        second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "        validation_folds.append(np.append(first_incr, second_incr))\n",
    "        training_folds.append(np.arange(samples_total))\n",
    "        training_folds[i] = np.delete(training_folds[i], validation_folds[i])\n",
    "        \n",
    "    # The second fold yields the best model\n",
    "    flattened_input_train = input_data[training_folds[1]]\n",
    "    flattened_input_valid = input_data[validation_folds[1]]\n",
    "    flattened_output_train = output_data[training_folds[1]]\n",
    "    flattened_output_valid = output_data[validation_folds[1]]\n",
    "    \n",
    "    # Remove input_data, output_data\n",
    "    del input_data, output_data, training_folds, validation_folds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on regime 0: 0.0353\n",
      "Number of samples in regime 0: 32419018\n"
     ]
    }
   ],
   "source": [
    "# # MSE that comes from regime 0 [for cl_area there are still some condensate-free clouds in the data set]\n",
    "# I = np.where(flattened_input_valid[:, 1] + flattened_input_valid[:, 2] == 0)[0]\n",
    "# M = np.mean(flattened_output_valid[I])\n",
    "\n",
    "# print('MSE on regime 0: %.4f'%np.mean((flattened_output_valid[I])**2))\n",
    "\n",
    "# # MSE \n",
    "# print('Number of samples in regime 0: %d'%len(I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == 'qubicc':\n",
    "    # Features\n",
    "    new_features = ['qv', 'qv_z', 'qv_zz', 'qc', 'qc_z', 'qc_zz', 'qi', 'qi_z', 'qi_zz', 'temp', 'temp_z', 'temp_zz', \\\n",
    "                    'pres', 'pres_z', 'pres_zz', 'zg', 'fr_land']\n",
    "\n",
    "    # To locate variables\n",
    "    loc = {}\n",
    "    for i in range(len(new_features)):\n",
    "        loc[new_features[i]] = i\n",
    "\n",
    "    # input_train.shape is (100000, 27, 17)\n",
    "    \n",
    "    ## Add relative humidity\n",
    "    pres_train = input_train[:, :, loc['pres']]\n",
    "    qv = input_train[:, :, loc['qv']]\n",
    "    temp = input_train[:, :, loc['temp']]\n",
    "\n",
    "    T0 = 273.15\n",
    "    r = 0.00263*pres_train*qv*np.exp((17.67*(temp-T0))/(temp-29.65))**(-1)\n",
    "\n",
    "    new_features.append('rh')\n",
    "    input_train = np.append(input_train, np.expand_dims(r, -1), axis=2)\n",
    "\n",
    "    # The same for input_valid\n",
    "    pres_valid = input_valid[:, :, loc['pres']]\n",
    "    qv = input_valid[:, :, loc['qv']]\n",
    "    temp = input_valid[:, :, loc['temp']]\n",
    "\n",
    "    T0 = 273.15\n",
    "    r = 0.00263*pres_valid*qv*np.exp((17.67*(temp-T0))/(temp-29.65))**(-1)\n",
    "\n",
    "    input_valid = np.append(input_valid, np.expand_dims(r, -1), axis=2)\n",
    "    \n",
    "    ## Add surface pressure to every sample\n",
    "    ps_train = np.expand_dims(np.repeat(np.expand_dims(pres_train[:, -1], -1), 27, axis=1), -1)\n",
    "    ps_valid = np.expand_dims(np.repeat(np.expand_dims(pres_valid[:, -1], -1), 27, axis=1), -1)\n",
    "\n",
    "    new_features.append('ps')\n",
    "    input_train = np.append(input_train, ps_train, axis=2)\n",
    "    input_valid = np.append(input_valid, ps_valid, axis=2)\n",
    "    \n",
    "    # Updating loc\n",
    "    loc = {}\n",
    "    for i in range(len(new_features)):\n",
    "        loc[new_features[i]] = i\n",
    "    \n",
    "    # input_train.shape is (100000, 27, 19)\n",
    "    \n",
    "    # Normalizing the data\n",
    "    if transformation=='normalized':\n",
    "        m = np.mean(input_train, axis=(0,1), dtype=np.float64)\n",
    "        s = np.std(input_train, axis=(0,1), dtype=np.float64)\n",
    "\n",
    "        input_train = (input_train - m)/s\n",
    "        input_valid = (input_valid - m)/s\n",
    "        \n",
    "    # Flatten before passing it to LinearRegression()\n",
    "    flattened_input_train = np.reshape(input_train, (-1, len(new_features)))\n",
    "    flattened_input_valid = np.reshape(input_valid, (-1, len(new_features)))\n",
    "    flattened_output_train = np.reshape(output_train, -1)\n",
    "    flattened_output_valid = np.reshape(output_valid, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into cloud regimes**\n",
    "\n",
    "According to both:\n",
    "- a*q_i + q_c\n",
    "- air pressure\n",
    "\n",
    "--> There is no easy way to specify a, so I choose it to be equal to 1 (alternatively one could think about mean(a qi) = mean(qc)). Then I can interpret qi+qc as the condensate mixing ratio.\n",
    "\n",
    "So I have four regimes in total: <br>\n",
    "1) 0 < qi+qc < 1.6e-5 and p < 7.9e4 [High altitude, little condensate]\n",
    "2) 0 < qi+qc < 1.6e-5 and p > 7.9e4 [Low altitude, little condensate]\n",
    "3) qi+qc > 1.6e-5 and p < 7.9e4 [High altitude, high condensate]\n",
    "4) qi+qc > 1.6e-5 and p > 7.9e4 [Low altitude, high condensate]\n",
    "\n",
    "For $qi + qc = 0$ we simply set $C = 0$.\n",
    "\n",
    "In every regime there are more than 2.3e6 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To locate variables\n",
    "loc = {}\n",
    "for i in range(len(new_features)):\n",
    "    loc[new_features[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if regime_type == 'isccp_based':\n",
    "    a = 1\n",
    "\n",
    "    cod_subs = a*flattened_input_train[:, loc['cli']] + flattened_input_train[:, loc['clw']]\n",
    "    cod_subs_med = np.median(cod_subs[cod_subs != 0])\n",
    "\n",
    "    pa_med = np.median(flattened_input_train[cod_subs != 0, loc['pa']])\n",
    "\n",
    "    # For the training data\n",
    "    input_train_reg_1 = flattened_input_train[(0 < cod_subs) & (cod_subs < cod_subs_med) & (flattened_input_train[:, loc['pa']] < pa_med)]\n",
    "    input_train_reg_2 = flattened_input_train[(0 < cod_subs) & (cod_subs < cod_subs_med) & (flattened_input_train[:, loc['pa']] > pa_med)]\n",
    "    input_train_reg_3 = flattened_input_train[(cod_subs > cod_subs_med) & (flattened_input_train[:, loc['pa']] < pa_med)]\n",
    "    input_train_reg_4 = flattened_input_train[(cod_subs > cod_subs_med) & (flattened_input_train[:, loc['pa']] > pa_med)]\n",
    "\n",
    "    output_train_reg_1 = flattened_output_train[(0 < cod_subs) & (cod_subs < cod_subs_med) & (flattened_input_train[:, loc['pa']] < pa_med)]\n",
    "    output_train_reg_2 = flattened_output_train[(0 < cod_subs) & (cod_subs < cod_subs_med) & (flattened_input_train[:, loc['pa']] > pa_med)]\n",
    "    output_train_reg_3 = flattened_output_train[(cod_subs > cod_subs_med) & (flattened_input_train[:, loc['pa']] < pa_med)]\n",
    "    output_train_reg_4 = flattened_output_train[(cod_subs > cod_subs_med) & (flattened_input_train[:, loc['pa']] > pa_med)]\n",
    "\n",
    "    # Same for the validation data\n",
    "    cod_subs = a*flattened_input_valid[:, loc['cli']] + flattened_input_valid[:, loc['clw']]\n",
    "\n",
    "    input_valid_reg_1 = flattened_input_valid[(0 < cod_subs) & (cod_subs < cod_subs_med) & (flattened_input_valid[:, loc['pa']] < pa_med)]\n",
    "    input_valid_reg_2 = flattened_input_valid[(0 < cod_subs) & (cod_subs < cod_subs_med) & (flattened_input_valid[:, loc['pa']] > pa_med)]\n",
    "    input_valid_reg_3 = flattened_input_valid[(cod_subs > cod_subs_med) & (flattened_input_valid[:, loc['pa']] < pa_med)]\n",
    "    input_valid_reg_4 = flattened_input_valid[(cod_subs > cod_subs_med) & (flattened_input_valid[:, loc['pa']] > pa_med)]\n",
    "\n",
    "    output_valid_reg_1 = flattened_output_valid[(0 < cod_subs) & (cod_subs < cod_subs_med) & (flattened_input_valid[:, loc['pa']] < pa_med)]\n",
    "    output_valid_reg_2 = flattened_output_valid[(0 < cod_subs) & (cod_subs < cod_subs_med) & (flattened_input_valid[:, loc['pa']] > pa_med)]\n",
    "    output_valid_reg_3 = flattened_output_valid[(cod_subs > cod_subs_med) & (flattened_input_valid[:, loc['pa']] < pa_med)]\n",
    "    output_valid_reg_4 = flattened_output_valid[(cod_subs > cod_subs_med) & (flattened_input_valid[:, loc['pa']] > pa_med)]\n",
    "\n",
    "    # Do the regimes have a similar size?\n",
    "    for i in range(1, 5):\n",
    "        print(locals()['input_train_reg_%d'%i].shape)\n",
    "        print(locals()['output_train_reg_%d'%i].shape)\n",
    "\n",
    "    # Do the regimes have a similar size?\n",
    "    for i in range(1, 5):\n",
    "        print(locals()['input_valid_reg_%d'%i].shape)\n",
    "        print(locals()['output_valid_reg_%d'%i].shape)\n",
    "\n",
    "elif regime_type == 'data_driven':\n",
    "    from sklearn import tree\n",
    "    \n",
    "    # Already remove the regime with clw + cli = 0\n",
    "    reg_not_0_train = np.where(flattened_input_train[:, loc['clw']] + flattened_input_train[:, loc['cli']] != 0)[0]\n",
    "    flattened_input_train = flattened_input_train[reg_not_0_train]\n",
    "    flattened_output_train = flattened_output_train[reg_not_0_train]\n",
    "    \n",
    "    reg_not_0_valid = np.where(flattened_input_valid[:, loc['clw']] + flattened_input_valid[:, loc['cli']] != 0)[0]\n",
    "    flattened_input_valid = flattened_input_valid[reg_not_0_valid]\n",
    "    flattened_output_valid = flattened_output_valid[reg_not_0_valid]\n",
    "    \n",
    "    # We only need to split the regimes further if no_of_regimes > 2\n",
    "    if no_of_regimes > 2:\n",
    "        # Take a subset of the data to train the decision tree on\n",
    "        subset_size = 10**7 # or 10**6\n",
    "\n",
    "        inds = np.random.randint(0, flattened_input_train.shape[0], subset_size)\n",
    "        input_subset = flattened_input_train[inds]\n",
    "        output_subset = flattened_output_train[inds]\n",
    "        \n",
    "        if dt_basis_nn_predictions:\n",
    "            # Load SFS cl_area NN with four input features\n",
    "            custom_objects = {}\n",
    "            custom_objects['leaky_relu'] = nn.leaky_relu\n",
    "\n",
    "            path_model = '~/workspace_icon-ml/cloud_cover_parameterization/neighborhood_based_on_seq_feat_sel_DYAMOND/saved_models/hyperparameter_tests/neighborhood_based_sfs_cl_area_no_features_4_False_False_16.txt'\n",
    "            model = load_model(path_model[:-3]+'h5', custom_objects=custom_objects)\n",
    "\n",
    "            # Normalize input_subset.\n",
    "            mean, std = read_mean_and_std(path_model)\n",
    "            input_subset_scaled = (np.concatenate([np.expand_dims(input_subset[:, loc[sel_var]], axis=1) for sel_var in ['clw', 'cli', 'ta', 'rh']], axis = 1) - mean)/std\n",
    "            \n",
    "            Y = model.predict(input_subset_scaled)\n",
    "        else:\n",
    "            Y = output_subset.copy()\n",
    "\n",
    "        classification_tree = tree.DecisionTreeRegressor(max_depth=3, max_leaf_nodes=(no_of_regimes-1)) # set max_depth to [2,3]\n",
    "        classification_tree.fit(input_subset, Y)\n",
    "        text_representation = tree.export_text(classification_tree, feature_names=new_features)\n",
    "\n",
    "        if dt_basis_nn_predictions:\n",
    "            dt_tree_path = '~/workspace_icon-ml/symbolic_regression/baselines/polynomial_results_v2/%s_data/decision_trees/based_on_nn_preds_%d_regimes.txt'%(data_source, no_of_regimes)\n",
    "        else:\n",
    "            dt_tree_path = '~/workspace_icon-ml/symbolic_regression/baselines/polynomial_results_v2/%s_data/decision_trees/based_on_orig_data_%d_regimes.txt'%(data_source, no_of_regimes)\n",
    "\n",
    "        with open(dt_tree_path, 'w') as file:\n",
    "            file.write(text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose the appropriate regime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-6954f38ef2f3>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6954f38ef2f3>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    if np.sum(ind_reg_train)\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if regime_type == 'isccp_based':\n",
    "    # Convert string into variable name according to the right regime\n",
    "    flattened_input_train = locals()['input_train_reg_%d'%regime].copy()\n",
    "    flattened_input_valid = locals()['input_valid_reg_%d'%regime].copy()\n",
    "\n",
    "    flattened_output_train = locals()['output_train_reg_%d'%regime].copy()\n",
    "    flattened_output_valid = locals()['output_valid_reg_%d'%regime].copy()\n",
    "elif regime_type == 'data_driven' and no_of_regimes > 2:\n",
    "    ind_reg_train = np.where(classification_tree.apply(flattened_input_train) == regime)\n",
    "    ind_reg_valid = np.where(classification_tree.apply(flattened_input_valid) == regime)\n",
    "    \n",
    "    # Sometimes, the regime is called differently...\n",
    "    if np.sum(ind_reg_train) == 0:\n",
    "        print('The regime %d does not exist, switching to regime %d instead.'%(regime, no_of_regimes))\n",
    "        ind_reg_train = np.where(classification_tree.apply(flattened_input_train) == no_of_regimes)\n",
    "        ind_reg_valid = np.where(classification_tree.apply(flattened_input_valid) == no_of_regimes)\n",
    "    \n",
    "    flattened_input_train = flattened_input_train[ind_reg_train]\n",
    "    flattened_input_valid = flattened_input_valid[ind_reg_valid]\n",
    "\n",
    "    flattened_output_train = flattened_output_train[ind_reg_train]\n",
    "    flattened_output_valid = flattened_output_valid[ind_reg_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transformation=='normalized':\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    scaler.fit(flattened_input_train)\n",
    "    \n",
    "    flattened_input_train = scaler.transform(flattened_input_train)\n",
    "    flattened_input_valid = scaler.transform(flattened_input_valid)\n",
    "    \n",
    "    d = {}\n",
    "    d['Feature names'] = str(new_features)\n",
    "    d['Mean'] = str(scaler.mean_)\n",
    "    d['Variance'] = str(scaler.var_)\n",
    "    d['Number of validation samples'] = int(np.minimum(flattened_input_valid.shape[0], 2*10**6))\n",
    "    \n",
    "    if regime_type == 'data_driven':\n",
    "        json_outpath = '~/workspace_icon-ml/symbolic_regression/baselines/polynomial_results_v2/%s/scalers_dt_basis_%s_no_of_regimes_%d_regime_%d.json'%(subfolder, dt_basis_nn_predictions, no_of_regimes, regime)\n",
    "    else:\n",
    "        json_outpath = '~/workspace_icon-ml/symbolic_regression/baselines/polynomial_results_v2/%s/scalers_regime_%d.json'%(subfolder, regime)\n",
    "    \n",
    "    with open(json_outpath, 'w') as file:\n",
    "        json.dump(d, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform features according to CDFs**\n",
    "\n",
    "The CDF is only computed in an approximative fashion on a regular grid, which is an approximation and could be sensitive to the grid spacing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fff357fb190>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_cdf(A, h):\n",
    "    '''\n",
    "        This method creates a CDF function based on A and does NOT transform A.\n",
    "        Note that len(a) = len(b).\n",
    "    \n",
    "        A: Vector of values\n",
    "        h: The values for the CDF are calculated at the points np.min(A) + kh, k in N\n",
    "        \n",
    "        Yields:\n",
    "        a, b: x- and y-values of the CDF\n",
    "    '''\n",
    "    a = np.arange(np.min(A), np.max(A)+h, h)\n",
    "\n",
    "    b = []\n",
    "    for k in range(len(a)):\n",
    "        b.append(np.sum(A <= a[k])/len(A))\n",
    "    \n",
    "    return a, np.array(b) \n",
    "\n",
    "def compute_inverse_cdf(X, a, b):\n",
    "    '''\n",
    "        Reconstructs the original values of the vector X (that has values in [0, 1]) \n",
    "        based on the CDF which is described by a mapping a -> b.\n",
    "    \n",
    "        It holds: len(X) = len(A).\n",
    "        If (a,b) = compute_cdf(A, h), then compute_inverse_cdf(b, a, b) = a.\n",
    "    '''\n",
    "    # Convert to nd-array just in case\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    A = [a[np.argmin(np.abs(X[i] - b))] for i in range(len(X))]\n",
    "    \n",
    "    return np.array(A)\n",
    "\n",
    "# h = (np.max(flattened_input_train[:, loc['hus']]) - np.min(flattened_input_train[:, loc['hus']]))/100\n",
    "# a, b = compute_cdf(flattened_input_train[:, loc['hus']], h)\n",
    "# plt.plot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the inputs\n",
    "if transformation=='cdf_in_and_out' or transformation=='cdf_in_only':\n",
    "    \n",
    "    cdf_transformed_train = {key: None for key in loc}\n",
    "    cdf_transformed_valid = {key: None for key in loc}\n",
    "    \n",
    "    # Save x values of the cdf to be able to invert it\n",
    "    cdf_x_values = {key: None for key in loc}\n",
    "    cdf_y_values = {key: None for key in loc}\n",
    "\n",
    "    # 37 seconds per key (have 24 keys) --> 15 minutes\n",
    "    for key in cdf_transformed_train.keys():\n",
    "        A = flattened_input_train[:, loc[key]]\n",
    "        A_val = flattened_input_valid[:, loc[key]]\n",
    "\n",
    "        # We allow for 100 different values\n",
    "        h = (np.max(A) - np.min(A))/100   \n",
    "\n",
    "        # Compute the CDF\n",
    "        a, b = compute_cdf(A, h)\n",
    "        \n",
    "        cdf_x_values[key] = a.tolist()\n",
    "        cdf_y_values[key] = b.tolist()\n",
    "\n",
    "        # Find the relevant index\n",
    "        inds = np.array((A - np.min(A))/h, dtype=int)\n",
    "\n",
    "        # Same for the validation data\n",
    "        inds_val = np.array((A_val - np.min(A))/h, dtype=int)\n",
    "\n",
    "        # For the validation data we could be out of sample so we have to take care of the indices\n",
    "        inds_val = np.maximum(np.minimum(inds_val, len(b) - 1), 0)\n",
    "\n",
    "        cdf_transformed_train[key] = b[inds]\n",
    "        cdf_transformed_valid[key] = b[inds_val]\n",
    "    \n",
    "    # Replace the data with the CDF-transformed data\n",
    "    for key in cdf_transformed_train.keys():\n",
    "        flattened_input_train[:, loc[key]] = cdf_transformed_train[key]\n",
    "        flattened_input_valid[:, loc[key]] = cdf_transformed_valid[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Test 1: Successful\n",
    "\n",
    "# q = np.array([1,1,2,3,3.5,4])\n",
    "\n",
    "# # We allow for 100 different values\n",
    "# h = (max(q) - min(q))/100\n",
    "\n",
    "# # Compute the CDF\n",
    "# a, b = compute_cdf(q, h)\n",
    "\n",
    "# # Find the relevant index\n",
    "# inds = np.array((q - min(q))/h, dtype=int)\n",
    "\n",
    "# print(b[inds])\n",
    "# plt.plot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform the output\n",
    "if transformation=='cdf_in_and_out':\n",
    "\n",
    "    A = flattened_output_train\n",
    "    A_val = flattened_output_valid\n",
    "\n",
    "    # We allow for 100 different values\n",
    "    h = (np.max(A) - np.min(A))/100\n",
    "\n",
    "    # Compute the CDF\n",
    "    a, b = compute_cdf(A, h)\n",
    "    plt.plot(a, b)\n",
    "\n",
    "    cdf_x_values[output_var] = a.tolist()\n",
    "    cdf_y_values[output_var] = b.tolist()\n",
    "    \n",
    "    # Find the relevant index\n",
    "    inds = np.array((A - np.min(A))/h, dtype=int)\n",
    "\n",
    "    # Same for the validation data\n",
    "    inds_val = np.array((A_val - np.min(A))/h, dtype=int)\n",
    "\n",
    "    cdf_transformed_train[output_var] = b[inds]\n",
    "    cdf_transformed_valid[output_var] = b[inds_val]\n",
    "    \n",
    "    # Replace the output data with the CDF-transformed data\n",
    "    flattened_output_train = cdf_transformed_train[output_var]\n",
    "    flattened_output_valid = cdf_transformed_valid[output_var]\n",
    "        \n",
    "    del cdf_transformed_train, cdf_transformed_valid\n",
    "    gc.collect()\n",
    "    \n",
    "    # # Write this file only if deg = 2, otherwise it will get overwritten for deg = 3 with its fewer input features\n",
    "    # if deg == 2:\n",
    "    # To be able to invert the CDF if necessary\n",
    "    with open('~/workspace_icon-ml/symbolic_regression/baselines/polynomial_results_v2/%s/cdf_x_values_regime_%d.json'%(subfolder, regime), 'w') as file:\n",
    "        json.dump(cdf_x_values, file)\n",
    "\n",
    "    with open('~/workspace_icon-ml/symbolic_regression/baselines/polynomial_results_v2/%s/cdf_y_values_regime_%d.json'%(subfolder, regime), 'w') as file:\n",
    "        json.dump(cdf_y_values, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Work only with 2 Mio samples each**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flattened_input_valid.shape[0] > 2*10**6:\n",
    "    subs_ind_train = np.random.randint(1, flattened_input_train.shape[0], 2*10**6)\n",
    "    subs_ind_val = np.random.randint(1, flattened_input_valid.shape[0], 2*10**6)\n",
    "\n",
    "    flattened_input_train = flattened_input_train[subs_ind_train]\n",
    "    flattened_output_train = flattened_output_train[subs_ind_train]\n",
    "    flattened_input_valid = flattened_input_valid[subs_ind_val]\n",
    "    flattened_output_valid = flattened_output_valid[subs_ind_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the feature selector**\n",
    "\n",
    "For 5 Mio samples max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For cv = 3 and deg = 3 we can do one loop/2 hours\n",
    "# For cv = 3 and deg = 2 we can do one loop/22 minutes\n",
    "\n",
    "cv = 2\n",
    "min_features_to_select = 1\n",
    "max_features_to_select = 11 # maximum number of features to select + 1\n",
    "\n",
    "# Collect all results\n",
    "dict_combined = {}\n",
    "\n",
    "# Generate a model of polynomial features\n",
    "# Turns 19 into 210 features\n",
    "poly = PolynomialFeatures(degree=deg)\n",
    "X_train = poly.fit_transform(flattened_input_train)\n",
    "\n",
    "if deg == 2:\n",
    "    assert X_train.shape[1] == 1 + len(new_features)/2*(len(new_features)+3)\n",
    "\n",
    "print(X_train.shape[1])\n",
    "\n",
    "new_features = poly.get_feature_names(new_features) # Update the feature names\n",
    "X_valid = poly.fit_transform(flattened_input_valid)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "for no_features in np.arange(min_features_to_select, max_features_to_select): \n",
    "    sfs = SequentialFeatureSelector(lin_reg, n_features_to_select=no_features, direction='forward', cv=cv, n_jobs=-1)\n",
    "    sfs.fit(X_train, flattened_output_train)\n",
    "    selected_features = np.array(new_features)[sfs.get_support()].tolist()\n",
    "    print(\n",
    "        \"Features selected by forward sequential selection: \"\n",
    "        f\"{selected_features}\"\n",
    "    )\n",
    "\n",
    "    # Plot coefficient variability\n",
    "    X_transformed = sfs.transform(X_train)\n",
    "\n",
    "    cv_model = cross_validate(\n",
    "        lin_reg,\n",
    "        X_transformed,\n",
    "        flattened_output_train,\n",
    "        cv=RepeatedKFold(n_splits=cv, n_repeats=cv),\n",
    "        return_estimator=True,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    coefs = pd.DataFrame(\n",
    "        [\n",
    "            est.coef_\n",
    "            for est in cv_model[\"estimator\"]\n",
    "        ],\n",
    "        columns=selected_features,\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    sns.stripplot(data=coefs, orient=\"h\", color=\"k\", alpha=0.5)\n",
    "    sns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5, width=.1)\n",
    "    plt.axvline(x=0, color=\".5\")\n",
    "    plt.title(\"Coefficient variability\")\n",
    "\n",
    "    if regime_type == 'data_driven' and not more_seeds:\n",
    "        plt.savefig('~/workspace_icon-ml/symbolic_regression/baselines/polynomial_results_v2/%s/data_driven_regimes/coefs_%d_features_degree_%d_%s_dt_basis_%s_no_of_regimes_%d_regime_%d.pdf'\\\n",
    "                %(subfolder, no_features, deg, output_var, dt_basis_nn_predictions, no_of_regimes, regime))\n",
    "    elif not more_seeds:\n",
    "        plt.savefig('~/workspace_icon-ml/symbolic_regression/baselines/polynomial_results_v2/%s/coefs_%d_features_degree_%d_%s_regime_%d.pdf'\\\n",
    "                %(subfolder, no_features, deg, output_var, regime))\n",
    "\n",
    "    # Predictions of the optimal multiple linear regression model trained on transformed data\n",
    "    lin_reg.fit(X_transformed, flattened_output_train)\n",
    "\n",
    "    Y_transformed = sfs.transform(X_valid)\n",
    "\n",
    "    clc_predictions_train = lin_reg.predict(X_transformed)\n",
    "    clc_predictions_valid = lin_reg.predict(Y_transformed)\n",
    "\n",
    "    # We can at least a posteriori enforce the [0, 100] constraint\n",
    "    if regime_type == 'data_driven':\n",
    "        clc_predictions_train = np.maximum(np.minimum(clc_predictions_train, 100), 0)\n",
    "        clc_predictions_valid = np.maximum(np.minimum(clc_predictions_valid, 100), 0)\n",
    "\n",
    "    if data_source == 'qubicc':    \n",
    "        ## Remove upper-most two layers to compute R2 values\n",
    "        input_train_upper_removed = []\n",
    "        output_train_upper_removed = []\n",
    "        input_valid_upper_removed = []\n",
    "        output_valid_upper_removed = []\n",
    "\n",
    "        assert X_transformed.shape[0] == Y_transformed.shape[0]\n",
    "\n",
    "        for i in range(X_transformed.shape[0]):\n",
    "            if not i%27 in [0, 1]:\n",
    "                input_train_upper_removed.append(X_transformed[i])\n",
    "                output_train_upper_removed.append(flattened_output_train[i])\n",
    "                input_valid_upper_removed.append(Y_transformed[i])\n",
    "                output_valid_upper_removed.append(flattened_output_valid[i])\n",
    "\n",
    "        input_train_upper_removed = np.array(input_train_upper_removed)\n",
    "        output_train_upper_removed = np.array(output_train_upper_removed)\n",
    "        input_valid_upper_removed = np.array(input_valid_upper_removed)\n",
    "        output_valid_upper_removed = np.array(output_valid_upper_removed)\n",
    "\n",
    "    # Output function and its R2-score\n",
    "    dict_exp = {}\n",
    "    for i in range(len(selected_features)):\n",
    "        dict_exp[selected_features[i]] = lin_reg.coef_[i]\n",
    "    dict_exp['Bias'] = lin_reg.intercept_\n",
    "    # DYAMOND: MSEs and R2 were computed on subsets of no_samples each\n",
    "    if transformation=='cdf_in_and_out':\n",
    "        # Inverting the CDF\n",
    "        clc_predictions_train = compute_inverse_cdf(clc_predictions_train, cdf_x_values[output_var], cdf_y_values[output_var])\n",
    "        clc_predictions_valid = compute_inverse_cdf(clc_predictions_valid, cdf_x_values[output_var], cdf_y_values[output_var])\n",
    "        flattened_output_train = compute_inverse_cdf(flattened_output_train, cdf_x_values[output_var], cdf_y_values[output_var])\n",
    "        flattened_output_valid = compute_inverse_cdf(flattened_output_valid, cdf_x_values[output_var], cdf_y_values[output_var])    \n",
    "    if data_source == 'qubicc':\n",
    "        dict_exp['R2 score on training data'] = lin_reg.score(input_train_upper_removed, output_train_upper_removed)\n",
    "        dict_exp['R2 score on validation data'] = lin_reg.score(input_valid_upper_removed, output_valid_upper_removed)\n",
    "    elif data_source == 'dyamond':\n",
    "        # Should be equivalent to lin_reg.score, however, lin_reg.score does not transform the output before comparing it to the truth!\n",
    "        dict_exp['R2 score on training data (regime)'] = 1 - np.mean((clc_predictions_train - flattened_output_train)**2)/np.var(flattened_output_train)\n",
    "        dict_exp['R2 score on validation data (regime)'] = 1 - np.mean((clc_predictions_valid - flattened_output_valid)**2)/np.var(flattened_output_valid)\n",
    "\n",
    "    dict_exp['MSE on training data (regime)'] = mean_squared_error(flattened_output_train, clc_predictions_train)\n",
    "    dict_exp['MSE on validation data (regime)'] = mean_squared_error(flattened_output_valid, clc_predictions_valid)\n",
    "\n",
    "    dict_combined['Number of variables %d'%no_features] = dict_exp\n",
    "\n",
    "    # Write dict_combined to the JSON outfile\n",
    "    append_dict_to_json(dict_combined, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds",
   "language": "python",
   "name": "clouds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

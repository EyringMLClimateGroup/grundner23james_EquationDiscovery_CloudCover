{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate no_spinup column-based NNs\n",
    "\n",
    "Usually:\n",
    "- Load data (Has spin-up removed, is downsampled)\n",
    "- Split into train/valid\n",
    "- Normalize validation set\n",
    "- Predict on validation set\n",
    "\n",
    "Here:\n",
    "- Load data\n",
    "- Remove spin-up\n",
    "- Normalize all data [Cannot downsample yet as it is cell-based]\n",
    "- Predict on all data\n",
    "- Downsample [Have to downsample before doing the train/validation split!]\n",
    "- Split into train/validation set\n",
    "- Compute MSE/R2 on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Ran with 800GB (750GB should also be fine)\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# For Leaky_ReLU:\n",
    "from tensorflow import nn \n",
    "\n",
    "t0 = time.time()\n",
    "path = '/home/b/b309170'\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, path + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "\n",
    "# Reloading custom file to incorporate changes dynamically\n",
    "import importlib\n",
    "import my_classes\n",
    "importlib.reload(my_classes)\n",
    "\n",
    "from my_classes import write_infofile\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import TimeOut\n",
    "\n",
    "# Cross-validation fold (in 0,1,2)\n",
    "# fold = int(sys.argv[1]) #!\n",
    "fold = 1\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_visible_devices(gpus[3], 'GPU')\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Cover or Cloud Area?\n",
    "# output_var = sys.argv[3] # Set output_var to one of {'cl_volume', 'cl_area'} #!\n",
    "output_var = 'cl_area'\n",
    "\n",
    "path_base = os.path.join(path, 'workspace_icon-ml/cloud_cover_parameterization/grid_column_based_DYAMOND')\n",
    "path_data = os.path.join(path, 'my_work/icon-ml_data/cloud_cover_parameterization/grid_column_based_DYAMOND')\n",
    "    \n",
    "path_model = os.path.join(path_base, 'saved_models')\n",
    "path_figures = os.path.join(path_base, 'figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't run on a CPU node\n",
    "try:\n",
    "    # Prevents crashes of the code\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "    # Allow the growth of memory Tensorflow allocates (limits memory usage overall)\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.transpose(np.load(path_data + '/cloud_cover_input_dyamond.npy'))\n",
    "\n",
    "# if output_var == 'cl_volume':\n",
    "#     output_data = np.transpose(np.load(path_data + '/cloud_cover_output_dyamond.npy'))\n",
    "# elif output_var == 'cl_area':\n",
    "#     output_data = np.transpose(np.load(path_data + '/cloud_area_output_dyamond.npy'))\n",
    "\n",
    "## Remove the spin-up & reshape back (467*79342, 163)\n",
    "# Actually, we have to remove the spinup here, if we want to have a model comparable to the other ones from symbolic regression!\n",
    "t_steps = 619\n",
    "h_fields = 79342\n",
    "no_vars = 163\n",
    "\n",
    "## For the input data\n",
    "B = np.zeros((t_steps, no_vars, h_fields))\n",
    "# Invert reshaping\n",
    "for i in range(no_vars):\n",
    "    B[:, i] = np.reshape(input_data[:, i], (t_steps, h_fields))\n",
    "# Discard spinup\n",
    "input_data = np.concatenate((B[80:329], B[(329+72):]), axis=0)\n",
    "\n",
    "# Reshape back\n",
    "B = [np.reshape(input_data[:, i], -1) for i in range(no_vars)]\n",
    "input_data = np.array(B).T\n",
    "\n",
    "# no_vars = 27\n",
    "\n",
    "# ## For the output data\n",
    "# B = np.zeros((t_steps, no_vars, h_fields))\n",
    "# # Invert reshaping\n",
    "# for i in range(no_vars):\n",
    "#     B[:, i] = np.reshape(output_data[:, i], (t_steps, h_fields))\n",
    "# # Discard spinup\n",
    "# output_data = np.concatenate((B[80:329], B[(329+72):]), axis=0)\n",
    "\n",
    "# # Reshape back\n",
    "# B = [np.reshape(output_data[:, i], -1) for i in range(no_vars)]\n",
    "# output_data = np.array(B).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37052714, 163)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(samples_total, no_of_features) = input_data.shape\n",
    "(samples_total, no_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that are constant in at least one of the training folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_fields = [27, 28, 29, 30, 31, 32, 135, 136, 137]\n",
    "assert no_of_features == 163\n",
    "input_data = np.delete(input_data, remove_fields, axis=1)\n",
    "no_of_features = no_of_features - len(remove_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function for the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {}\n",
    "custom_objects['leaky_relu'] = nn.leaky_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'cross_validation_column_based_%s_fold_%d_no_spinup.h5'%(output_var, (fold+1))\n",
    "model = load_model(os.path.join(path_model, model_name), custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful functions to plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_data, batch_size=2**20):\n",
    "    '''\n",
    "        Model predictions\n",
    "    '''        \n",
    "    for i in range(input_data.shape[0]//batch_size + 1): \n",
    "        if i == 0:\n",
    "            predictions = model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            predictions = np.concatenate((predictions, model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])), axis=0)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the models on the data\n",
    "\n",
    "Add training and validation losses to the text files. <br>\n",
    "Print results per vertical layer (respective validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_losses = [] ; valid_losses = [] ; valid_means = [] ; valid_model_predictions = []\n",
    "t_steps = 467; h_fields = 79342\n",
    "\n",
    "filename = 'cross_validation_column_based_%s_fold_%d_no_spinup'%(output_var, (fold+1))\n",
    "    \n",
    "#Standardize according to the fold\n",
    "mean, std = read_mean_and_std(os.path.join(path_model, filename+'.txt'))\n",
    "\n",
    "#Load the data for the respective fold\n",
    "input_scaled = (input_data - mean)/std\n",
    "predictions = predict(model, input_scaled)\n",
    "\n",
    "## Reshape and downsample\n",
    "pred_reshaped = np.zeros((t_steps, 27, h_fields))\n",
    "\n",
    "for i in range(27):\n",
    "    pred_reshaped[:, i] = np.reshape(predictions[:, i], (t_steps, h_fields))\n",
    "    \n",
    "pred_reshaped = np.reshape(pred_reshaped, -1)\n",
    "\n",
    "#Load indices that we should keep\n",
    "inds = np.load('~/my_work/icon-ml_data/cloud_cover_parameterization/neighborhood_based_SR_DYAMOND/indices_to_keep_after_downsampling.npy')\n",
    "if output_var == 'cl_area':\n",
    "    output_data = np.load('~/my_work/icon-ml_data/cloud_cover_parameterization/neighborhood_based_SR_DYAMOND/cloud_area_output_dyamond.npy')\n",
    "\n",
    "# The maximum index to keep should be close to the total number of indices\n",
    "assert (np.max(inds) - len(pred_reshaped)) < 100 \n",
    "    \n",
    "pred_reshaped = pred_reshaped[inds]\n",
    "\n",
    "assert len(pred_reshaped) == len(output_data)\n",
    "samples_total = len(pred_reshaped)\n",
    "\n",
    "# Split into training/validation sets\n",
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])\n",
    "    \n",
    "# Validation data\n",
    "output_train = output_data[training_folds[1]]\n",
    "output_valid = output_data[validation_folds[1]]\n",
    "preds_train = pred_reshaped[training_folds[1]]\n",
    "preds_valid = pred_reshaped[validation_folds[1]]\n",
    "\n",
    "# Losses\n",
    "mse_train_unbounded = np.mean((output_train - preds_train)**2, dtype=np.float64)\n",
    "mse_valid_unbounded = np.mean((output_valid - preds_valid)**2, dtype=np.float64)\n",
    "mse_train_bounded = np.mean((output_train - np.minimum(np.maximum(preds_train, 0), 100))**2, dtype=np.float64)\n",
    "mse_valid_bounded = np.mean((output_valid - np.minimum(np.maximum(preds_valid, 0), 100))**2, dtype=np.float64)\n",
    "\n",
    "# Save output\n",
    "with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "    file.write('Unbounded training loss: %.4f\\n'%(mse_train_unbounded))\n",
    "    file.write('Unbounded validation loss: %.4f\\n'%(mse_valid_unbounded))\n",
    "    file.write('Bounded training loss: %.4f\\n'%(mse_train_bounded))\n",
    "    file.write('Bounded validation loss: %.4f\\n'%(mse_valid_bounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds",
   "language": "python",
   "name": "clouds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

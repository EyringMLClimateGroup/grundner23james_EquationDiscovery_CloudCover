{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "1. We read the data from the npy files\n",
    "2. We combine the QUBICC and NARVAL data\n",
    "4. Set up cross validation\n",
    "\n",
    "During cross-validation:\n",
    "\n",
    "1. We scale the data, convert to tf data\n",
    "2. Plot training progress, model biases \n",
    "3. Write losses and epochs into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "# Ran with 800GB (750GB should also be fine)\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "#Import sklearn before tensorflow (static Thread-local storage)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# For Leaky_ReLU:\n",
    "from tensorflow import nn \n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('PDF')\n",
    "\n",
    "t0 = time.time()\n",
    "path = '/home/b/b309170'\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, path + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "\n",
    "# Reloading custom file to incorporate changes dynamically\n",
    "import importlib\n",
    "import my_classes\n",
    "importlib.reload(my_classes)\n",
    "\n",
    "from my_classes import write_infofile\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import TimeOut\n",
    "\n",
    "# Cross-validation fold (in 0,1,2)\n",
    "fold = int(sys.argv[1])\n",
    "\n",
    "# 'all', 'no_spinup'\n",
    "days = 'no_spinup'\n",
    "\n",
    "# Minutes per fold\n",
    "timeout = 450 \n",
    "\n",
    "# Maximum amount of epochs for each model\n",
    "epochs = 30 \n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_visible_devices(gpus[3], 'GPU')\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Cover or Cloud Area?\n",
    "output_var = sys.argv[2] # Set output_var to one of {'cl_volume', 'cl_area'}\n",
    "\n",
    "path_base = os.path.join(path, 'workspace_icon-ml/cloud_cover_parameterization/grid_column_based_DYAMOND')\n",
    "path_data = os.path.join(path, 'my_work/icon-ml_data/cloud_cover_parameterization/grid_column_based_DYAMOND')\n",
    "    \n",
    "path_model = os.path.join(path_base, 'saved_models')\n",
    "path_figures = os.path.join(path_base, 'figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't run on a CPU node\n",
    "try:\n",
    "    # Prevents crashes of the code\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "    # Allow the growth of memory Tensorflow allocates (limits memory usage overall)\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.transpose(np.load(path_data + '/cloud_cover_input_dyamond.npy'))\n",
    "\n",
    "if output_var == 'cl_volume':\n",
    "    output_data = np.transpose(np.load(path_data + '/cloud_cover_output_dyamond.npy'))\n",
    "elif output_var == 'cl_area':\n",
    "    output_data = np.transpose(np.load(path_data + '/cloud_area_output_dyamond.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually, we have to remove the spinup here, if we want to have a model comparable to the other ones from symbolic regression!\n",
    "if days == 'no_spinup':\n",
    "    \n",
    "    t_steps = 619\n",
    "    h_fields = 79342\n",
    "    no_vars = 163\n",
    "    \n",
    "    ## For the input data\n",
    "    B = np.zeros((t_steps, no_vars, h_fields))\n",
    "    # Invert reshaping\n",
    "    for i in range(no_vars):\n",
    "        B[:, i] = np.reshape(input_data[:, i], (t_steps, h_fields))\n",
    "    # Discard spinup\n",
    "    input_data = np.concatenate((B[80:329], B[(329+72):]), axis=0)\n",
    "    \n",
    "    # Reshape back\n",
    "    B = [np.reshape(input_data[:, i], -1) for i in range(no_vars)]\n",
    "    input_data = np.array(B).T\n",
    "    \n",
    "    no_vars = 27\n",
    "    \n",
    "    ## For the output data\n",
    "    B = np.zeros((t_steps, no_vars, h_fields))\n",
    "    # Invert reshaping\n",
    "    for i in range(no_vars):\n",
    "        B[:, i] = np.reshape(output_data[:, i], (t_steps, h_fields))\n",
    "    # Discard spinup\n",
    "    output_data = np.concatenate((B[80:329], B[(329+72):]), axis=0)\n",
    "    \n",
    "    # Reshape back\n",
    "    B = [np.reshape(output_data[:, i], -1) for i in range(no_vars)]\n",
    "    output_data = np.array(B).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37052714, 163)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(samples_total, no_of_features) = input_data.shape\n",
    "(samples_total, no_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Temporal cross-validation*\n",
    "\n",
    "Split into 2-weeks increments (when working with 3 months of data). It's 25 day increments with 5 months of data. <br>\n",
    "1.: Validate on increments 1 and 4 <br>\n",
    "2.: Validate on increments 2 and 5 <br>\n",
    "3.: Validate on increments 3 and 6\n",
    "\n",
    "--> 2/3 training data, 1/3 validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that are constant in at least one of the training folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove_fields = []\n",
    "# constant_0 = (np.max(input_data[training_folds[0]], axis=0) - np.min(input_data[training_folds[0]], axis=0) < 1e-6)\n",
    "# constant_1 = (np.max(input_data[training_folds[1]], axis=0) - np.min(input_data[training_folds[1]], axis=0) < 1e-6)\n",
    "# constant_2 = (np.max(input_data[training_folds[2]], axis=0) - np.min(input_data[training_folds[2]], axis=0) < 1e-6)\n",
    "# for i in range(no_of_features):\n",
    "#     if constant_0[i] or constant_1[i] or constant_2[i]:\n",
    "#         print(i)\n",
    "#         remove_fields.append(i)\n",
    "\n",
    "# remove_fields\n",
    "\n",
    "remove_fields = [27, 28, 29, 30, 31, 32, 135, 136, 137]\n",
    "assert no_of_features == 163\n",
    "input_data = np.delete(input_data, remove_fields, axis=1)\n",
    "no_of_features = no_of_features - len(remove_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function for the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "                [\n",
    "                    tf.keras.layers.Dense(256, activation='relu', input_dim = no_of_features),\n",
    "                    tf.keras.layers.Dense(256, activation='relu'),\n",
    "                    tf.keras.layers.Dense(27, activation='linear', dtype='float32'),\n",
    "                ],\n",
    "                name=\"column_based_model\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 3-fold cross-validation\n",
    "\n",
    "When the training is lost in a local minimum, often a re-run helps with a different initialization of the model weights.\n",
    "Or possibly a different shuffling seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By decreasing timeout we make sure every fold gets the same amount of time\n",
    "# After all, data-loading took some time (Have 3 folds, 60 seconds/minute)\n",
    "# timeout = timeout - 1/3*1/60*(time.time() - t0)\n",
    "timeout = timeout - 1/60*(time.time() - t0)\n",
    "t0 = time.time()\n",
    "\n",
    "if days == 'all':\n",
    "    filename = 'cross_validation_column_based_%s_fold_%d'%(output_var, fold+1)\n",
    "elif days == 'no_spinup':\n",
    "    filename = 'cross_validation_column_based_%s_fold_%d_no_spinup'%(output_var, fold+1)\n",
    "\n",
    "#Standardize according to the fold\n",
    "scaler.fit(input_data[training_folds[fold]])\n",
    "\n",
    "# Write the accompanying info-file [only once]\n",
    "if not os.path.exists(os.path.join(path_model, filename + '.txt')):\n",
    "    # We save the scaling parameters in a file [only once]\n",
    "    if output_var == 'cl_volume':\n",
    "        seed_i = int(str(0) + str(fold))\n",
    "    elif output_var == 'cl_area':\n",
    "        seed_i = int(str(1) + str(fold))\n",
    "    with open(path_model+'/scaler_%d.txt'%seed_i, 'a') as file:\n",
    "        file.write('Standard Scaler mean values:\\n')\n",
    "        file.write(str(scaler.mean_))\n",
    "        file.write('\\nStandard Scaler standard deviation:\\n')\n",
    "        file.write(str(np.sqrt(scaler.var_)))\n",
    "    # Taken from preprocessing\n",
    "    input_variables = []\n",
    "    variables = ['hus', 'clw', 'cli', 'ta', 'pa', 'zg']\n",
    "    for el in variables:\n",
    "        for i in range(21, 48):\n",
    "            input_variables.append(el+'_%d'%i)\n",
    "    input_variables.append('fr_land')\n",
    "    np.delete(input_variables, remove_fields)\n",
    "    \n",
    "    in_and_out_variables = input_variables.copy()\n",
    "    variables = [output_var]\n",
    "    for el in variables:\n",
    "        for i in range(21, 48):\n",
    "            in_and_out_variables.append(el+'_%d'%i)\n",
    "    with open(os.path.join(path_model, filename + '.txt'), 'a') as file:\n",
    "        write_infofile(file, str(in_and_out_variables), str(input_variables), path_model, path_data, seed_i)\n",
    "\n",
    "#Load the data for the respective fold and convert it to tf data\n",
    "input_train = scaler.transform(input_data[training_folds[fold]])\n",
    "input_valid = scaler.transform(input_data[validation_folds[fold]]) \n",
    "output_train = output_data[training_folds[fold]]\n",
    "output_valid = output_data[validation_folds[fold]]\n",
    "\n",
    "# Clear memory (Reduces memory requirement to 151 GB)\n",
    "del input_data, output_data, first_incr, second_incr, validation_folds, training_folds\n",
    "gc.collect()\n",
    "\n",
    "#Feed the model. Increase the learning rate by a factor of 2 when increasing the batch size by a factor of 4\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "#Train the model\n",
    "#     time_callback = TimeOut(t0, timeout*(i+1))\n",
    "time_callback = TimeOut(t0, timeout)\n",
    "history = model.fit(input_train, output_train, epochs=epochs, verbose=2, batch_size=128,\n",
    "                    validation_data=(input_valid, output_valid), callbacks=[time_callback])\n",
    "#     history = model.fit(train_ds, epochs=epochs, validation_data=valid_ds, callbacks=[time_callback])\n",
    "\n",
    "#Save the model     \n",
    "#Serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(os.path.join(path_model, filename+\".yaml\"), \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "#Serialize model and weights to a single HDF5-file\n",
    "model.save(os.path.join(path_model, filename+'.h5'), \"w\")\n",
    "print('Saved model to disk')\n",
    "\n",
    "#Plot the training history\n",
    "if len(history.history['loss']) > len(history.history['val_loss']):\n",
    "    del history.history['loss'][-1]\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.savefig(os.path.join(path_figures, filename+'.pdf'))\n",
    "\n",
    "with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "    file.write('Results from the %d-th fold\\n'%(fold+1))\n",
    "    file.write('Training epochs: %d\\n'%(len(history.history['val_loss'])))\n",
    "    file.write('Weights restored from epoch: %d\\n\\n'%(1+np.argmin(history.history['val_loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (based on the module python3/2022.01)",
   "language": "python",
   "name": "python3_2022_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

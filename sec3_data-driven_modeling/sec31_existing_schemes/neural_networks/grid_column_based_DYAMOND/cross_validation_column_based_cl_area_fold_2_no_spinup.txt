How to use the model:
model = tensorflow.keras.models.load_model(filename+'.h5')
model.predict(scaled input data)

Input/Output
------------
Input and output variables:
['hus_21', 'hus_22', 'hus_23', 'hus_24', 'hus_25', 'hus_26', 'hus_27', 'hus_28', 'hus_29', 'hus_30', 'hus_31', 'hus_32', 'hus_33', 'hus_34', 'hus_35', 'hus_36', 'hus_37', 'hus_38', 'hus_39', 'hus_40', 'hus_41', 'hus_42', 'hus_43', 'hus_44', 'hus_45', 'hus_46', 'hus_47', 'clw_27', 'clw_28', 'clw_29', 'clw_30', 'clw_31', 'clw_32', 'clw_33', 'clw_34', 'clw_35', 'clw_36', 'clw_37', 'clw_38', 'clw_39', 'clw_40', 'clw_41', 'clw_42', 'clw_43', 'clw_44', 'clw_45', 'clw_46', 'clw_47', 'cli_21', 'cli_22', 'cli_23', 'cli_24', 'cli_25', 'cli_26', 'cli_27', 'cli_28', 'cli_29', 'cli_30', 'cli_31', 'cli_32', 'cli_33', 'cli_34', 'cli_35', 'cli_36', 'cli_37', 'cli_38', 'cli_39', 'cli_40', 'cli_41', 'cli_42', 'cli_43', 'cli_44', 'cli_45', 'cli_46', 'cli_47', 'ta_21', 'ta_22', 'ta_23', 'ta_24', 'ta_25', 'ta_26', 'ta_27', 'ta_28', 'ta_29', 'ta_30', 'ta_31', 'ta_32', 'ta_33', 'ta_34', 'ta_35', 'ta_36', 'ta_37', 'ta_38', 'ta_39', 'ta_40', 'ta_41', 'ta_42', 'ta_43', 'ta_44', 'ta_45', 'ta_46', 'ta_47', 'pa_21', 'pa_22', 'pa_23', 'pa_24', 'pa_25', 'pa_26', 'pa_27', 'pa_28', 'pa_29', 'pa_30', 'pa_31', 'pa_32', 'pa_33', 'pa_34', 'pa_35', 'pa_36', 'pa_37', 'pa_38', 'pa_39', 'pa_40', 'pa_41', 'pa_42', 'pa_43', 'pa_44', 'pa_45', 'pa_46', 'pa_47', 'zg_24', 'zg_25', 'zg_26', 'zg_27', 'zg_28', 'zg_29', 'zg_30', 'zg_31', 'zg_32', 'zg_33', 'zg_34', 'zg_35', 'zg_36', 'zg_37', 'zg_38', 'zg_39', 'zg_40', 'zg_41', 'zg_42', 'zg_43', 'zg_44', 'zg_45', 'zg_46', 'zg_47', 'fr_land', 'cl_area_21', 'cl_area_22', 'cl_area_23', 'cl_area_24', 'cl_area_25', 'cl_area_26', 'cl_area_27', 'cl_area_28', 'cl_area_29', 'cl_area_30', 'cl_area_31', 'cl_area_32', 'cl_area_33', 'cl_area_34', 'cl_area_35', 'cl_area_36', 'cl_area_37', 'cl_area_38', 'cl_area_39', 'cl_area_40', 'cl_area_41', 'cl_area_42', 'cl_area_43', 'cl_area_44', 'cl_area_45', 'cl_area_46', 'cl_area_47']
The (order of) input variables:
['hus_21', 'hus_22', 'hus_23', 'hus_24', 'hus_25', 'hus_26', 'hus_27', 'hus_28', 'hus_29', 'hus_30', 'hus_31', 'hus_32', 'hus_33', 'hus_34', 'hus_35', 'hus_36', 'hus_37', 'hus_38', 'hus_39', 'hus_40', 'hus_41', 'hus_42', 'hus_43', 'hus_44', 'hus_45', 'hus_46', 'hus_47', 'clw_27', 'clw_28', 'clw_29', 'clw_30', 'clw_31', 'clw_32', 'clw_33', 'clw_34', 'clw_35', 'clw_36', 'clw_37', 'clw_38', 'clw_39', 'clw_40', 'clw_41', 'clw_42', 'clw_43', 'clw_44', 'clw_45', 'clw_46', 'clw_47', 'cli_21', 'cli_22', 'cli_23', 'cli_24', 'cli_25', 'cli_26', 'cli_27', 'cli_28', 'cli_29', 'cli_30', 'cli_31', 'cli_32', 'cli_33', 'cli_34', 'cli_35', 'cli_36', 'cli_37', 'cli_38', 'cli_39', 'cli_40', 'cli_41', 'cli_42', 'cli_43', 'cli_44', 'cli_45', 'cli_46', 'cli_47', 'ta_21', 'ta_22', 'ta_23', 'ta_24', 'ta_25', 'ta_26', 'ta_27', 'ta_28', 'ta_29', 'ta_30', 'ta_31', 'ta_32', 'ta_33', 'ta_34', 'ta_35', 'ta_36', 'ta_37', 'ta_38', 'ta_39', 'ta_40', 'ta_41', 'ta_42', 'ta_43', 'ta_44', 'ta_45', 'ta_46', 'ta_47', 'pa_21', 'pa_22', 'pa_23', 'pa_24', 'pa_25', 'pa_26', 'pa_27', 'pa_28', 'pa_29', 'pa_30', 'pa_31', 'pa_32', 'pa_33', 'pa_34', 'pa_35', 'pa_36', 'pa_37', 'pa_38', 'pa_39', 'pa_40', 'pa_41', 'pa_42', 'pa_43', 'pa_44', 'pa_45', 'pa_46', 'pa_47', 'zg_24', 'zg_25', 'zg_26', 'zg_27', 'zg_28', 'zg_29', 'zg_30', 'zg_31', 'zg_32', 'zg_33', 'zg_34', 'zg_35', 'zg_36', 'zg_37', 'zg_38', 'zg_39', 'zg_40', 'zg_41', 'zg_42', 'zg_43', 'zg_44', 'zg_45', 'zg_46', 'zg_47', 'fr_land']

Scaling
-------
Standard Scaler mean values:
[2.80224432e-06 2.77506371e-06 2.81805496e-06 3.15574111e-06
 4.77383656e-06 1.07029180e-05 2.75587008e-05 6.54330409e-05
 1.34666182e-04 2.49775104e-04 4.19248455e-04 6.69996869e-04
 9.93352893e-04 1.39770273e-03 1.87953760e-03 2.42471752e-03
 3.05062500e-03 3.79166397e-03 4.66444863e-03 5.65993509e-03
 6.66116834e-03 7.58761859e-03 8.43548637e-03 9.10535976e-03
 9.47004644e-03 9.63894229e-03 9.78731767e-03 2.25734134e-11
 1.07990438e-08 1.10932175e-07 2.54386872e-07 4.77288985e-07
 9.99310953e-07 2.20012251e-06 4.22388367e-06 6.48495663e-06
 8.15088351e-06 1.07584341e-05 1.43790020e-05 1.91827409e-05
 2.55964139e-05 3.13555398e-05 3.28809813e-05 2.92315621e-05
 1.74208497e-05 5.71368840e-06 1.82757943e-06 8.89108395e-07
 7.90724163e-11 1.11098165e-10 2.31696866e-09 5.46298272e-08
 3.90496165e-07 1.30123033e-06 2.45502430e-06 3.12755953e-06
 3.01517611e-06 2.58487060e-06 2.41748929e-06 2.26389120e-06
 2.03474789e-06 1.73227001e-06 1.38044838e-06 1.13983360e-06
 9.77862777e-07 8.54329144e-07 7.68939890e-07 7.20441123e-07
 7.04097085e-07 6.89786867e-07 6.37473168e-07 5.10639146e-07
 3.25634400e-07 1.82001795e-07 1.21589306e-07 2.10915755e+02
 2.07422182e+02 2.04678403e+02 2.04787267e+02 2.08057537e+02
 2.12882497e+02 2.18208002e+02 2.24058860e+02 2.30394444e+02
 2.37106777e+02 2.43576462e+02 2.49837685e+02 2.55280144e+02
 2.60131863e+02 2.64462955e+02 2.68308195e+02 2.71664215e+02
 2.74514794e+02 2.76893307e+02 2.78821695e+02 2.80423072e+02
 2.81930886e+02 2.83385821e+02 2.84741347e+02 2.85926879e+02
 2.86750358e+02 2.87042258e+02 4.74339405e+03 6.22761960e+03
 8.02922027e+03 1.02577968e+04 1.29289271e+04 1.60255364e+04
 1.95246109e+04 2.34142971e+04 2.76094654e+04 3.21808994e+04
 3.68798109e+04 4.19443590e+04 4.69666639e+04 5.20651250e+04
 5.72016316e+04 6.22840003e+04 6.72523522e+04 7.20517628e+04
 7.66555181e+04 8.10147664e+04 8.49538106e+04 8.84927605e+04
 9.15846158e+04 9.41865496e+04 9.63349590e+04 9.78091661e+04
 9.85373098e+04 1.61343496e+04 1.47416995e+04 1.34231662e+04
 1.21770560e+04 1.10014816e+04 9.89472646e+03 8.85530308e+03
 7.88190237e+03 6.97320374e+03 6.12783552e+03 5.34435336e+03
 4.62123738e+03 3.95696638e+03 3.34999845e+03 2.79882064e+03
 2.30209430e+03 1.85879431e+03 1.46813759e+03 1.12951633e+03
 8.42621232e+02 6.07523481e+02 4.24851433e+02 2.96607112e+02
 2.26444809e+02 2.63982730e-01]
Standard Scaler standard deviation:
[2.62078738e-07 3.49493873e-07 5.81412938e-07 8.84274536e-07
 2.04526528e-06 7.99638355e-06 2.67317268e-05 7.20658960e-05
 1.60508136e-04 3.10741806e-04 5.20304543e-04 8.03423767e-04
 1.13565718e-03 1.51159982e-03 1.90820764e-03 2.29881658e-03
 2.70718350e-03 3.14239284e-03 3.57634822e-03 3.97217184e-03
 4.33109614e-03 4.69603903e-03 5.11437161e-03 5.51420738e-03
 5.71831738e-03 5.80536506e-03 5.88293739e-03 3.85995962e-09
 1.97586586e-07 1.10572606e-06 2.09556159e-06 3.37240466e-06
 5.76136251e-06 1.02763380e-05 1.75463524e-05 2.57464259e-05
 3.08167336e-05 3.66931357e-05 4.31764897e-05 5.00518738e-05
 5.76457839e-05 6.35443580e-05 6.31581258e-05 5.80157227e-05
 4.60348950e-05 2.74383203e-05 1.57778498e-05 1.13921888e-05
 4.54680935e-09 6.05932731e-09 1.76721078e-07 1.53109555e-06
 4.67760087e-06 8.51295999e-06 1.08917215e-05 1.14518169e-05
 1.04938160e-05 9.18376710e-06 8.60000673e-06 7.89396245e-06
 7.01116195e-06 6.04281652e-06 5.09093283e-06 4.38800942e-06
 3.82535717e-06 3.37065381e-06 3.01601748e-06 2.73428328e-06
 2.53476851e-06 2.40282966e-06 2.27590708e-06 2.00609144e-06
 1.58844825e-06 1.12075772e-06 8.89415381e-07 7.59711010e+00
 8.71969877e+00 1.02109946e+01 1.03693874e+01 8.58587557e+00
 6.37351979e+00 6.13143379e+00 8.26717870e+00 1.06772104e+01
 1.24333992e+01 1.32456565e+01 1.34385967e+01 1.33012800e+01
 1.30514494e+01 1.28111849e+01 1.26620503e+01 1.25993172e+01
 1.25943802e+01 1.26556431e+01 1.27763087e+01 1.29779264e+01
 1.32902823e+01 1.37006705e+01 1.42001889e+01 1.47714294e+01
 1.52365116e+01 1.55852987e+01 3.55675756e+02 4.46934624e+02
 5.74813859e+02 7.63067403e+02 1.00704580e+03 1.27585417e+03
 1.53441366e+03 1.75379242e+03 1.91572053e+03 2.02474949e+03
 2.09119104e+03 2.15290356e+03 2.22782019e+03 2.35082934e+03
 2.53541724e+03 2.79611175e+03 3.12108752e+03 3.49502250e+03
 3.91050870e+03 4.35477729e+03 4.77782814e+03 5.18454389e+03
 5.55875682e+03 5.88539107e+03 6.16117954e+03 6.34340435e+03
 6.44442977e+03 1.70291990e+00 4.59272540e+00 7.61410539e+00
 1.21769668e+01 1.88176364e+01 2.81605143e+01 4.16273829e+01
 6.14613631e+01 8.88563376e+01 1.23559213e+02 1.63941836e+02
 2.07832219e+02 2.53696387e+02 3.00012144e+02 3.45128161e+02
 3.87807019e+02 4.27756475e+02 4.64818159e+02 4.98349023e+02
 5.27689756e+02 5.52131235e+02 5.70908573e+02 5.84124321e+02
 5.89474723e+02 4.28362748e-01]
=> Apply this standard scaling to (only) the input data before processing.

Preprocessed data
-----------------
/home/b/b309170/my_work/icon-ml_data/cloud_cover_parameterization/grid_column_based_DYAMOND/cloud_cover_input_train_11.npy
/home/b/b309170/my_work/icon-ml_data/cloud_cover_parameterization/grid_column_based_DYAMOND/cloud_cover_input_valid_11.npy
/home/b/b309170/my_work/icon-ml_data/cloud_cover_parameterization/grid_column_based_DYAMOND/cloud_cover_output_train_11.npy
/home/b/b309170/my_work/icon-ml_data/cloud_cover_parameterization/grid_column_based_DYAMOND/cloud_cover_output_valid_11.npy
/home/b/b309170/my_work/icon-ml_data/cloud_cover_parameterization/grid_column_based_DYAMOND/cloud_cover_input_test_11.npy
/home/b/b309170/my_work/icon-ml_data/cloud_cover_parameterization/grid_column_based_DYAMOND/cloud_cover_output_test_11.npy

Model
-----
Results from the 2-th fold
Training epochs: 30
Weights restored from epoch: 30

Unbounded training loss: 33.4556
Unbounded validation loss: 34.3128
Bounded training loss: 32.7174
Bounded validation loss: 33.3721

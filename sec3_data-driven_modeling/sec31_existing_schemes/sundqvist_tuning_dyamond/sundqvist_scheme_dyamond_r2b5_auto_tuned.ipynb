{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sundqvist Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we fit the Sundqvist model where we fit the tuning parameters to the data? <br>\n",
    "We let the parameters depend on whether they are taken over land or over the sea.\n",
    "\n",
    "In this version, we find the optimal set of hyperparameters automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 samples, grid_spacing of 0.2: 12 seconds\n",
    "# 1000 samples, grid_spacing of 0.1: 130 seconds\n",
    "\n",
    "# 100.000 samples, grid_spacing of 0.2: 850 seconds\n",
    "# 100.000 samples, grid_spacing of 0.1: Should take 2-3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "sys.path.insert(0, '/home/b/b309170/workspace_icon-ml/symbolic_regression/')\n",
    "from functions import evaluate_sundqvist\n",
    "\n",
    "# Added to the PDF name\n",
    "ran = np.random.randint(10**3)\n",
    "print(ran)\n",
    "\n",
    "output_var = sys.argv[1] \n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load columns of data\n",
    "folder_data = '/home/b/b309170/my_work/icon-ml_data/cloud_cover_parameterization/neighborhood_based_SR_DYAMOND/'\n",
    "\n",
    "input_data = np.load(os.path.join(folder_data, 'cloud_cover_input_dyamond.npy'))\n",
    "if output_var == 'cl_volume':\n",
    "    output_data = np.load(os.path.join(folder_data, 'cloud_cover_output_dyamond.npy'))\n",
    "elif output_var == 'cl_area':\n",
    "    output_data = np.load(os.path.join(folder_data, 'cloud_area_output_dyamond.npy'))\n",
    "    \n",
    "vert_layers = np.load(os.path.join(folder_data, 'samples_vertical_layers_dyamond.npy'))\n",
    "\n",
    "new_features = ['hus', 'clw', 'cli', 'ta', 'pa', 'zg', 'fr_land', 'U', 'rh', 'ps', 'hus_z', 'hus_zz', 'clw_z', 'clw_zz', 'cli_z',\\\n",
    "                'cli_zz', 'ta_z', 'ta_zz', 'pa_z', 'pa_zz', 'U_z', 'U_zz', 'rh_z', 'rh_zz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_total, no_of_features = input_data.shape\n",
    "\n",
    "# Split into train/valid\n",
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])\n",
    "\n",
    "# The second fold yields the best model\n",
    "input_train = input_data[training_folds[1]]\n",
    "input_valid = input_data[validation_folds[1]]\n",
    "output_train = output_data[training_folds[1]]\n",
    "output_valid = output_data[validation_folds[1]]\n",
    "\n",
    "# Remove input_data, output_data\n",
    "del input_data, output_data, training_folds, validation_folds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190119664, 24)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To locate variables\n",
    "loc = {}\n",
    "for i in range(len(new_features)):\n",
    "    loc[new_features[i]] = i\n",
    "    \n",
    "input_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the training data into cells over land vs sea\n",
    "land_ind = np.where(input_train[:, loc['fr_land']] > 0.5)[0]\n",
    "sea_ind = np.where(input_train[:, loc['fr_land']] <= 0.5)[0]\n",
    "\n",
    "input_land = input_train[land_ind]\n",
    "output_land = output_train[land_ind]\n",
    "input_sea = input_train[sea_ind]\n",
    "output_sea = output_train[sea_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23899404745423913"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(land_ind)/input_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting hyperparameters\n",
    "Original ones: $r_{sat} = 1, r_{0, top} = 0.8, r_{0, surf} = 0.968, n = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sundq_Layer(tf.keras.layers.Layer):\n",
    "\n",
    "    # These are the output units\n",
    "    def __init__(self, units=1):\n",
    "        super(Sundq_Layer, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):  # Create the state of the layer (weights)\n",
    "        \n",
    "        # Initializing with the original values\n",
    "        # rsat must always be greater than r0_top and r0_surf! How could we enforce this? (*)\n",
    "        rsat_init = tf.constant_initializer(1)\n",
    "        r0_top_init = tf.constant_initializer(0.8)\n",
    "        r0_surf_init = tf.constant_initializer(0.968)\n",
    "        n_init = tf.constant_initializer(2)  \n",
    "    \n",
    "        self.rsat = tf.Variable(name='rsat', initial_value=rsat_init(shape=(1, self.units), dtype='float32'), trainable=True)\n",
    "        self.r0_top = tf.Variable(name='r0_top', initial_value=r0_top_init(shape=(1, self.units), dtype='float32'), trainable=True)\n",
    "        self.r0_surf = tf.Variable(name='r0_surf', initial_value=r0_surf_init(shape=(1, self.units), dtype='float32'), trainable=True)\n",
    "        self.n = tf.Variable(name='n', initial_value=n_init(shape=(1, self.units), dtype='float32'), trainable=True)\n",
    "\n",
    "    def call(self, inputs):  # Defines the computation from inputs to outputs\n",
    "        ps = inputs[:, 0]\n",
    "        p = inputs[:, 1]\n",
    "        rh = inputs[:, 2]\n",
    "        \n",
    "        r0 = self.r0_top + (self.r0_surf - self.r0_top)*tf.exp(1-(ps/p)**self.n)\n",
    "        \n",
    "        # div < 0, only if rsat < r0. But this goes against (*)\n",
    "        div = (tf.minimum(rh, self.rsat) - self.rsat)/(r0 - self.rsat)\n",
    "        \n",
    "        # tf.sqrt is tricky, because its gradient in 0 is infinite!\n",
    "        c = 1 - tf.sqrt(tf.maximum(div, 1e-9)) # in [0,1]\n",
    "        \n",
    "        # If rh > r0 we return c, otherwise we set it to 0\n",
    "        c_out = tf.maximum(tf.sign(rh - r0), 0)*c\n",
    "        \n",
    "        return 100*tf.transpose(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Land**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters from the hyperparameter search\n",
    "epochs_opt = 10\n",
    "batchsize_opt = 32\n",
    "optimizer_opt = tf.keras.optimizers.Adagrad\n",
    "lr_opt = 0.0523026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-19 17:18:53.498872: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-19 17:18:53.542349: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-19 17:18:53.542663: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-19 17:18:53.543308: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (l40033.atos.local): /proc/driver/nvidia/version does not exist\n",
      "2022-06-19 17:18:53.550318: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-19 17:18:53.559083: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "sundq_layer = Sundq_Layer()\n",
    "model = tf.keras.models.Sequential(sundq_layer)\n",
    "model.compile(optimizer=optimizer_opt(learning_rate=lr_opt), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1419921/1419921 - 601s - loss: 969.3347\n",
      "[array([[0.97819006]], dtype=float32), array([[0.05164935]], dtype=float32), array([[0.39053056]], dtype=float32), array([[1.3016738]], dtype=float32)]\n",
      "Epoch 2/10\n",
      "1419921/1419921 - 612s - loss: 969.3267\n",
      "[array([[0.97544396]], dtype=float32), array([[0.05206378]], dtype=float32), array([[0.39572528]], dtype=float32), array([[1.3008726]], dtype=float32)]\n",
      "Epoch 3/10\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(model.layers[0].get_weights()))\n",
    "\n",
    "inds = np.random.randint(0, input_land.shape[0], input_land.shape[0]//10)\n",
    "history = model.fit(input_land[inds][:, [loc['ps'], loc['pa'], loc['rh']]], output_land[inds], epochs=epochs_opt, batch_size=batchsize_opt, verbose=2, \\\n",
    "                   callbacks = [print_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsat, r0_top, r0_surf, n\n",
    "best_land = [model.weights[i].numpy()[0][0] for i in range(len(model.weights))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Trained the land model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sundq_layer = Sundq_Layer()\n",
    "model = tf.keras.models.Sequential(sundq_layer)\n",
    "model.compile(optimizer=optimizer_opt(learning_rate=lr_opt), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(model.layers[0].get_weights()))\n",
    "\n",
    "inds = np.random.randint(0, input_sea.shape[0], input_sea.shape[0]//10)\n",
    "history = model.fit(input_sea[inds][:, [loc['ps'], loc['pa'], loc['rh']]], output_sea[inds], epochs=epochs_opt, batch_size=batchsize_opt, verbose=2, \\\n",
    "                   callbacks = [print_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsat, r0_top, r0_surf, n\n",
    "best_sea = [model.weights[i].numpy()[0][0] for i in range(len(model.weights))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Trained the sea model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance with the best hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to JSON. In case we don't make it to the second-next cell.\n",
    "results = {}\n",
    "results['Order of parameters'] = 'rsat, r0_top, r0_surf, n'\n",
    "results['Best params land'] = str(best_land)\n",
    "results['Best params sea'] = str(best_sea)\n",
    "\n",
    "with open('/home/b/b309170/workspace_icon-ml/symbolic_regression/baselines/sundqvist_tuning_dyamond/results_auto_tuned_%s.json'%output_var, 'w') as file:\n",
    "    json.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiate between original, manually and automatically tuned!\n",
    "mse_train = evaluate_sundqvist(input_train, output_train, loc, tuned='custom', best_land=best_land, best_sea=best_sea, compute_r2=False)\n",
    "mse_train_land = evaluate_sundqvist(input_land, output_land, loc, tuned='custom', best_land=best_land, best_sea=best_sea, compute_r2=False)\n",
    "mse_train_sea = evaluate_sundqvist(input_sea, output_sea, loc, tuned='custom', best_land=best_land, best_sea=best_sea, compute_r2=False)\n",
    "mse_valid = evaluate_sundqvist(input_valid, output_valid, loc, tuned='custom', best_land=best_land, best_sea=best_sea, compute_r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to JSON\n",
    "results = {}\n",
    "results['Order of parameters'] = 'rsat, r0_top, r0_surf, n'\n",
    "results['Best params land'] = str(best_land)\n",
    "results['Best params sea'] = str(best_sea)\n",
    "results['Training MSE'] = mse_train\n",
    "results['Land MSE'] = mse_train_land\n",
    "results['Sea MSE'] = mse_train_sea\n",
    "results['Validation MSE'] = mse_valid\n",
    "\n",
    "with open('/home/b/b309170/workspace_icon-ml/symbolic_regression/baselines/sundqvist_tuning_dyamond/results_auto_tuned_%s.json'%output_var, 'w') as file:\n",
    "    json.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(output_land, bins=40, log=True)\n",
    "# plt.hist(predictions, bins=40, log = True)\n",
    "# plt.legend(['Truth: QUBICC over land', 'Original Sundqvist Scheme'])\n",
    "# plt.title('Cloud Cover distributions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (based on the module python3/2022.01)",
   "language": "python",
   "name": "python3_2022_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

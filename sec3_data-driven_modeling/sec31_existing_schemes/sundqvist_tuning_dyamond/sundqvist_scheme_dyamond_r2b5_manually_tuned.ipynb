{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sundqvist Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we fit the Sundqvist model where we fit the tuning parameters to the data? <br>\n",
    "We let the parameters depend on whether they are taken over land or over the sea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All samples (190119664): 5.5s per entry of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, '~/workspace_icon-ml/symbolic_regression/')\n",
    "from functions import evaluate_sundqvist\n",
    "\n",
    "# Grid search space of hyperparameters\n",
    "grid_spacing = 0.05\n",
    "\n",
    "# Shall we use the tuned hyperparameters?\n",
    "tuned = False\n",
    "\n",
    "# Added to the PDF name\n",
    "hour_min = '%d_%d'%(datetime.datetime.now().hour, datetime.datetime.now().minute)\n",
    "\n",
    "output_var = sys.argv[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load columns of data\n",
    "folder_data = '~/my_work/icon-ml_data/cloud_cover_parameterization/neighborhood_based_SR_DYAMOND/'\n",
    "\n",
    "input_data = np.load(os.path.join(folder_data, 'cloud_cover_input_dyamond.npy'))\n",
    "if output_var == 'cl_volume':\n",
    "    output_data = np.load(os.path.join(folder_data, 'cloud_cover_output_dyamond.npy'))\n",
    "elif output_var == 'cl_area':\n",
    "    output_data = np.load(os.path.join(folder_data, 'cloud_area_output_dyamond.npy'))\n",
    "\n",
    "new_features = ['hus', 'clw', 'cli', 'ta', 'pa', 'zg', 'fr_land', 'U', 'rh', 'ps', 'hus_z', 'hus_zz', 'clw_z', 'clw_zz', 'cli_z',\\\n",
    "                'cli_zz', 'ta_z', 'ta_zz', 'pa_z', 'pa_zz', 'U_z', 'U_zz', 'rh_z', 'rh_zz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_total, no_of_features = input_data.shape\n",
    "\n",
    "# Split into train/valid\n",
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])\n",
    "\n",
    "# The second fold yields the best model\n",
    "input_train = input_data[training_folds[1]]\n",
    "input_valid = input_data[validation_folds[1]]\n",
    "output_train = output_data[training_folds[1]]\n",
    "output_valid = output_data[validation_folds[1]]\n",
    "\n",
    "# Remove input_data, output_data\n",
    "del input_data, output_data, training_folds, validation_folds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190119664, 24)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To locate variables\n",
    "loc = {}\n",
    "for i in range(len(new_features)):\n",
    "    loc[new_features[i]] = i\n",
    "    \n",
    "input_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the training data into cells over land vs sea\n",
    "land_ind = np.where(input_train[:, loc['fr_land']] > 0.5)[0]\n",
    "sea_ind = np.where(input_train[:, loc['fr_land']] <= 0.5)[0]\n",
    "\n",
    "input_land = input_train[land_ind]\n",
    "output_land = output_train[land_ind]\n",
    "input_sea = input_train[sea_ind]\n",
    "output_sea = output_train[sea_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23899404745423913"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(land_ind)/input_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-11-6ca57ad496a8>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-6ca57ad496a8>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    n_range_sea = np.arange(0.6, 2.2, grid_spacing)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# rsat actually shouldn't really ever be smaller than r0_top or r0_surf\n",
    "\n",
    "# # First try (0.08):\n",
    "# rsat_range_land = np.arange(0.9, 1.2, grid_spacing)\n",
    "# r0_top_range_land = np.arange(0.2, 0.9, grid_spacing)\n",
    "# r0_surf_range_land = np.arange(0.6, 1, grid_spacing)\n",
    "# n_range_land = np.arange(0.6, 2.2, grid_spacing)\n",
    "\n",
    "# rsat_range_sea = np.arange(0.9, 1.2, grid_spacing)\n",
    "# r0_top_range_sea = np.arange(0.2, 0.9, grid_spacing)\n",
    "# r0_surf_range_sea = np.arange(0.6, 1, grid_spacing)\n",
    "# n_range_sea = np.arange(0.6, 2.2, grid_spacing)\n",
    "\n",
    "# --> Ran for six hours\n",
    "\n",
    "# Second try (0.05):\n",
    "if output_var == 'cl_area':\n",
    "    rsat_range_land = np.arange(0.75, 0.9, grid_spacing)\n",
    "elif output_var == 'cl_volume':\n",
    "    rsat_range_land = np.arange(0.9, 1.05, grid_spacing)\n",
    "r0_top_range_land = np.arange(0.01, 0.3, grid_spacing)\n",
    "r0_surf_range_land = np.arange(0.4, 0.6, grid_spacing)\n",
    "n_range_land = np.arange(2.12, 3, grid_spacing)\n",
    "\n",
    "rsat_range_sea = np.arange(0.9, 1.05, grid_spacing)\n",
    "r0_top_range_sea = np.arange(0.01, 0.3, grid_spacing)\n",
    "if output_var == 'cl_area':\n",
    "    r0_surf_range_sea = np.arange(0.4, 0.6, grid_spacing)\n",
    "elif output_var == 'cl_volume':\n",
    "    r0_surf_range_sea = np.arange(0.8, 0.95, grid_spacing)\n",
    "n_range_sea = np.arange(2.12, 3, grid_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated required time to run the notebook: 5.5 hours\n"
     ]
    }
   ],
   "source": [
    "# Estimated required time to run the notebook in hours\n",
    "# Factor of 2 instead of len(rsat_range_land) due to multiprocessing\n",
    "print('Estimated required time to run the notebook: %.1f hours'%((5.5*(2*len(r0_top_range_sea)*len(r0_surf_range_sea)*len(n_range_sea) +\\\n",
    "     2*len(r0_top_range_land)*len(r0_surf_range_land)*len(n_range_land)))/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hyperparams_outer_land(rsat):\n",
    "    return search_hyperparams_inner_land(rsat, r0_top_range_land, r0_surf_range_land, n_range_land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hyperparams_outer_sea(rsat):\n",
    "    return search_hyperparams_inner_sea(rsat, r0_top_range_sea, r0_surf_range_sea, n_range_sea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import multiprocessing as mlp\n",
    "import gc\n",
    "\n",
    "@contextmanager\n",
    "def poolcontext(*args, **kwargs):\n",
    "    pool = mlp.Pool(*args, **kwargs)\n",
    "    yield pool\n",
    "    pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hyperparams_inner_land(rsat, r0_top_range, r0_surf_range, n_range):\n",
    "    mse_tensor = -np.ones((1, len(r0_top_range), len(r0_surf_range), len(n_range)))\n",
    "    i2 = -1\n",
    "    for r0_top in r0_top_range:\n",
    "        i3 = -1\n",
    "        i2 += 1\n",
    "        for r0_surf in r0_surf_range:\n",
    "            i4 = -1\n",
    "            i3 += 1\n",
    "            for n in n_range:\n",
    "                i4 += 1\n",
    "                # What is the average error with this set of tuning parameters?\n",
    "                ps = input_land[:, loc['ps']]\n",
    "                p = input_land[:, loc['pa']]\n",
    "                r = input_land[:, loc['rh']]\n",
    "                try:\n",
    "                    r0 = r0_top + (r0_surf - r0_top)*np.exp(1-(ps/p)**n)\n",
    "                    c = np.where(r>r0, 1-np.sqrt((np.minimum(r, rsat) - rsat)/(r0 - rsat)), 0)\n",
    "                except:\n",
    "                    c = 2\n",
    "\n",
    "                mse_tensor[0, i2, i3, i4] = np.mean((100*c - output_land)**2)\n",
    "                    \n",
    "    return mse_tensor\n",
    "\n",
    "def search_hyperparams_inner_sea(rsat, r0_top_range, r0_surf_range, n_range):\n",
    "    mse_tensor = -np.ones((1, len(r0_top_range), len(r0_surf_range), len(n_range)))\n",
    "    i2 = -1\n",
    "    for r0_top in r0_top_range:\n",
    "        i3 = -1\n",
    "        i2 += 1\n",
    "        for r0_surf in r0_surf_range:\n",
    "            i4 = -1\n",
    "            i3 += 1\n",
    "            for n in n_range:\n",
    "                i4 += 1\n",
    "                # What is the average error with this set of tuning parameters?\n",
    "                ps = input_sea[:, loc['ps']]\n",
    "                p = input_sea[:, loc['pa']]\n",
    "                r = input_sea[:, loc['rh']]\n",
    "                try:\n",
    "                    r0 = r0_top + (r0_surf - r0_top)*np.exp(1-(ps/p)**n)\n",
    "                    c = np.where(r>r0, 1-np.sqrt((np.minimum(r, rsat) - rsat)/(r0 - rsat)), 0)\n",
    "                except:\n",
    "                    c = 2\n",
    "\n",
    "                mse_tensor[0, i2, i3, i4] = np.mean((100*c - output_sea)**2)\n",
    "                    \n",
    "    return mse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting hyperparameters\n",
    "Originally: $r_{sat} = 1, r_{0, top} = 0.8, r_{0, surf} = 0.968, n = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Land**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = len(rsat_range_land)\n",
    "with poolcontext(processes=procs) as pool:\n",
    "    # Every process received a part of data_dict\n",
    "    mse_tensor_land = pool.map(search_hyperparams_outer_land, rsat_range_land)\n",
    "    \n",
    "mse_tensor_land = np.squeeze(np.array(mse_tensor_land))\n",
    "                \n",
    "# assert np.all(mse_tensor_land >= 0)\n",
    "np.save('~/workspace_icon-ml/symbolic_regression/baselines/sundqvist_tuning_dyamond/mse_tensor_land_%s.npy'%hour_min, mse_tensor_land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mse = 10**10\n",
    "opt_ind = []\n",
    "for i in range(mse_tensor_land.shape[0]):\n",
    "    for j in range(mse_tensor_land.shape[1]):\n",
    "        for k in range(mse_tensor_land.shape[2]):\n",
    "            for l in range(mse_tensor_land.shape[3]):\n",
    "                if mse_tensor_land[i,j,k,l] < min_mse:\n",
    "                    min_mse = mse_tensor_land[i,j,k,l]\n",
    "                    opt_ind = [i, j, k, l]                 \n",
    "                    \n",
    "with open('~/workspace_icon-ml/symbolic_regression/baselines/sundqvist_tuning_dyamond/best_results.txt', 'a') as file:\n",
    "    file.write('Time it took to get through land: %.3f\\n'%(time.time() - t0))\n",
    "    file.write('Output variable: %s\\n'%output_var)\n",
    "    file.write('Best values for the land part: %s\\n'%str([rsat_range_land[opt_ind[0]], r0_top_range_land[opt_ind[1]], r0_surf_range_land[opt_ind[2]], n_range_land[opt_ind[3]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = len(rsat_range_sea)\n",
    "with poolcontext(processes=procs) as pool:\n",
    "    # Every process received a part of data_dict\n",
    "    mse_tensor_sea = pool.map(search_hyperparams_outer_sea, rsat_range_sea)\n",
    "    \n",
    "mse_tensor_sea = np.squeeze(np.array(mse_tensor_sea))\n",
    "                \n",
    "# assert np.all(mse_tensor_land >= 0)\n",
    "np.save('~/workspace_icon-ml/symbolic_regression/baselines/sundqvist_tuning_dyamond/mse_tensor_sea_%s.npy'%hour_min, mse_tensor_sea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mse = 10**10\n",
    "opt_ind = []\n",
    "for i in range(mse_tensor_sea.shape[0]):\n",
    "    for j in range(mse_tensor_sea.shape[1]):\n",
    "        for k in range(mse_tensor_sea.shape[2]):\n",
    "            for l in range(mse_tensor_sea.shape[3]):\n",
    "                if mse_tensor_sea[i,j,k,l] < min_mse:\n",
    "                    min_mse = mse_tensor_sea[i,j,k,l]\n",
    "                    opt_ind = [i, j, k, l]                 \n",
    "                    \n",
    "with open('~/workspace_icon-ml/symbolic_regression/baselines/sundqvist_tuning_dyamond/best_results.txt', 'a') as file:\n",
    "    file.write('Time it took to get through the sea: %.3f\\n'%(time.time() - t0))\n",
    "    file.write('Output variable: %s\\n'%output_var)\n",
    "    file.write('Best values for the sea part: %s\\n'%str([rsat_range_sea[opt_ind[0]], r0_top_range_sea[opt_ind[1]], r0_surf_range_sea[opt_ind[2]], n_range_sea[opt_ind[3]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results (To run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse_tensor_land = np.load('~/workspace_icon-ml/symbolic_regression/baselines/sundqvist_tuning_narval_r2b4/mse_tensor_land_%d.npy'%ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.subplots_adjust(hspace=0.4)\n",
    "# plt.suptitle(\"Hyperparameter performance - land\", fontsize=18, y=1)\n",
    "\n",
    "# hyp_par = ['rsat', 'r0_top', 'r0_surf', 'n']\n",
    "# hyp_par_range = [rsat_range_land, r0_top_range_land, r0_surf_range_land, n_range_land]\n",
    "\n",
    "# axes = (0,1,2,3)\n",
    "# for i, par in enumerate(hyp_par):\n",
    "#     # Add new subplot iteratively\n",
    "#     ax = plt.subplot(2, 2, i + 1)\n",
    "    \n",
    "#     axis = axes[:i] + axes[(i+1):]\n",
    "#     ax.plot(hyp_par_range[i], np.min(mse_tensor_land, axis=axis))\n",
    "    \n",
    "#     ax.set_title(par)\n",
    "#     if i in [0, 2]:\n",
    "#         ax.set_ylabel('minimal MSE')\n",
    "\n",
    "# plt.savefig('~/workspace_icon-ml/symbolic_regression/baselines/sundqvist_tuning_narval_r2b4/hyp_land_%d.pdf'%hour_min, \\\n",
    "#             bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse_tensor_sea = np.load('~/workspace_icon-ml/symbolic_regression/baselines/sundqvist_tuning_narval_r2b4/mse_tensor_sea_%d.npy'%ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.subplots_adjust(hspace=0.4)\n",
    "# plt.suptitle(\"Hyperparameter performance - sea\", fontsize=18, y=1)\n",
    "\n",
    "# hyp_par = ['rsat', 'r0_top', 'r0_surf', 'n']\n",
    "# hyp_par_range = [rsat_range_sea, r0_top_range_sea, r0_surf_range_sea, n_range_sea]\n",
    "\n",
    "# axes = (0,1,2,3)\n",
    "# for i, par in enumerate(hyp_par):\n",
    "#     # Add new subplot iteratively\n",
    "#     ax = plt.subplot(2, 2, i + 1)\n",
    "    \n",
    "#     axis = axes[:i] + axes[(i+1):]\n",
    "#     ax.plot(hyp_par_range[i], np.min(mse_tensor_sea, axis=axis))\n",
    "    \n",
    "#     ax.set_title(par)\n",
    "#     if i in [0, 2]:\n",
    "#         ax.set_ylabel('minimal MSE')\n",
    "\n",
    "# plt.savefig('~/workspace_icon-ml/symbolic_regression/baselines/sundqvist_tuning_narval_r2b4/hyp_sea_%d.pdf'%hour_min, \\\n",
    "#            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance with the best hyperparameter setting (To run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned='manually'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Differentiate between original, manually and automatically tuned!\n",
    "# mse_train = evaluate_sundqvist(input_train, output_train, loc, tuned='custom', best_land=best_land, best_sea=best_sea, compute_r2=False)\n",
    "# mse_train_land = evaluate_sundqvist(input_land, output_land, loc, tuned='custom', best_land=best_land, best_sea=best_sea, compute_r2=False)\n",
    "# mse_train_sea = evaluate_sundqvist(input_sea, output_sea, loc, tuned='custom', best_land=best_land, best_sea=best_sea, compute_r2=False)\n",
    "# mse_valid = evaluate_sundqvist(input_valid, output_valid, loc, tuned='custom', best_land=best_land, best_sea=best_sea, compute_r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('~/workspace_icon-ml/symbolic_regression/baselines/sundqvist_results/manual_gradient_descent/results.txt', 'a') as file:\n",
    "#     file.write('With tuned hyperparameters: %s\\n'%tuned)\n",
    "#     file.write('Training score:\\n')\n",
    "#     file.write('MSE: %.3f, R2: %.3f\\n'%(mse_train, r2_train))\n",
    "#     file.write('Over land: \\n')\n",
    "#     file.write('MSE: %.3f, R2: %.3f\\n'%(mse_train_land, r2_train_land))\n",
    "#     file.write('Over sea:\\n')\n",
    "#     file.write('MSE: %.3f, R2: %.3f\\n'%(mse_train_sea, r2_train_sea))\n",
    "#     file.write('Validation score:\\n')\n",
    "#     file.write('MSE: %.3f, R2: %.3f\\n\\n'%(mse_valid, r2_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds",
   "language": "python",
   "name": "clouds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PySR Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DYAMOND cl_area_fraction**\n",
    "\n",
    "We set up PySR very carefully to only receive equations that satisfy certain physical constraints (hard/weak constraints) [that would have been nice but I cannot pass input features to the PySR loss function...] and are interpretable (i.e., tweak the complexity of operators individually). We are also careful about the sample of data pysr should learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Added (max(min(x,100),0) - y) to the loss function. We hope the model learns the remaining constraints by itself.\n",
    "- Use complexity settings for the individual operators to keep the model interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpysr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PySRRegressor\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 12/8/22: Changed from 10 to SEED. Added loss_exp.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(SEED) \n\u001b[1;32m     16\u001b[0m loss_exp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;66;03m# 3,4,5. Default is 2.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '-f'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import pysr\n",
    "import json\n",
    "import time\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import tree\n",
    "from pysr import PySRRegressor\n",
    "\n",
    "# 12/8/22: Changed from 10 to SEED. Added loss_exp.\n",
    "SEED = int(sys.argv[1])\n",
    "np.random.seed(SEED) \n",
    "loss_exp = int(sys.argv[2]) # 3,4,5. Default is 2.\n",
    "\n",
    "# Complexity of addition and multiplication. Try: [1,2,3]\n",
    "# Complexity of variables is always set to 1\n",
    "# verylow_ops_complexity = int(sys.argv[1])\n",
    "verylow_ops_complexity = 3\n",
    "\n",
    "# Try 1,2,3,4\n",
    "# no_of_regimes = int(sys.argv[2])\n",
    "no_of_regimes = 2\n",
    "# Which regime should we work with (pick from [1, ..., (no_of_regimes - 1)])\n",
    "# Set to 1 if no_of_regimes = 1\n",
    "# regime = int(sys.argv[3])\n",
    "regime = 1\n",
    "\n",
    "# Try 500,1000,5000,10000\n",
    "# subset_size = int(sys.argv[4])\n",
    "subset_size = 5000\n",
    "\n",
    "# 221108: Try eight input features\n",
    "no_features = 5\n",
    "\n",
    "# 221214: Without loc_issue?\n",
    "loc_issue = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_possible_features = ['hus', 'clw', 'cli', 'ta', 'pa', 'zg', 'fr_land', 'U', 'rh', 'ps', 'hus_z', 'hus_zz', 'clw_z', 'clw_zz', 'cli_z',\\\n",
    "            'cli_zz', 'ta_z', 'ta_zz', 'pa_z', 'pa_zz', 'U_z', 'U_zz', 'rh_z', 'rh_zz']\n",
    "\n",
    "loc_all = {}\n",
    "for i in range(len(all_possible_features)):\n",
    "    loc_all[all_possible_features[i]] = i\n",
    "    \n",
    "# Features\n",
    "if no_features == 5:\n",
    "    features = ['rh', 'ta', 'clw', 'cli', 'rh_z']\n",
    "elif no_features == 8:\n",
    "    features = ['rh', 'ta', 'clw', 'cli', 'rh_z', 'rh_zz', 'pa_z', 'pa_zz']\n",
    "    \n",
    "loc = {}\n",
    "for i in range(len(features)):\n",
    "    loc[features[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join('/home/b/b309170/my_work/icon-ml_data/cloud_cover_parameterization/neighborhood_based_SR_DYAMOND')\n",
    "\n",
    "# Load the input data and pick the five best features (rh, ta, clw, cli, rh_z)\n",
    "input_data = np.load(path_data + '/cloud_cover_input_dyamond.npy')\n",
    "input_data = np.concatenate([np.expand_dims(input_data[:, loc_all[sel_var]], axis=1) for sel_var in features], axis = 1)\n",
    "\n",
    "output_data = np.load(path_data + '/cloud_area_output_dyamond.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285179494, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(samples_total, no_of_features) = input_data.shape\n",
    "(samples_total, no_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct training and validation data\n",
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The second fold yields the best model\n",
    "flattened_input_train = input_data[training_folds[1]]\n",
    "flattened_input_valid = input_data[validation_folds[1]]\n",
    "flattened_output_train = output_data[training_folds[1]]\n",
    "flattened_output_valid = output_data[validation_folds[1]]\n",
    "    \n",
    "# Remove input_data, output_data\n",
    "del input_data, output_data, training_folds, validation_folds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the data into decision tree-based regimes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if no_of_regimes > 1:\n",
    "    # Already remove the regime with clw + cli = 0\n",
    "    if loc_issue:\n",
    "        reg_not_0_train = np.where(flattened_input_train[:, loc_all['clw']] + flattened_input_train[:, loc_all['cli']] > 1e-20)[0]\n",
    "        reg_not_0_valid = np.where(flattened_input_valid[:, loc_all['clw']] + flattened_input_valid[:, loc_all['cli']] > 1e-20)[0]\n",
    "    else:\n",
    "        reg_not_0_train = np.where(flattened_input_train[:, loc['clw']] + flattened_input_train[:, loc['cli']] > 1e-20)[0]\n",
    "        reg_not_0_valid = np.where(flattened_input_valid[:, loc['clw']] + flattened_input_valid[:, loc['cli']] > 1e-20)[0]\n",
    "        \n",
    "    flattened_input_train = flattened_input_train[reg_not_0_train]\n",
    "    flattened_output_train = flattened_output_train[reg_not_0_train]\n",
    "    flattened_input_valid = flattened_input_valid[reg_not_0_valid]\n",
    "    flattened_output_valid = flattened_output_valid[reg_not_0_valid]\n",
    "\n",
    "    # We only need to split the regimes further if no_of_regimes > 2\n",
    "    if no_of_regimes > 2:\n",
    "        # Take a subset of the data to train the decision tree on\n",
    "        subset_size = 10**7 # or 10**6\n",
    "\n",
    "        inds = np.random.randint(0, flattened_input_train.shape[0], subset_size)\n",
    "        input_subset = flattened_input_train[inds]\n",
    "        output_subset = flattened_output_train[inds]\n",
    "\n",
    "        classification_tree = tree.DecisionTreeRegressor(max_depth=3, max_leaf_nodes=(no_of_regimes-1)) # set max_depth to [2,3]\n",
    "        classification_tree.fit(input_subset, output_subset)\n",
    "        text_representation = tree.export_text(classification_tree, feature_names=features)\n",
    "        print(text_representation)\n",
    "\n",
    "        ind_reg_train = np.where(classification_tree.apply(flattened_input_train) == regime)\n",
    "        ind_reg_valid = np.where(classification_tree.apply(flattened_input_valid) == regime)\n",
    "\n",
    "        # Sometimes, the regime is called differently...\n",
    "        if np.sum(ind_reg_train) == 0:\n",
    "            print('The regime %d does not exist, switching to regime %d instead.'%(regime, no_of_regimes))\n",
    "            ind_reg_train = np.where(classification_tree.apply(flattened_input_train) == no_of_regimes)\n",
    "            ind_reg_valid = np.where(classification_tree.apply(flattened_input_valid) == no_of_regimes)\n",
    "\n",
    "        flattened_input_train = flattened_input_train[ind_reg_train]\n",
    "        flattened_input_valid = flattened_input_valid[ind_reg_valid]\n",
    "\n",
    "        flattened_output_train = flattened_output_train[ind_reg_train]\n",
    "        flattened_output_valid = flattened_output_valid[ind_reg_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pick the subset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = np.random.randint(0, len(flattened_output_train), subset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "mean_all = [4.12205844e-03,2.25493498e-05,3.38180032e-06,2.57065512e+02,6.00030443e+04,5.64080139e+03,2.35046400e-01,1.32776682e+01,6.02512234e-01,9.86270417e+04,-1.27545273e-06,-4.02484958e-10,1.65204582e-08,-4.34660202e-11,4.29441131e-10,-1.82817316e-12,-4.68742483e-03,-7.54899040e-07,-7.51544542e+00,-1.06989723e-04,1.65615172e-03,-9.27604679e-06,-4.76200071e-05,-1.32246548e-07]\n",
    "std_all = [5.07648249e-03,5.69702638e-05,1.01308124e-05,3.00533874e+01,3.12514292e+04,5.66963918e+03,4.11184302e-01,1.11389888e+01,3.32494615e-01,6.24039256e+03,2.03179260e-06,1.17041141e-08,1.33311867e-07,1.42840744e-09,6.73384546e-09,5.07424672e-11,5.82875686e-03,6.34826092e-05,3.53136052e+00,1.13215264e-02,6.62892130e-03,6.08144307e-05,2.58065098e-04,2.49552692e-06]\n",
    "\n",
    "mean = np.concatenate([np.expand_dims(mean_all[loc_all[sel_var]], axis=0) for sel_var in features], axis = 0)\n",
    "std = np.concatenate([np.expand_dims(std_all[loc_all[sel_var]], axis=0) for sel_var in features], axis = 0)\n",
    "\n",
    "# Work with scaled training folds\n",
    "train_data_scaled = (flattened_input_train - mean)/std\n",
    "valid_data_scaled = (flattened_input_valid - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run pySR**\n",
    "\n",
    "- Add constraints --> Try to disallow non-interpretable equations as much as possible!\n",
    "- Wisely select potential operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reveal the location of Julia\n",
    "if '/home/b/b309170/my_work/Miniconda3/envs/pysr/bin' not in os.environ[\"PATH\"]:\n",
    "    os.environ[\"PATH\"] = os.environ[\"PATH\"] + ':/home/b/b309170/my_work/Miniconda3/envs/pysr/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Floor, ceil, mod, sign --> Removed because I don't want discontinuities within regimes\n",
    "# log2, log10 --> Removed because log is sufficient\n",
    "\n",
    "unary_operators = [\"cos\",\"exp\",\"sin\",\"tan\",\"tanh\",\"cosh\",\"sinh\",\"abs\",\"cube\",\"neg\",\"acosh\",\n",
    "        \"asinh\",\"atanh\", \"erf\", \"log_abs\",\"sqrt_abs\",'gamma','relu']\n",
    "\n",
    "binary_operators = [\"div\",\"mult\",\"plus\",\"sub\",\"pow\"]\n",
    "\n",
    "# Very low-complexity operators (x)\n",
    "very_low_complex_ops = [\"mult\", \"plus\", \"sub\", \"neg\"] \n",
    "\n",
    "# Low-complexity operators (2x)\n",
    "low_complex_ops = [\"div\",\"abs\", \"sqrt_abs\", \"cube\", 'relu']\n",
    "\n",
    "# Medium-complexity operators (3x)\n",
    "medium_complex_ops = ['cos', 'sin', \"exp\", 'tan', \"tanh\", \"cosh\", \"sinh\", \"log_abs\"]\n",
    "\n",
    "# High-complexity operators (9x)\n",
    "high_complex_ops = [\"pow\", 'gamma', \"asinh\",\"atanh\", \"acosh\", 'acos', 'asin', 'atan', \"erf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$cos(x) + sin(y + z)$ is as complex as $gamma(x+y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/b309170/my_work/Miniconda3/envs/pysr/lib/python3.10/site-packages/pysr/sr.py:1162: UserWarning: Note: Using a large maxsize for the equation search will be exponentially slower and use significant memory. You should consider turning `use_frequency` to False, and perhaps use `warmup_maxsize_by`.\n",
      "  warnings.warn(\n",
      "/home/b/b309170/my_work/Miniconda3/envs/pysr/lib/python3.10/site-packages/pysr/sr.py:1225: UserWarning: Note: it looks like you are running in Jupyter. The progress bar will be turned off.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# # Disallow nesting with itself\n",
    "# nested_constraints = {op: {op: 1} for op in unary_operators_ext}\n",
    "\n",
    "# Output folder\n",
    "if no_features == 5 and loss_exp == 2 and loc_issue:\n",
    "    tempdir = \"/home/b/b309170/workspace_icon-ml/symbolic_regression/finding_symmetries/pysr_results_dyamond_on_regimes/no_of_regimes_%d/regime_%d\"%(no_of_regimes, regime)\n",
    "elif no_features == 8 and loc_issue:\n",
    "    tempdir = \"/home/b/b309170/workspace_icon-ml/symbolic_regression/finding_symmetries/pysr_results_dyamond_on_regimes/no_of_regimes_%d/regime_%d_eight_features\"%(no_of_regimes, regime)\n",
    "elif loss_exp != 2 and loc_issue:\n",
    "    tempdir = \"/home/b/b309170/workspace_icon-ml/symbolic_regression/finding_symmetries/pysr_results_dyamond_on_regimes/no_of_regimes_%d/regime_%d_loss_exp_%d\"%(no_of_regimes, regime, loss_exp)\n",
    "elif not loc_issue:\n",
    "    tempdir = \"/home/b/b309170/workspace_icon-ml/symbolic_regression/finding_symmetries/pysr_results_dyamond_on_regimes/no_of_regimes_%d/regime_%d_no_loc_issue_loss_exp_%d\"%(no_of_regimes, regime, loss_exp)\n",
    "    \n",
    "# Takes four times as long with the extended amount of possible operators\n",
    "model = PySRRegressor(\n",
    "    niterations=10000000,  # Run forever\n",
    "    timeout_in_seconds=int(60 * 60 * 7.7),\n",
    "    # ^ Alternatively, stop after 7.7 hours have passed.\n",
    "    maxsize=90, # 12/8/22: Reduced from 200 to 90\n",
    "    # ^ Maximal complexity of an equation\n",
    "    maxdepth=4,\n",
    "    # ^ Avoid very deep nesting.\n",
    "    complexity_of_operators = {**{key: 8*verylow_ops_complexity for key in high_complex_ops},\n",
    "                               **{key: 3*verylow_ops_complexity for key in medium_complex_ops}, \n",
    "                               **{key: 2*verylow_ops_complexity for key in low_complex_ops},\n",
    "                               **{key: verylow_ops_complexity for key in very_low_complex_ops}},\n",
    "    complexity_of_variables = 1,\n",
    "    populations=20,\n",
    "    binary_operators=binary_operators,\n",
    "    unary_operators=unary_operators,\n",
    "    extra_sympy_mappings={'inv': lambda x: 1/x},\n",
    "    model_selection=\"best\",\n",
    "    # x is the prediction\n",
    "    loss=\"loss(x, y) = (abs(max(min(x,100),0) - y))^%d\"%loss_exp,  # Custom loss function (julia syntax)\n",
    "    tempdir=tempdir,\n",
    "    temp_equation_file=True,\n",
    "    delete_tempfiles=False,\n",
    "    progress = False,\n",
    ")\n",
    "\n",
    "model.fit(train_data_scaled[subset], flattened_output_train[subset])\n",
    "# model.sympy()\n",
    "\n",
    "time.time() - t0\n",
    "\n",
    "# So that we know the size of the subset and the number of iterations\n",
    "with open(os.path.join(model.tempdir_, 'out.txt') , 'a') as file:\n",
    "    file.write('The search took %.2f minutes.\\n'%((time.time() - t0)/60))\n",
    "    file.write('MSE on the validation data: %.3f\\n'%np.mean((model.predict(valid_data_scaled) - flattened_output_valid)**2, dtype=np.float64))\n",
    "    file.write('Size of the subset: %d\\n'%subset_size)\n",
    "    file.write('Complexity of very low complexity ops: %d\\n'%verylow_ops_complexity)\n",
    "    file.write(model.latex_table(precision=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysr",
   "language": "python",
   "name": "pysr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

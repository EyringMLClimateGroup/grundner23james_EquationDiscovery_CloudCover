{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867c0ea0-1ecb-4102-be68-0a813a79c426",
   "metadata": {},
   "source": [
    "### Evaluate the 24-feature NN\n",
    "\n",
    "- Data path: /home/b/b309170/bd1179_work/DYAMOND/hcg_data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f6ce1c-bc5e-490d-ac03-cf6a39a58f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need 960GB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68116c36-a723-447a-be69-3bfe4c0afb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import nn \n",
    "from tensorflow.keras import backend as K \n",
    "\n",
    "sys.path.insert(0, os.environ['HOME'] + '/my_work/published_code/grundner23james_EquationDiscovery_CloudCover_addressing_reviews/sec2_data/')\n",
    "import my_classes\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import load_data\n",
    "from functions import append_dict_to_json\n",
    "\n",
    "# Good performance with bs_exp = 23\n",
    "# OOM when bs_exp too high, but possibly bs_exp > 23 would be better.\n",
    "bs_exp = 20\n",
    "print(bs_exp)\n",
    "\n",
    "# num_cells = int(sys.argv[2]) #[1, 8, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740906a-9ecc-4689-b09b-3e2925aaf5c9",
   "metadata": {},
   "source": [
    "**Load the 20/40/80km DYAMOND data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d011daf-2528-438e-b659-89c2134abd82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/b309170/my_work/published_code/grundner23james_EquationDiscovery_CloudCover_addressing_reviews/sec2_data/my_classes.py:405: FutureWarning: In xarray version 0.15 the default behaviour of `open_mfdataset`\n",
      "will change. To retain the existing behavior, pass\n",
      "combine='nested'. To use future default behavior, pass\n",
      "combine='by_coords'. See\n",
      "http://xarray.pydata.org/en/stable/combining.html#combining-multi\n",
      "\n",
      "  DS = xr.open_mfdataset(path+'/zg/zg*')\n",
      "/home/b/b309170/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/xarray/backends/api.py:933: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
      "to use the new `combine_by_coords` function (or the\n",
      "`combine='by_coords'` option to `open_mfdataset`) to order the datasets\n",
      "before concatenation. Alternatively, to continue concatenating based\n",
      "on the order the datasets are supplied in future, please use the new\n",
      "`combine_nested` function (or the `combine='nested'` option to\n",
      "open_mfdataset).\n",
      "  from_openmfds=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "qc\n",
      "qi\n",
      "t\n",
      "pres\n",
      "u\n",
      "v\n",
      "clc\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "order_of_vars = ['q', 'qc', 'qi', 't', 'pres', 'u', 'v', 'zg', 'fr_land', 'clc']\n",
    "\n",
    "data_path = '/home/b/b309170/bd1179_work/DYAMOND/hcg_data_r2b7'\n",
    "data_dict = load_data(source='split_by_var_name', days='all', vert_interp=False, \\\n",
    "                      resolution='R02B07', order_of_vars=order_of_vars, path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504fc02b-b37e-4235-baab-0ccaddd7c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS, VLAYERS, HFIELDS = data_dict['q'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b47fd2f-65b9-4c59-8215-c5a9592a23cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['fr_land'] = np.repeat(np.expand_dims(data_dict['fr_land'], axis=0), VLAYERS, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9693c7b-82a2-4fc5-892a-1ded9b02bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['zg'] = np.repeat(np.expand_dims(data_dict['zg'].T, axis=0), TIMESTEPS, axis=0)\n",
    "data_dict['fr_land'] = np.repeat(np.expand_dims(data_dict['fr_land'], axis=0), TIMESTEPS, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e753cf7a-9ac8-4eb2-a8aa-46adc1f51ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 60, 1310720)\n",
      "(80, 60, 1310720)\n",
      "(80, 60, 1310720)\n",
      "(80, 60, 1310720)\n",
      "(80, 60, 1310720)\n",
      "(80, 60, 1310720)\n",
      "(80, 60, 1310720)\n",
      "(80, 60, 1310720)\n",
      "(80, 60, 1310720)\n",
      "(80, 60, 1310720)\n"
     ]
    }
   ],
   "source": [
    "# # Only keep the lowest 60 levels (ensure that all fields have the same vertical grid)\n",
    "# for key in data_dict.keys():\n",
    "#     data_dict[key] = data_dict[key][:, -60:].copy()\n",
    "#     # print(data_dict[key].shape)\n",
    "    \n",
    "# To avoid OOM I now only take every second entry!\n",
    "print('Expecting around 330000 horizontal fields')\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = data_dict[key][:, -60:, ::4].copy()\n",
    "    print(data_dict[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb376b5-51d3-46fa-a753-c84cc2fb5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add magnitude of horizontal wind\n",
    "data_dict['U'] = np.sqrt(data_dict['u']**2 + data_dict['v']**2)\n",
    "del data_dict['u']\n",
    "del data_dict['v']\n",
    "\n",
    "# Add RH\n",
    "T0 = 273.15\n",
    "r = 0.00263*data_dict['pres']*data_dict['q']*np.exp((17.67*(data_dict['t']-T0))/(data_dict['t']-29.65))**(-1)\n",
    "data_dict['rh'] = r\n",
    "\n",
    "# Update\n",
    "TIMESTEPS, VLAYERS, HFIELDS = data_dict['q'].shape\n",
    "\n",
    "# Add ps\n",
    "ps = np.repeat(np.expand_dims(data_dict['pres'][:, -1], axis=1), VLAYERS, axis=1)\n",
    "data_dict['ps'] = ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ddcd32-8f83-467b-a3fd-cf0058dc82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add derivatives\n",
    "data_dict['rh_z'] = (r[:, :-1] - r[:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['rh_zz'] = (data_dict['rh_z'][:, :-1] - data_dict['rh_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])\n",
    "data_dict['t_z'] = (data_dict['t'][:, :-1] - data_dict['t'][:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['t_zz'] = (data_dict['t_z'][:, :-1] - data_dict['t_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])\n",
    "data_dict['pres_z'] = (data_dict['pres'][:, :-1] - data_dict['pres'][:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['pres_zz'] = (data_dict['pres_z'][:, :-1] - data_dict['pres_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])\n",
    "data_dict['qc_z'] = (data_dict['qc'][:, :-1] - data_dict['qc'][:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['qc_zz'] = (data_dict['qc_z'][:, :-1] - data_dict['qc_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a68d1-6949-4396-83b7-48b54d18c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['U_z'] = (data_dict['U'][:, :-1] - data_dict['U'][:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['U_zz'] = (data_dict['U_z'][:, :-1] - data_dict['U_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])\n",
    "data_dict['qi_z'] = (data_dict['qi'][:, :-1] - data_dict['qi'][:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['qi_zz'] = (data_dict['qi_z'][:, :-1] - data_dict['qi_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42778e-b6f7-4016-ba02-3d8d6f639a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['q_z'] = (data_dict['q'][:, :-1] - data_dict['q'][:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['q_zz'] = (data_dict['q_z'][:, :-1] - data_dict['q_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cde5c2-e6c5-4a31-a1a8-3893b81a63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9138e17c-8379-4b8e-9396-13ee2d5d2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the lowest 58 levels (up to 21km)\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = data_dict[key][:, -58:].copy()\n",
    "\n",
    "# Data output\n",
    "data_output = 100*data_dict['clc']\n",
    "del data_dict['clc']\n",
    "\n",
    "# ## LESS DATA ## #!\n",
    "# for key in data_dict.keys():\n",
    "#     data_dict[key] = data_dict[key][0::3]\n",
    "# data_output = data_output[0::3]\n",
    "# TIMESTEPS = TIMESTEPS//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91c739-ee4e-4933-ae2a-0e4193ee1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cp2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91811b3d-180f-44c8-aaa2-ba5890f82d61",
   "metadata": {},
   "source": [
    "**All features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f528b0c-7b35-460e-b9ed-a2ac9a6e548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nn = ['q', 'qc', 'qi', 't', 'pres', 'zg', 'fr_land', 'U', 'rh', 'ps', 'q_z', 'q_zz', 'qc_z',\\\n",
    "            'qc_zz', 'qi_z', 'qi_zz', 't_z', 't_zz', 'pres_z', 'pres_zz', 'U_z', 'U_zz', 'rh_z', 'rh_zz']\n",
    "\n",
    "k = 0\n",
    "loc = {}\n",
    "for feat in features_nn:\n",
    "    loc[feat] = k\n",
    "    k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1cc4c1-0cbf-4bdf-ba68-c437fc53c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725499b7-f7f5-4be0-b8f6-2633f5019a7f",
   "metadata": {},
   "source": [
    "**Cast dict into ndarray and reshape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8e575cc-78ca-4c44-bf5d-9782570c2caf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'q'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-dfc1105fac41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_array_not_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_nn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata_array_not_T\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'q'"
     ]
    }
   ],
   "source": [
    "# data_array = np.zeros((data_dict['q'].size, len(data_dict.keys())), dtype=np.float32)\n",
    "\n",
    "# # Directly use nparray\n",
    "# data_array = np.array(np.reshape(data_dict[features_nn[0]], (-1,1)), dtype=np.float32)\n",
    "# del data_dict[features_nn[0]]\n",
    "\n",
    "# for key in features_nn[1:]:\n",
    "#     print(key)\n",
    "#     data_array = np.concatenate(data_array, np.reshape(data_dict[key], (-1,1)), axis=1)\n",
    "#     del data_dict[key]\n",
    "#     gc.collect()\n",
    "    \n",
    "# del data_dict\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "data_array_not_T = []\n",
    "for key in features_nn:\n",
    "    print(key)\n",
    "    data_array_not_T.append(np.reshape(data_dict[key], -1))\n",
    "    del data_dict[key]\n",
    "    gc.collect()\n",
    "    \n",
    "# Convert into np array and transpose\n",
    "data_array = np.transpose(np.array(data_array_not_T, dtype=np.float32))\n",
    "data_output = np.reshape(data_output, -1)\n",
    "\n",
    "del data_array_not_T\n",
    "gc.collect()\n",
    "    \n",
    "print('cpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43370d5f-0570-4f06-900d-2da5e8a3d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert into np array and transpose\n",
    "# data_array = np.array(data_array, dtype=np.float32)\n",
    "# print('cpc2')\n",
    "\n",
    "# data_array = np.transpose(data_array)\n",
    "# print('cpc3')\n",
    "\n",
    "data_output = np.reshape(data_output, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4044507e-eb01-4fd2-8ff3-64b9bb84a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b405d5-dbee-4f13-b205-786c94e65cbc",
   "metadata": {},
   "source": [
    "**Final cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f4eea-6a6d-4656-8f35-943e3aa1ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_small_batches(model, input_data, batch_size=2**20):\n",
    "    # Using predict_on_batch on the entire dataset results in an OOM error\n",
    "    # Curiously it works best if we use predict_on_batch on small subsets of the data instead of predict(..., batch_size=...) \n",
    "    for i in range(1 + input_data.shape[0]//batch_size):\n",
    "        if i == 0:\n",
    "            a = model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            a = np.concatenate((a, model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])), axis=0)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    pred_adj = np.minimum(np.maximum(a, 0), 100) \n",
    "    \n",
    "    return pred_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a899ae5-3dfa-4428-8df7-9f604d5ff221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute only once\n",
    "VAR = np.var(data_output)\n",
    "\n",
    "# For the NNs\n",
    "custom_objects = {}\n",
    "custom_objects['leaky_relu'] = nn.leaky_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f0503-6195-4813-9e4b-6bfaa768589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cp5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51b834-0b85-4aaa-8cdd-b9dad7197132",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get mean and std\n",
    "nn_path = os.environ['HOME'] + '/workspace_icon-ml/cloud_cover_parameterization/neighborhood_based_SR_DYAMOND/saved_models'\n",
    "\n",
    "model_name = 'cross_validation_neighborhood_based_sr_cl_area_fold_2'\n",
    "\n",
    "## Get mean and std from the model-file\n",
    "mean, std = read_mean_and_std(os.path.join(nn_path, model_name + '.txt'))\n",
    "\n",
    "## Scale all data using this mean and std\n",
    "data_array = (data_array - np.float32(mean))/np.float32(std)\n",
    "\n",
    "results = {} \n",
    "parent_key = 'SFS_NN_24_no_tl'\n",
    "results[parent_key] = {}  \n",
    "\n",
    "model = load_model(os.path.join(nn_path, model_name + '.h5'), custom_objects)\n",
    "\n",
    "## Evaluate model on scaled data\n",
    "predictions = predict_on_small_batches(model, data_array, batch_size=2**bs_exp)\n",
    "\n",
    "# Mean-squared error\n",
    "mse = np.mean((predictions[:, 0] - data_output)**2)\n",
    "results[parent_key]['MSE'] = float(mse)\n",
    "print(mse)\n",
    "\n",
    "# R2-value\n",
    "r2 = 1 - mse/VAR\n",
    "results[parent_key]['R2'] = float(r2)\n",
    "\n",
    "## Write results to json-file\n",
    "append_dict_to_json(results, os.environ['HOME'] + '/my_work/published_code/grundner23james_EquationDiscovery_CloudCover_addressing_reviews/sec5_results/transfer_to_higher_resolutions/results/24_feat_nn_r2b7.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds",
   "language": "python",
   "name": "clouds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

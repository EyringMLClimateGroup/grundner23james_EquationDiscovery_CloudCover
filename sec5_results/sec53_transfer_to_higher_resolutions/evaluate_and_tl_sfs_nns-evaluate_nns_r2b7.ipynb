{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcaabb13-963e-42d3-8614-46dd25937f3b",
   "metadata": {},
   "source": [
    "### Evaluate the SFS NNs on higher-res DYAMOND data\n",
    "\n",
    "- Data path: /home/b/b309170/bd1179_work/DYAMOND/hcg_data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b29fbd-86eb-421d-b24d-c8fd6eb242cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need 960GB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68116c36-a723-447a-be69-3bfe4c0afb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import nn \n",
    "from tensorflow.keras import backend as K \n",
    "\n",
    "sys.path.insert(0, os.environ['HOME'] + '/my_work/published_code/grundner23james_EquationDiscovery_CloudCover_addressing_reviews/sec2_data/')\n",
    "import my_classes\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import load_data\n",
    "from functions import append_dict_to_json\n",
    "\n",
    "# Good performance with bs_exp = 23 and on a gpu\n",
    "# OOM when bs_exp too high, but possibly bs_exp > 23 would be better.\n",
    "bs_exp = 20\n",
    "print(bs_exp)\n",
    "\n",
    "# num_cells = int(sys.argv[2]) #[1, 8, 32]\n",
    "SFS_MODEL = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740906a-9ecc-4689-b09b-3e2925aaf5c9",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138e17c-8379-4b8e-9396-13ee2d5d2ae4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/b309170/my_work/published_code/grundner23james_EquationDiscovery_CloudCover_addressing_reviews/sec2_data/my_classes.py:403: FutureWarning: In xarray version 0.15 the default behaviour of `open_mfdataset`\n",
      "will change. To retain the existing behavior, pass\n",
      "combine='nested'. To use future default behavior, pass\n",
      "combine='by_coords'. See\n",
      "http://xarray.pydata.org/en/stable/combining.html#combining-multi\n",
      "\n",
      "  DS = xr.open_mfdataset(path+'/zg/zg*')\n",
      "/home/b/b309170/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/xarray/backends/api.py:933: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
      "to use the new `combine_by_coords` function (or the\n",
      "`combine='by_coords'` option to `open_mfdataset`) to order the datasets\n",
      "before concatenation. Alternatively, to continue concatenating based\n",
      "on the order the datasets are supplied in future, please use the new\n",
      "`combine_nested` function (or the `combine='nested'` option to\n",
      "open_mfdataset).\n",
      "  from_openmfds=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "qc\n",
      "qi\n",
      "t\n",
      "pres\n",
      "u\n",
      "v\n",
      "clc\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "order_of_vars = ['q', 'qc', 'qi', 't', 'pres', 'u', 'v', 'zg', 'fr_land', 'clc']\n",
    "\n",
    "data_path = '/home/b/b309170/bd1179_work/DYAMOND/hcg_data_r2b7'\n",
    "data_dict = load_data(source='split_by_var_name', days='all', vert_interp=False, \\\n",
    "                      resolution='R02B07', order_of_vars=order_of_vars, path=data_path)\n",
    "\n",
    "TIMESTEPS, VLAYERS, HFIELDS = data_dict['q'].shape\n",
    "\n",
    "data_dict['fr_land'] = np.repeat(np.expand_dims(data_dict['fr_land'], axis=0), VLAYERS, axis=0)\n",
    "\n",
    "data_dict['zg'] = np.repeat(np.expand_dims(data_dict['zg'].T, axis=0), TIMESTEPS, axis=0)\n",
    "data_dict['fr_land'] = np.repeat(np.expand_dims(data_dict['fr_land'], axis=0), TIMESTEPS, axis=0)\n",
    "\n",
    "# Only keep the lowest 60 levels (ensure that all fields have the same vertical grid)\n",
    "# To avoid OOM I now only take every second entry!\n",
    "print('Expecting around 650000 horizontal fields')\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = data_dict[key][:, -60:, ::2].copy()\n",
    "    print(data_dict[key].shape)\n",
    "\n",
    "# Add magnitude of horizontal wind\n",
    "data_dict['U'] = np.sqrt(data_dict['u']**2 + data_dict['v']**2)\n",
    "del data_dict['u']\n",
    "del data_dict['v']\n",
    "\n",
    "# Add RH\n",
    "T0 = 273.15\n",
    "r = 0.00263*data_dict['pres']*data_dict['q']*np.exp((17.67*(data_dict['t']-T0))/(data_dict['t']-29.65))**(-1)\n",
    "data_dict['rh'] = r\n",
    "\n",
    "# Update\n",
    "TIMESTEPS, VLAYERS, HFIELDS = data_dict['q'].shape\n",
    "\n",
    "# Add ps\n",
    "ps = np.repeat(np.expand_dims(data_dict['pres'][:, -1], axis=1), VLAYERS, axis=1)\n",
    "data_dict['ps'] = ps\n",
    "\n",
    "# Add derivatives\n",
    "data_dict['rh_z'] = (r[:, :-1] - r[:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['rh_zz'] = (data_dict['rh_z'][:, :-1] - data_dict['rh_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])\n",
    "data_dict['t_z'] = (data_dict['t'][:, :-1] - data_dict['t'][:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['t_zz'] = (data_dict['t_z'][:, :-1] - data_dict['t_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])\n",
    "data_dict['pres_z'] = (data_dict['pres'][:, :-1] - data_dict['pres'][:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['pres_zz'] = (data_dict['pres_z'][:, :-1] - data_dict['pres_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])\n",
    "data_dict['qc_z'] = (data_dict['qc'][:, :-1] - data_dict['qc'][:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['qc_zz'] = (data_dict['qc_z'][:, :-1] - data_dict['qc_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])\n",
    "data_dict['U_z'] = (data_dict['U'][:, :-1] - data_dict['U'][:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['U_zz'] = (data_dict['U_z'][:, :-1] - data_dict['U_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])\n",
    "data_dict['qi_z'] = (data_dict['qi'][:, :-1] - data_dict['qi'][:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['qi_zz'] = (data_dict['qi_z'][:, :-1] - data_dict['qi_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])\n",
    "data_dict['q_z'] = (data_dict['q'][:, :-1] - data_dict['q'][:, 1:])/(data_dict['zg'][:, :-1] - data_dict['zg'][:, 1:])\n",
    "data_dict['q_zz'] = (data_dict['q_z'][:, :-1] - data_dict['q_z'][:, 1:])/(data_dict['zg'][:, 1:-1] - data_dict['zg'][:, 2:])\n",
    "\n",
    "# Only keep the lowest 58 levels (up to 21km)\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = data_dict[key][:, -58:].copy()\n",
    "\n",
    "# Data output\n",
    "data_output = 100*data_dict['clc']\n",
    "del data_dict['clc']\n",
    "\n",
    "# ## LESS DATA ## #!\n",
    "# for key in data_dict.keys():\n",
    "#     data_dict[key] = data_dict[key][0::3]\n",
    "# data_output = data_output[0::3]\n",
    "# TIMESTEPS = TIMESTEPS//3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91811b3d-180f-44c8-aaa2-ba5890f82d61",
   "metadata": {},
   "source": [
    "**All features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f528b0c-7b35-460e-b9ed-a2ac9a6e548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nn = ['q', 'qc', 'qi', 't', 'pres', 'zg', 'fr_land', 'U', 'rh', 'ps', 'q_z', 'q_zz', 'qc_z',\\\n",
    "            'qc_zz', 'qi_z', 'qi_zz', 't_z', 't_zz', 'pres_z', 'pres_zz', 'U_z', 'U_zz', 'rh_z', 'rh_zz']\n",
    "\n",
    "k = 0\n",
    "loc = {}\n",
    "for feat in features_nn:\n",
    "    loc[feat] = k\n",
    "    k = k + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725499b7-f7f5-4be0-b8f6-2633f5019a7f",
   "metadata": {},
   "source": [
    "**Cast dict into ndarray and reshape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43370d5f-0570-4f06-900d-2da5e8a3d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_array = np.zeros((data_dict['q'].size, len(data_dict.keys())), dtype=np.float32)\n",
    "\n",
    "k = 0\n",
    "data_array_not_T = []\n",
    "for key in features_nn:\n",
    "    data_array_not_T.append(np.reshape(data_dict[key], -1))\n",
    "    del data_dict[key]\n",
    "    k += 1\n",
    "\n",
    "# Convert into np array and transpose\n",
    "data_array = np.transpose(np.array(data_array_not_T, dtype=np.float32))\n",
    "data_output = np.reshape(data_output, -1)\n",
    "\n",
    "del data_array_not_T\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6817b7b6-dbfe-4c51-ab1c-0c22bcfa9f8a",
   "metadata": {},
   "source": [
    "**Loop through SFS NNs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5e419-5883-477b-b068-7830b10fe3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_features(sfs_ind):\n",
    "    '''\n",
    "        Extract the relevant feature names and their order for a given SFS NN\n",
    "    '''\n",
    "    conv = {'clw': 'qc', 'cli': 'qi', 'ta': 't', 'pa_z': 'pres_z'}\n",
    "    with open(os.environ['HOME'] + '/workspace_icon-ml/cloud_cover_parameterization/neighborhood_based_on_seq_feat_sel_DYAMOND/saved_models/neighborhood_based_sfs_cl_area_no_features_%d.txt'%sfs_ind, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for k in range(len(lines)):\n",
    "            if lines[k].startswith('The (order of) input variables'):\n",
    "                out_line = lines[k+1][1:-2].split(' ')\n",
    "    for ind in range(len(out_line)):\n",
    "        out_line[ind] = out_line[ind][1:-1]\n",
    "        # Rename if the name is different in ERA5\n",
    "        if out_line[ind] in conv.keys():\n",
    "            out_line[ind] = conv[out_line[ind]]\n",
    "    return out_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b405d5-dbee-4f13-b205-786c94e65cbc",
   "metadata": {},
   "source": [
    "**Final cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c030268-126f-44d5-9b1f-4b1d60005684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_small_batches(model, input_data, batch_size=2**20):\n",
    "    # Using predict_on_batch on the entire dataset results in an OOM error\n",
    "    # Curiously it works best if we use predict_on_batch on small subsets of the data instead of predict(..., batch_size=...) \n",
    "    for i in range(1 + input_data.shape[0]//batch_size):\n",
    "        if i == 0:\n",
    "            a = model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            a = np.concatenate((a, model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])), axis=0)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    pred_adj = np.minimum(np.maximum(a, 0), 100) \n",
    "    \n",
    "    return pred_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a899ae5-3dfa-4428-8df7-9f604d5ff221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute only once\n",
    "VAR = np.var(data_output)\n",
    "\n",
    "# For the NNs\n",
    "custom_objects = {}\n",
    "custom_objects['leaky_relu'] = nn.leaky_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51b834-0b85-4aaa-8cdd-b9dad7197132",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sfs_ind in range(SFS_MODEL, SFS_MODEL + 1):\n",
    "    ## Get mean and std\n",
    "    nn_path = os.environ['HOME'] + '/workspace_icon-ml/cloud_cover_parameterization/neighborhood_based_on_seq_feat_sel_DYAMOND/saved_models'\n",
    "\n",
    "    # Select the appropriate features    \n",
    "    features_inds = []\n",
    "    features_nn = which_features(sfs_ind)\n",
    "    for k in range(sfs_ind):\n",
    "        features_inds.append(loc[features_nn[k]])\n",
    "    data_array_sfs_nn = data_array[:, features_inds]\n",
    "\n",
    "    if sfs_ind in [4,5,6,7]:\n",
    "        if sfs_ind == 4:\n",
    "            thrd_lay = 'False'\n",
    "        else:\n",
    "            thrd_lay = 'True'\n",
    "        model_name = 'hyperparameter_tests/neighborhood_based_sfs_cl_area_no_features_%d_False_%s_16'%(sfs_ind,thrd_lay)\n",
    "        if sfs_ind == 7:\n",
    "            model_name = 'hyperparameter_tests/neighborhood_based_sfs_cl_area_no_features_7_True_True_32'\n",
    "    else:\n",
    "        model_name = 'neighborhood_based_sfs_cl_area_no_features_%d'%sfs_ind\n",
    "\n",
    "    ## Get mean and std from the model-file\n",
    "    mean, std = read_mean_and_std(os.path.join(nn_path, model_name + '.txt'))\n",
    "\n",
    "    ## Scale all data using this mean and std\n",
    "    data_array_scaled = (data_array_sfs_nn - np.float32(mean))/np.float32(std)\n",
    "    \n",
    "    results = {} \n",
    "    parent_key = 'SFS_NN_%d_no_tl'%(sfs_ind)\n",
    "    results[parent_key] = {}  \n",
    "\n",
    "    model = load_model(os.path.join(nn_path, model_name + '.h5'), custom_objects)\n",
    "\n",
    "    ## Evaluate model on scaled data\n",
    "    predictions = predict_on_small_batches(model, data_array_scaled)\n",
    "\n",
    "    # Mean-squared error\n",
    "    mse = np.mean((predictions[:, 0] - data_output)**2)\n",
    "    results[parent_key]['MSE'] = float(mse)\n",
    "    print(mse)\n",
    "\n",
    "    # R2-value\n",
    "    r2 = 1 - mse/VAR\n",
    "    results[parent_key]['R2'] = float(r2)\n",
    "\n",
    "    ## Write results to json-file\n",
    "    append_dict_to_json(results, os.environ['HOME'] + '/my_work/published_code/grundner23james_EquationDiscovery_CloudCover_addressing_reviews/sec5_results/transfer_to_higher_resolutions/results/sfs6_nn_r2b7.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8a70d4-2494-427f-95d1-65fa609183e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds",
   "language": "python",
   "name": "clouds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

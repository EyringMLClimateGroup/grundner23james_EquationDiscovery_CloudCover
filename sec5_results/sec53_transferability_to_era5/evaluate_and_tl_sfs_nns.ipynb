{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867c0ea0-1ecb-4102-be68-0a813a79c426",
   "metadata": {},
   "source": [
    "### SFS NNs\n",
    "\n",
    "Executed through /home/b/b309170/scripts/run_era5_evalute_and_transfer_learn_4.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872cc345-739d-4d38-a9bd-b005d451e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with 960GB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "762c00f5-d2b6-4cb8-9fe5-bdf4e68f2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('PDF')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import nn \n",
    "\n",
    "## Transfer learn? ##\n",
    "# tl_bool = bool(int(sys.argv[1]))\n",
    "tl_bool = True\n",
    "subset_exp = int(sys.argv[1])\n",
    "# subset_exp = 2\n",
    "number_horizontal_locations = 10**subset_exp\n",
    "# How long to re-train? Setting it to 40 minutes.\n",
    "timeout = 40\n",
    "# The original LR_INIT was 4.33e-4.\n",
    "LR_INIT = 4.33e-4\n",
    "SEED = int(sys.argv[2])\n",
    "# SEED = 10\n",
    "\n",
    "# If the model was already transfer-learned we can set this to True\n",
    "already_trained = False\n",
    "\n",
    "sys.path.insert(0, '/home/b/b309170/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "import my_classes\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import load_data\n",
    "from my_classes import TimeOut\n",
    "\n",
    "sys.path.insert(0, '/home/b/b309170/workspace_icon-ml/symbolic_regression/')\n",
    "from functions import add_derivatives\n",
    "from functions import append_dict_to_json\n",
    "\n",
    "num_cells = int(sys.argv[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf3f631-6490-4ea2-9c6a-b3b643291c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the script:\n",
      "1674464690.6535661\n"
     ]
    }
   ],
   "source": [
    "# How long does it take to load the data\n",
    "print('Starting the script:')\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddfe5914-3d4f-47c2-bed0-bd266ad6d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PWD = '/home/b/b309170/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e749b4-81c1-4f7d-8e4a-cd8c9ada2386",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5346f6-c28e-4a44-b614-e8190670e478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "clwc\n",
      "ciwc\n",
      "t\n",
      "pa\n",
      "u\n",
      "v\n",
      "cc\n",
      "all\n",
      "Assertion warning. Max cc not 100. Instead:\n",
      "100.000015\n"
     ]
    }
   ],
   "source": [
    "order_of_vars = ['q', 'clwc', 'ciwc', 't', 'pa', 'u', 'v', 'zg', 'fr_land', 'cc']\n",
    "data_dict = load_data(source='era5', days='all', order_of_vars=order_of_vars)\n",
    "\n",
    "TIMESTEPS, VLAYERS, HFIELDS = data_dict['q'].shape\n",
    "\n",
    "data_dict['fr_land'] = np.repeat(np.expand_dims(data_dict['fr_land'], axis=1), VLAYERS, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1414a1be-faaa-4161-a9cf-73d121d86ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add magnitude of horizontal wind\n",
    "data_dict['U'] = np.sqrt(data_dict['u']**2 + data_dict['v']**2)\n",
    "del data_dict['u']\n",
    "del data_dict['v']\n",
    "\n",
    "# Add RH\n",
    "T0 = 273.15\n",
    "r = 0.00263*data_dict['pa']*data_dict['q']*np.exp((17.67*(data_dict['t']-T0))/(data_dict['t']-29.65))**(-1)\n",
    "data_dict['rh'] = r\n",
    "\n",
    "# Add ps\n",
    "ps = np.repeat(np.expand_dims(data_dict['pa'][:, -1], axis=1), VLAYERS, axis=1)\n",
    "data_dict['ps'] = ps\n",
    "\n",
    "# Removing four upper-most levels\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = data_dict[key][:, 4:].copy()\n",
    "\n",
    "# Data output\n",
    "data_output = data_dict['cc']\n",
    "del data_dict['cc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3e243-33bf-4694-a515-1331908cadda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Requires 15 - 20 minutes: Takes around one minute per folder. There are 14 folders\n",
    "# Requires 133G: Requires 9.5G = 56G/(2*3) per folder\n",
    "\n",
    "# Load derivatives\n",
    "for folder in os.listdir('/home/b/b309170/bd1179_work/ERA5/hvcg_data'):\n",
    "    if folder.endswith('z'):\n",
    "        # Initialize all_npy_files with empty tensor\n",
    "        all_npy_files = np.zeros((0, VLAYERS-4, HFIELDS), dtype=np.float32)\n",
    "        \n",
    "        # Load all filenames in the folder containing the derivatives. The filenames are sorted chronologically.\n",
    "        npy_file_names = sorted(os.listdir(os.path.join('/home/b/b309170/bd1179_work/ERA5/hvcg_data', folder)))        \n",
    "        \n",
    "        for file in npy_file_names:\n",
    "            # Load three-hourly data and convert directly to float32\n",
    "            npy_file = np.load('/home/b/b309170/bd1179_work/ERA5/hvcg_data/%s/%s'%(folder,file), mmap_mode='r')\n",
    "            npy_file = np.float32(npy_file[0::3].copy())\n",
    "            all_npy_files = np.concatenate((all_npy_files, npy_file), axis=0)\n",
    "        data_dict[folder] = all_npy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caace00-5183-4e49-af0d-d318dbb6d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = len(data_dict.keys())\n",
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2aa85-1c24-4528-9353-bd15bfb525da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading takes ~3 hours, 15 minutes\n",
    "print('End of data loading:')\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a90a0b-d8cc-4f5b-a067-d26550383de8",
   "metadata": {},
   "source": [
    "**All features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9230d6-2120-46ce-8567-9e8de58b8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nn = ['q', 'clwc', 'ciwc', 't', 'pa', 'zg', 'fr_land', 'U', 'rh', 'ps', 'q_z', 'q_zz', 'clwc_z',\\\n",
    "            'clwc_zz', 'ciwc_z', 'ciwc_zz', 't_z', 't_zz', 'pa_z', 'pa_zz', 'U_z', 'U_zz', 'rh_z', 'rh_zz']\n",
    "\n",
    "k = 0\n",
    "loc = {}\n",
    "for feat in features_nn:\n",
    "    loc[feat] = k\n",
    "    k = k + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9c2cc-9765-4ec2-a01c-dbfa3344e754",
   "metadata": {},
   "source": [
    "**Cast dict into ndarray and reshape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a348a6-ca0d-4b04-ba81-6169b547dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_array = np.zeros((data_dict['q'].size, len(data_dict.keys())), dtype=np.float32)\n",
    "\n",
    "k = 0\n",
    "data_array_not_T = []\n",
    "for key in features_nn:\n",
    "    data_array_not_T.append(np.reshape(data_dict[key], -1))\n",
    "    del data_dict[key]\n",
    "    k += 1\n",
    "\n",
    "# Convert into np array and transpose\n",
    "data_array = np.transpose(np.array(data_array_not_T, dtype=np.float32))\n",
    "data_output = np.reshape(data_output, -1)\n",
    "\n",
    "del data_array_not_T\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b5932-7459-452c-8675-d722d1644cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think this takes around 3.5 hours\n",
    "print('End of casting:')\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17188d44-5adc-4ab2-bf8c-bf88cc624435",
   "metadata": {},
   "source": [
    "**Pick the subset to train on. Only relevant if tl_bool is True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e3f23-f0f7-4932-8704-99a9e4e241bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.seed(SEED)\n",
    "subset = np.random.randint(0, HFIELDS, number_horizontal_locations)\n",
    "# Convert to regular int to make check_sum JSON serializable\n",
    "check_sum = int(np.sum(subset))\n",
    "\n",
    "# Collecting all grid cell indices for the horizontal fields given by subset\n",
    "Z = np.zeros((TIMESTEPS, 27, HFIELDS), dtype=int)\n",
    "for k in range(HFIELDS):\n",
    "    Z[:,:,k] = k\n",
    "Z_res = np.reshape(Z, -1)\n",
    "subset_inds = np.concatenate([np.where(Z_res == s)[0] for s in subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591ec13-d715-441b-947c-d658be41f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = data_array[subset_inds[:num_cells]] #num_hours*27\n",
    "train_output = data_output[subset_inds[:num_cells]] #num_hours*27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb343a77-0074-4663-8c9c-12dba3a206a1",
   "metadata": {},
   "source": [
    "**2) Loop through SFS NNs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d36e0-dcbd-4f3f-9ff1-782f8b480b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_features(sfs_ind):\n",
    "    '''\n",
    "        Extract the relevant feature names and their order for a given SFS NN\n",
    "    '''\n",
    "    conv = {'cli': 'ciwc', 'clw': 'clwc', 'ta': 't', 'ta_z': 't_z'}\n",
    "    with open('/home/b/b309170/workspace_icon-ml/cloud_cover_parameterization/neighborhood_based_on_seq_feat_sel_DYAMOND/saved_models/neighborhood_based_sfs_cl_area_no_features_%d.txt'%sfs_ind, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for k in range(len(lines)):\n",
    "            if lines[k].startswith('The (order of) input variables'):\n",
    "                out_line = lines[k+1][1:-2].split(' ')\n",
    "    for ind in range(len(out_line)):\n",
    "        out_line[ind] = out_line[ind][1:-1]\n",
    "        # Rename if the name is different in ERA5\n",
    "        if out_line[ind] in conv.keys():\n",
    "            out_line[ind] = conv[out_line[ind]]\n",
    "    return out_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6abdcd2-ad4b-4b5a-b565-3ce65311037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start training:')\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25b4bd53-728c-4ce2-bdde-47a26904093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 1/5\n",
      "4/4 - 6s - loss: 934.1410 - val_loss: 716.8510\n",
      "Epoch 2/5\n",
      "4/4 - 5s - loss: 886.0610 - val_loss: 714.3365\n",
      "Epoch 3/5\n",
      "4/4 - 5s - loss: 875.3347 - val_loss: 709.9414\n",
      "Epoch 4/5\n",
      "4/4 - 5s - loss: 864.7082 - val_loss: 704.7211\n",
      "Epoch 5/5\n",
      "4/4 - 5s - loss: 814.8870 - val_loss: 708.5840\n",
      "Restore model weights from the end of the best epoch\n",
      "Saved model to disk\n",
      "700.57574\n",
      "Starting training\n",
      "Epoch 1/5\n",
      "4/4 - 6s - loss: 714.5229 - val_loss: 557.0905\n",
      "Epoch 2/5\n",
      "4/4 - 5s - loss: 501.7715 - val_loss: 559.8063\n",
      "Epoch 3/5\n",
      "4/4 - 5s - loss: 518.5615 - val_loss: 547.6897\n",
      "Epoch 4/5\n",
      "4/4 - 5s - loss: 398.9344 - val_loss: 529.5034\n",
      "Epoch 5/5\n",
      "4/4 - 5s - loss: 346.3876 - val_loss: 508.9203\n",
      "Restore model weights from the end of the best epoch\n",
      "Saved model to disk\n",
      "492.85507\n",
      "Starting training\n",
      "Epoch 1/5\n",
      "4/4 - 6s - loss: 2715.5110 - val_loss: 404.7828\n",
      "Epoch 2/5\n",
      "4/4 - 5s - loss: 2323.5691 - val_loss: 391.5742\n",
      "Epoch 3/5\n",
      "4/4 - 5s - loss: 2110.9302 - val_loss: 398.5379\n",
      "Epoch 4/5\n",
      "4/4 - 5s - loss: 1868.7275 - val_loss: 417.3879\n",
      "Epoch 5/5\n",
      "4/4 - 5s - loss: 1718.8448 - val_loss: 457.3786\n",
      "Restore model weights from the end of the best epoch\n",
      "Saved model to disk\n",
      "378.50266\n",
      "Starting training\n",
      "Epoch 1/5\n",
      "4/4 - 5s - loss: 74.1628 - val_loss: 402.2884\n",
      "Epoch 2/5\n",
      "4/4 - 4s - loss: 72.1834 - val_loss: 395.0791\n",
      "Epoch 3/5\n",
      "4/4 - 4s - loss: 70.2391 - val_loss: 387.7393\n",
      "Epoch 4/5\n",
      "4/4 - 4s - loss: 68.7543 - val_loss: 379.8194\n",
      "Epoch 5/5\n",
      "4/4 - 4s - loss: 66.4743 - val_loss: 372.4600\n",
      "Restore model weights from the end of the best epoch\n",
      "Saved model to disk\n",
      "361.1719\n",
      "Starting training\n",
      "Epoch 1/5\n",
      "4/4 - 5s - loss: 49.2723 - val_loss: 362.5023\n",
      "Epoch 2/5\n",
      "4/4 - 4s - loss: 43.0462 - val_loss: 339.6852\n",
      "Epoch 3/5\n",
      "4/4 - 4s - loss: 36.2915 - val_loss: 319.1663\n",
      "Epoch 4/5\n",
      "4/4 - 4s - loss: 31.8642 - val_loss: 299.3028\n",
      "Epoch 5/5\n",
      "4/4 - 4s - loss: 28.6649 - val_loss: 281.7976\n",
      "Restore model weights from the end of the best epoch\n",
      "Saved model to disk\n",
      "273.24673\n",
      "Starting training\n",
      "Epoch 1/5\n",
      "4/4 - 5s - loss: 45.5583 - val_loss: 367.2105\n",
      "Epoch 2/5\n",
      "4/4 - 4s - loss: 37.5474 - val_loss: 345.6692\n",
      "Epoch 3/5\n",
      "4/4 - 4s - loss: 30.3116 - val_loss: 326.4822\n",
      "Epoch 4/5\n",
      "4/4 - 4s - loss: 25.4864 - val_loss: 308.6055\n",
      "Epoch 5/5\n",
      "4/4 - 5s - loss: 21.4909 - val_loss: 293.5009\n",
      "Restore model weights from the end of the best epoch\n",
      "Saved model to disk\n",
      "285.06882\n",
      "Starting training\n",
      "Epoch 1/5\n",
      "4/4 - 5s - loss: 2402.5737 - val_loss: 430.0637\n",
      "Epoch 2/5\n",
      "4/4 - 5s - loss: 1595.0564 - val_loss: 461.3102\n",
      "Epoch 3/5\n",
      "4/4 - 5s - loss: 1359.3600 - val_loss: 477.8105\n",
      "Epoch 4/5\n",
      "4/4 - 5s - loss: 1208.4679 - val_loss: 487.8199\n",
      "Epoch 5/5\n",
      "4/4 - 5s - loss: 1038.6869 - val_loss: 492.9103\n",
      "Restore model weights from the end of the best epoch\n",
      "Saved model to disk\n",
      "424.12848\n",
      "Starting training\n",
      "Epoch 1/5\n",
      "4/4 - 6s - loss: 2600.7654 - val_loss: 389.0627\n",
      "Epoch 2/5\n",
      "4/4 - 6s - loss: 1486.4230 - val_loss: 439.5523\n",
      "Epoch 3/5\n",
      "4/4 - 5s - loss: 895.5630 - val_loss: 479.8162\n",
      "Epoch 4/5\n",
      "4/4 - 5s - loss: 768.7917 - val_loss: 505.2999\n",
      "Epoch 5/5\n",
      "4/4 - 5s - loss: 600.8098 - val_loss: 530.4711\n",
      "Restore model weights from the end of the best epoch\n",
      "Saved model to disk\n",
      "383.6447\n",
      "Starting training\n",
      "Epoch 1/5\n",
      "4/4 - 6s - loss: 2165.6833 - val_loss: 419.7026\n",
      "Epoch 2/5\n",
      "4/4 - 5s - loss: 1329.8304 - val_loss: 456.1486\n",
      "Epoch 3/5\n",
      "4/4 - 5s - loss: 905.3394 - val_loss: 456.4691\n",
      "Epoch 4/5\n",
      "4/4 - 5s - loss: 674.8954 - val_loss: 443.5527\n",
      "Epoch 5/5\n",
      "4/4 - 5s - loss: 472.8997 - val_loss: 418.9044\n",
      "Restore model weights from the end of the best epoch\n",
      "Saved model to disk\n",
      "411.46332\n",
      "Starting training\n",
      "Epoch 1/5\n",
      "4/4 - 6s - loss: 2878.0974 - val_loss: 400.7669\n",
      "Epoch 2/5\n",
      "4/4 - 5s - loss: 1107.0999 - val_loss: 418.0350\n",
      "Epoch 3/5\n",
      "4/4 - 5s - loss: 982.5782 - val_loss: 431.1464\n",
      "Epoch 4/5\n",
      "4/4 - 5s - loss: 835.5481 - val_loss: 433.1875\n",
      "Epoch 5/5\n",
      "4/4 - 5s - loss: 564.8787 - val_loss: 427.7270\n",
      "Restore model weights from the end of the best epoch\n",
      "Saved model to disk\n",
      "394.77554\n"
     ]
    }
   ],
   "source": [
    "results = {} \n",
    "all_preds = []\n",
    "for sfs_ind in range(1, 11):\n",
    "    features_nn = which_features(sfs_ind)\n",
    "\n",
    "    nn_path = '/home/b/b309170/workspace_icon-ml/cloud_cover_parameterization/neighborhood_based_on_seq_feat_sel_DYAMOND/saved_models'\n",
    "\n",
    "    custom_objects = {}\n",
    "    custom_objects['leaky_relu'] = nn.leaky_relu\n",
    "\n",
    "    # Select the appropriate features    \n",
    "    features_inds = []\n",
    "    for k in range(sfs_ind):\n",
    "        features_inds.append(loc[features_nn[k]])\n",
    "    data_array_sfs_nn = data_array[:, features_inds]\n",
    "    train_input_sfs_nn = train_input[:, features_inds]\n",
    "\n",
    "    if sfs_ind in [4,5,6,7]:\n",
    "        if sfs_ind == 4:\n",
    "            thrd_lay = 'False'\n",
    "        else:\n",
    "            thrd_lay = 'True'\n",
    "        model_name = 'hyperparameter_tests/neighborhood_based_sfs_cl_area_no_features_%d_False_%s_16'%(sfs_ind,thrd_lay)\n",
    "        if sfs_ind == 7:\n",
    "            model_name = 'hyperparameter_tests/neighborhood_based_sfs_cl_area_no_features_7_True_True_32'\n",
    "    else:\n",
    "        model_name = 'neighborhood_based_sfs_cl_area_no_features_%d'%sfs_ind\n",
    "\n",
    "    # Override model name and model path if the model was already trained    \n",
    "    if already_trained:\n",
    "        nn_path = '/home/b/b309170/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/models'\n",
    "        model_name = 'SFS_NN_%d_tl_%d_seed_%d.h5'%(sfs_ind, subset_exp, SEED)\n",
    "\n",
    "    model = load_model(os.path.join(nn_path, model_name + '.h5'), custom_objects)\n",
    "\n",
    "    mean, std = read_mean_and_std(os.path.join(nn_path, model_name + '.txt'))\n",
    "\n",
    "    # To ensure that the matrices stay in float32\n",
    "    mean = np.float32(mean)\n",
    "    std = np.float32(std)\n",
    "\n",
    "    data_array_scaled = np.float32((data_array_sfs_nn - mean)/std)\n",
    "    train_input_scaled = np.float32((train_input_sfs_nn - mean)/std)\n",
    "\n",
    "    del data_array_sfs_nn, train_input_sfs_nn\n",
    "    gc.collect()\n",
    "\n",
    "    # If tl_bool, we transfer learn to a subset first before evaluating the model!\n",
    "    if tl_bool:\n",
    "        parent_key = 'SFS_NN_%d_tl_%d_num_cells_%d_seed_%d'%(sfs_ind, subset_exp, num_cells, SEED)\n",
    "        results[parent_key] = {}  \n",
    "        results['number_horizontal_locations'] = number_horizontal_locations\n",
    "\n",
    "        ## Training the model ##\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=LR_INIT, epsilon=0.1),\n",
    "            loss=tf.keras.losses.MeanSquaredError()\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        time_callback = TimeOut(t0, timeout)\n",
    "\n",
    "        print('Should be 3693600')\n",
    "        print(train_input_scaled.shape)\n",
    "        print(train_output.shape)\n",
    "\n",
    "        # 20 mins per epoch\n",
    "        history = model.fit(x=train_input_scaled, y=train_output,\n",
    "                            epochs=50, verbose=2, callbacks=[time_callback])\n",
    "\n",
    "        #Serialize model to YAML\n",
    "        model_json = model.to_json()\n",
    "        with open(os.path.join(PWD, 'results/era5_1979-2021/models', parent_key+\".json\"), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        #Serialize model and weights to a single HDF5-file\n",
    "        model.save(os.path.join(PWD, 'results/era5_1979-2021/models', parent_key+'.h5'), \"w\")\n",
    "        print('Saved model to disk')\n",
    "\n",
    "        #Plot the training history\n",
    "        # if len(history.history['loss']) > len(history.history['val_loss']):\n",
    "        #     del history.history['loss'][-1]\n",
    "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "        plt.grid(True)\n",
    "        plt.ylabel('Mean Squared Error')\n",
    "        plt.xlabel('Number of epochs')\n",
    "        plt.savefig(os.path.join(PWD, 'results/era5_1979-2021/models', parent_key+'.pdf'))\n",
    "    else:\n",
    "        parent_key = 'SFS_NN_%d'%sfs_ind\n",
    "        results[parent_key] = {}\n",
    "\n",
    "## Looks like we have to outsource the predictions... ##\n",
    "\n",
    "    # del train_input_scaled\n",
    "    # gc.collect()\n",
    "\n",
    "#                 predictions = model.predict_on_batch(data_array_scaled)\n",
    "#                 predictions = np.minimum(np.maximum(predictions, 0), 100)\n",
    "\n",
    "#                 # Mean-squared error\n",
    "#                 mse = np.mean((predictions[:, 0] - data_output)**2)\n",
    "#                 results[parent_key]['MSE'] = float(mse)\n",
    "#                 print(mse)\n",
    "\n",
    "#                 # R2-value\n",
    "#                 r2 = 1 - mse/np.var(data_output)\n",
    "#                 results[parent_key]['R2'] = float(r2)\n",
    "\n",
    "#                 all_preds.append(predictions)\n",
    "\n",
    "#                 # ## Save plot\n",
    "#                 # plt.hist(predictions,bins=100)\n",
    "#                 # plt.hist(data_output,bins=100,alpha=0.7)\n",
    "#                 # plt.yscale('log')\n",
    "#                 # plt.legend(['NN', 'ERA5'])\n",
    "#                 # plt.savefig('/home/b/b309170/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/SFS_NN_%d.pdf'%sfs_ind)\n",
    "#                 # plt.clf()\n",
    "\n",
    "#             # Dump all SFS NN results    \n",
    "#             append_dict_to_json(results, '/home/b/b309170/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/sfs_based_nn.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c0010-c02d-4a4c-9faf-3aa5a4cc72e1",
   "metadata": {},
   "source": [
    "**Extra plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e2f73-ecbc-4b84-a700-caa2c0dc4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All predictions in one plot instead of 10 plots?\n",
    "# plt.hist(data_output,bins=100,histtype='step',color='k')\n",
    "# for k in range(0,3):\n",
    "#     plt.hist(all_preds[k],bins=100,histtype='step')\n",
    "# plt.yscale('log')\n",
    "# plt.legend(['ERA5', 'SFS_NN_1', 'SFS_NN_2', 'SFS_NN_3'])\n",
    "# plt.savefig('/home/b/b309170/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/SFS_NN_1-3.pdf')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8cf390-e489-4bfb-afea-3653e80b56c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All predictions in one plot instead of 10 plots?\n",
    "# plt.hist(data_output,bins=100,histtype='step',color='k')\n",
    "# for k in range(3,6):\n",
    "#     plt.hist(all_preds[k],bins=100,histtype='step')\n",
    "# plt.yscale('log')\n",
    "# plt.legend(['ERA5', 'SFS_NN_4', 'SFS_NN_5', 'SFS_NN_6'])\n",
    "# plt.savefig('/home/b/b309170/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/SFS_NN_4-6.pdf')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d46859-bdb8-4acf-b22e-6ae4055038dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All predictions in one plot instead of 10 plots?\n",
    "# plt.hist(data_output,bins=100,histtype='step',color='k')\n",
    "# for k in range(6, 10):\n",
    "#     plt.hist(all_preds[k],bins=100,histtype='step')\n",
    "# plt.yscale('log')\n",
    "# plt.legend(['ERA5', 'SFS_NN_7', 'SFS_NN_8', 'SFS_NN_9', 'SFS_NN_10'])\n",
    "# plt.savefig('/home/b/b309170/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/SFS_NN_7-10.pdf')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e33cc-19ea-4b93-aff7-82bb2ec8e922",
   "metadata": {},
   "source": [
    "**Plot R2 vs number of features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454785b8-03ec-4052-a630-e0c891b24f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/b/b309170/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/sfs_based_nn.json', 'r') as file:\n",
    "#     d = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753ed54-c9bb-4709-8d16-7e9cb5044181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %matplotlib inline\n",
    "# plt.plot(np.arange(1,11), [d['SFS_NN_%d'%k]['R2'] for k in range(1,11)], 'bo')\n",
    "# plt.xlabel('Number of features')\n",
    "# plt.ylabel('R2 score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds",
   "language": "python",
   "name": "clouds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

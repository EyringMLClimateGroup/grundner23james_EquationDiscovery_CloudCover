{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867c0ea0-1ecb-4102-be68-0a813a79c426",
   "metadata": {},
   "source": [
    "### Evaluate SFS NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b29fbd-86eb-421d-b24d-c8fd6eb242cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need 960GB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68116c36-a723-447a-be69-3bfe4c0afb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import nn \n",
    "from tensorflow.keras import backend as K \n",
    "\n",
    "sys.path.insert(0, '~/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "import my_classes\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import load_data\n",
    "\n",
    "sys.path.insert(0, '~/workspace_icon-ml/symbolic_regression/')\n",
    "from functions import append_dict_to_json\n",
    "\n",
    "model_type = 'trained' # ['trained', 'original']\n",
    "print(model_type)\n",
    "\n",
    "# Good performance with bs_exp = 23 and on a gpu\n",
    "# OOM when bs_exp too high, but possibly bs_exp > 23 would be better.\n",
    "bs_exp = int(sys.argv[1]) # 23\n",
    "print(bs_exp)\n",
    "\n",
    "# num_cells = int(sys.argv[2]) #[1, 8, 32]\n",
    "SFS_MODEL = int(sys.argv[2]) #[1, ..., 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740906a-9ecc-4689-b09b-3e2925aaf5c9",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9138e17c-8379-4b8e-9396-13ee2d5d2ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "Assertion warning. Max cc not 100. Instead:\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "order_of_vars = ['q', 'clwc', 'ciwc', 't', 'pa', 'u', 'v', 'zg', 'fr_land', 'cc']\n",
    "data_dict = load_data(source='era5', days='all', order_of_vars=order_of_vars) #!\n",
    "\n",
    "TIMESTEPS, VLAYERS, HFIELDS = data_dict['q'].shape\n",
    "\n",
    "data_dict['fr_land'] = np.repeat(np.expand_dims(data_dict['fr_land'], axis=1), VLAYERS, axis=1)\n",
    "\n",
    "# Add magnitude of horizontal wind\n",
    "data_dict['U'] = np.sqrt(data_dict['u']**2 + data_dict['v']**2)\n",
    "del data_dict['u']\n",
    "del data_dict['v']\n",
    "\n",
    "# Add RH\n",
    "T0 = 273.15\n",
    "r = 0.00263*data_dict['pa']*data_dict['q']*np.exp((17.67*(data_dict['t']-T0))/(data_dict['t']-29.65))**(-1)\n",
    "data_dict['rh'] = r\n",
    "\n",
    "# Add ps\n",
    "ps = np.repeat(np.expand_dims(data_dict['pa'][:, -1], axis=1), VLAYERS, axis=1)\n",
    "data_dict['ps'] = ps\n",
    "\n",
    "# Removing four upper-most levels\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = data_dict[key][:, 4:].copy()\n",
    "\n",
    "# Data output\n",
    "data_output = data_dict['cc']\n",
    "del data_dict['cc']\n",
    "\n",
    "# ## LESS DATA ## #!\n",
    "# for key in data_dict.keys():\n",
    "#     data_dict[key] = data_dict[key][0::3]\n",
    "# data_output = data_output[0::3]\n",
    "# TIMESTEPS = TIMESTEPS//3\n",
    "\n",
    "# Requires 15 - 20 minutes: Takes around one minute per folder. There are 14 folders\n",
    "# Requires 133G: Requires 9.5G = 56G/(2*3) per folder\n",
    "\n",
    "# Load derivatives\n",
    "for folder in os.listdir('~/bd1179_work/ERA5/hvcg_data'):\n",
    "    if folder.endswith('z'):\n",
    "        # Initialize all_npy_files with empty tensor\n",
    "        all_npy_files = np.zeros((0, VLAYERS-4, HFIELDS), dtype=np.float32)\n",
    "        \n",
    "        # Load all filenames in the folder containing the derivatives. The filenames are sorted chronologically.\n",
    "        npy_file_names = sorted(os.listdir(os.path.join('~/bd1179_work/ERA5/hvcg_data', folder)))        \n",
    "        \n",
    "        for file in npy_file_names: #!\n",
    "            # Load three-hourly data and convert directly to float32\n",
    "            npy_file = np.load('~/bd1179_work/ERA5/hvcg_data/%s/%s'%(folder,file), mmap_mode='r')\n",
    "            npy_file = np.float32(npy_file[0::3].copy())\n",
    "            all_npy_files = np.concatenate((all_npy_files, npy_file), axis=0)\n",
    "        data_dict[folder] = all_npy_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91811b3d-180f-44c8-aaa2-ba5890f82d61",
   "metadata": {},
   "source": [
    "**All features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f528b0c-7b35-460e-b9ed-a2ac9a6e548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nn = ['q', 'clwc', 'ciwc', 't', 'pa', 'zg', 'fr_land', 'U', 'rh', 'ps', 'q_z', 'q_zz', 'clwc_z',\\\n",
    "            'clwc_zz', 'ciwc_z', 'ciwc_zz', 't_z', 't_zz', 'pa_z', 'pa_zz', 'U_z', 'U_zz', 'rh_z', 'rh_zz']\n",
    "\n",
    "k = 0\n",
    "loc = {}\n",
    "for feat in features_nn:\n",
    "    loc[feat] = k\n",
    "    k = k + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725499b7-f7f5-4be0-b8f6-2633f5019a7f",
   "metadata": {},
   "source": [
    "**Cast dict into ndarray and reshape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43370d5f-0570-4f06-900d-2da5e8a3d9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_array = np.zeros((data_dict['q'].size, len(data_dict.keys())), dtype=np.float32)\n",
    "\n",
    "k = 0\n",
    "data_array_not_T = []\n",
    "for key in features_nn:\n",
    "    data_array_not_T.append(np.reshape(data_dict[key], -1))\n",
    "    del data_dict[key]\n",
    "    k += 1\n",
    "\n",
    "# Convert into np array and transpose\n",
    "data_array = np.transpose(np.array(data_array_not_T, dtype=np.float32))\n",
    "data_output = np.reshape(data_output, -1)\n",
    "\n",
    "del data_array_not_T\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6817b7b6-dbfe-4c51-ab1c-0c22bcfa9f8a",
   "metadata": {},
   "source": [
    "**Loop through SFS NNs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8a5e419-5883-477b-b068-7830b10fe3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_features(sfs_ind):\n",
    "    '''\n",
    "        Extract the relevant feature names and their order for a given SFS NN\n",
    "    '''\n",
    "    conv = {'cli': 'ciwc', 'clw': 'clwc', 'ta': 't', 'ta_z': 't_z'}\n",
    "    with open('~/workspace_icon-ml/cloud_cover_parameterization/neighborhood_based_on_seq_feat_sel_DYAMOND/saved_models/neighborhood_based_sfs_cl_area_no_features_%d.txt'%sfs_ind, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for k in range(len(lines)):\n",
    "            if lines[k].startswith('The (order of) input variables'):\n",
    "                out_line = lines[k+1][1:-2].split(' ')\n",
    "    for ind in range(len(out_line)):\n",
    "        out_line[ind] = out_line[ind][1:-1]\n",
    "        # Rename if the name is different in ERA5\n",
    "        if out_line[ind] in conv.keys():\n",
    "            out_line[ind] = conv[out_line[ind]]\n",
    "    return out_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b405d5-dbee-4f13-b205-786c94e65cbc",
   "metadata": {},
   "source": [
    "**Final cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c030268-126f-44d5-9b1f-4b1d60005684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_small_batches(model, input_data, batch_size=2**20):\n",
    "    # Using predict_on_batch on the entire dataset results in an OOM error\n",
    "    # Curiously it works best if we use predict_on_batch on small subsets of the data instead of predict(..., batch_size=...) \n",
    "    for i in range(1 + input_data.shape[0]//batch_size):\n",
    "        if i == 0:\n",
    "            a = model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            a = np.concatenate((a, model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])), axis=0)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    pred_adj = np.minimum(np.maximum(a, 0), 100) \n",
    "    \n",
    "    return pred_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a899ae5-3dfa-4428-8df7-9f604d5ff221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute only once\n",
    "VAR = np.var(data_output)\n",
    "\n",
    "# For the NNs\n",
    "custom_objects = {}\n",
    "custom_objects['leaky_relu'] = nn.leaky_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf51b834-0b85-4aaa-8cdd-b9dad7197132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628.8782\n",
      "New file created or first entry added\n",
      "266.2604\n",
      "261.9217\n",
      "262.42874\n",
      "253.12701\n",
      "338.86075\n",
      "289.49634\n",
      "273.83743\n",
      "257.10925\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f901952c973f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m## Evaluate model on scaled data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_on_small_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_array_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m# Mean-squared error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-647f21c0ef92>\u001b[0m in \u001b[0;36mpredict_on_small_batches\u001b[0;34m(model, input_data, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1984\u001b[0m       \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_batch_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    947\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/my_work/Miniconda3/envs/clouds/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sfs_ind in range(SFS_MODEL, SFS_MODEL + 1):\n",
    "    ## Get mean and std\n",
    "    nn_path = '~/workspace_icon-ml/cloud_cover_parameterization/neighborhood_based_on_seq_feat_sel_DYAMOND/saved_models'\n",
    "\n",
    "    # Select the appropriate features    \n",
    "    features_inds = []\n",
    "    features_nn = which_features(sfs_ind)\n",
    "    for k in range(sfs_ind):\n",
    "        features_inds.append(loc[features_nn[k]])\n",
    "    data_array_sfs_nn = data_array[:, features_inds]\n",
    "\n",
    "    if sfs_ind in [4,5,6,7]:\n",
    "        if sfs_ind == 4:\n",
    "            thrd_lay = 'False'\n",
    "        else:\n",
    "            thrd_lay = 'True'\n",
    "        model_name = 'hyperparameter_tests/neighborhood_based_sfs_cl_area_no_features_%d_False_%s_16'%(sfs_ind,thrd_lay)\n",
    "        if sfs_ind == 7:\n",
    "            model_name = 'hyperparameter_tests/neighborhood_based_sfs_cl_area_no_features_7_True_True_32'\n",
    "    else:\n",
    "        model_name = 'neighborhood_based_sfs_cl_area_no_features_%d'%sfs_ind\n",
    "\n",
    "    ## Get mean and std from the model-file\n",
    "    mean, std = read_mean_and_std(os.path.join(nn_path, model_name + '.txt'))\n",
    "\n",
    "    ## Scale all data using this mean and std\n",
    "    data_array_scaled = (data_array_sfs_nn - np.float32(mean))/np.float32(std)\n",
    "    \n",
    "    # Case 1\n",
    "    if model_type == 'trained':\n",
    "        for subset_exp in [2]:\n",
    "            for seed in 10*np.arange(1, 7):           \n",
    "\n",
    "                ## Load model\n",
    "                results = {} \n",
    "                parent_key = 'SFS_NN_%d_tl_%d_seed_%d'%(sfs_ind, subset_exp, seed)\n",
    "                results[parent_key] = {}  \n",
    "\n",
    "                nn_path = '~/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/models'\n",
    "\n",
    "                try:\n",
    "                    model = load_model(os.path.join(nn_path, parent_key + '.h5'), custom_objects)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                ## Evaluate model on scaled data\n",
    "                predictions = predict_on_small_batches(model, data_array_scaled)\n",
    "\n",
    "                # Mean-squared error\n",
    "                mse = np.mean((predictions[:, 0] - data_output)**2)\n",
    "                results[parent_key]['MSE'] = float(mse)\n",
    "                print(mse)\n",
    "\n",
    "                # R2-value\n",
    "                r2 = 1 - mse/VAR\n",
    "                results[parent_key]['R2'] = float(r2)\n",
    "\n",
    "                ## Write results to json-file\n",
    "                append_dict_to_json(results, '~/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/sfs_based_nn.json')\n",
    "\n",
    "    # Case 2\n",
    "    if model_type == 'original':\n",
    "        results = {} \n",
    "        parent_key = 'SFS_NN_%d_no_tl'%(sfs_ind)\n",
    "        results[parent_key] = {}  \n",
    "            \n",
    "        model = load_model(os.path.join(nn_path, model_name + '.h5'), custom_objects)\n",
    "\n",
    "        ## Evaluate model on scaled data\n",
    "        predictions = predict_on_small_batches(model, data_array_scaled)\n",
    "\n",
    "        # Mean-squared error\n",
    "        mse = np.mean((predictions[:, 0] - data_output)**2)\n",
    "        results[parent_key]['MSE'] = float(mse)\n",
    "        print(mse)\n",
    "\n",
    "        # R2-value\n",
    "        r2 = 1 - mse/VAR\n",
    "        results[parent_key]['R2'] = float(r2)\n",
    "\n",
    "        ## Write results to json-file\n",
    "        append_dict_to_json(results, '~/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/sfs_based_nn.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds",
   "language": "python",
   "name": "clouds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

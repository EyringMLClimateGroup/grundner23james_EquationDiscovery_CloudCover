{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867c0ea0-1ecb-4102-be68-0a813a79c426",
   "metadata": {},
   "source": [
    "### Evaluate SFS NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b29fbd-86eb-421d-b24d-c8fd6eb242cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need 960GB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68116c36-a723-447a-be69-3bfe4c0afb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import nn \n",
    "from tensorflow.keras import backend as K \n",
    "\n",
    "sys.path.insert(0, '~/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "import my_classes\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import load_data\n",
    "\n",
    "sys.path.insert(0, '~/workspace_icon-ml/symbolic_regression/')\n",
    "from functions import append_dict_to_json\n",
    "\n",
    "model_type = 'trained' # ['trained', 'original']\n",
    "print(model_type)\n",
    "\n",
    "# Good performance with bs_exp = 23 and on a gpu\n",
    "# OOM when bs_exp too high, but possibly bs_exp > 23 would be better.\n",
    "bs_exp = int(sys.argv[1]) # 23\n",
    "print(bs_exp)\n",
    "\n",
    "# num_cells = int(sys.argv[2]) #[1, 8, 32]\n",
    "SFS_MODEL = int(sys.argv[2]) #[1, ..., 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740906a-9ecc-4689-b09b-3e2925aaf5c9",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9138e17c-8379-4b8e-9396-13ee2d5d2ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "Assertion warning. Max cc not 100. Instead:\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "order_of_vars = ['q', 'clwc', 'ciwc', 't', 'pa', 'u', 'v', 'zg', 'fr_land', 'cc']\n",
    "data_dict = load_data(source='era5', days='all', order_of_vars=order_of_vars) #!\n",
    "\n",
    "TIMESTEPS, VLAYERS, HFIELDS = data_dict['q'].shape\n",
    "\n",
    "data_dict['fr_land'] = np.repeat(np.expand_dims(data_dict['fr_land'], axis=1), VLAYERS, axis=1)\n",
    "\n",
    "# Add magnitude of horizontal wind\n",
    "data_dict['U'] = np.sqrt(data_dict['u']**2 + data_dict['v']**2)\n",
    "del data_dict['u']\n",
    "del data_dict['v']\n",
    "\n",
    "# Add RH\n",
    "T0 = 273.15\n",
    "r = 0.00263*data_dict['pa']*data_dict['q']*np.exp((17.67*(data_dict['t']-T0))/(data_dict['t']-29.65))**(-1)\n",
    "data_dict['rh'] = r\n",
    "\n",
    "# Add ps\n",
    "ps = np.repeat(np.expand_dims(data_dict['pa'][:, -1], axis=1), VLAYERS, axis=1)\n",
    "data_dict['ps'] = ps\n",
    "\n",
    "# Removing four upper-most levels\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = data_dict[key][:, 4:].copy()\n",
    "\n",
    "# Data output\n",
    "data_output = data_dict['cc']\n",
    "del data_dict['cc']\n",
    "\n",
    "# ## LESS DATA ## #!\n",
    "# for key in data_dict.keys():\n",
    "#     data_dict[key] = data_dict[key][0::3]\n",
    "# data_output = data_output[0::3]\n",
    "# TIMESTEPS = TIMESTEPS//3\n",
    "\n",
    "# Requires 15 - 20 minutes: Takes around one minute per folder. There are 14 folders\n",
    "# Requires 133G: Requires 9.5G = 56G/(2*3) per folder\n",
    "\n",
    "# Load derivatives\n",
    "for folder in os.listdir('~/bd1179_work/ERA5/hvcg_data'):\n",
    "    if folder.endswith('z'):\n",
    "        # Initialize all_npy_files with empty tensor\n",
    "        all_npy_files = np.zeros((0, VLAYERS-4, HFIELDS), dtype=np.float32)\n",
    "        \n",
    "        # Load all filenames in the folder containing the derivatives. The filenames are sorted chronologically.\n",
    "        npy_file_names = sorted(os.listdir(os.path.join('~/bd1179_work/ERA5/hvcg_data', folder)))        \n",
    "        \n",
    "        for file in npy_file_names: #!\n",
    "            # Load three-hourly data and convert directly to float32\n",
    "            npy_file = np.load('~/bd1179_work/ERA5/hvcg_data/%s/%s'%(folder,file), mmap_mode='r')\n",
    "            npy_file = np.float32(npy_file[0::3].copy())\n",
    "            all_npy_files = np.concatenate((all_npy_files, npy_file), axis=0)\n",
    "        data_dict[folder] = all_npy_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91811b3d-180f-44c8-aaa2-ba5890f82d61",
   "metadata": {},
   "source": [
    "**All features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f528b0c-7b35-460e-b9ed-a2ac9a6e548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nn = ['q', 'clwc', 'ciwc', 't', 'pa', 'zg', 'fr_land', 'U', 'rh', 'ps', 'q_z', 'q_zz', 'clwc_z',\\\n",
    "            'clwc_zz', 'ciwc_z', 'ciwc_zz', 't_z', 't_zz', 'pa_z', 'pa_zz', 'U_z', 'U_zz', 'rh_z', 'rh_zz']\n",
    "\n",
    "k = 0\n",
    "loc = {}\n",
    "for feat in features_nn:\n",
    "    loc[feat] = k\n",
    "    k = k + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725499b7-f7f5-4be0-b8f6-2633f5019a7f",
   "metadata": {},
   "source": [
    "**Cast dict into ndarray and reshape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43370d5f-0570-4f06-900d-2da5e8a3d9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_array = np.zeros((data_dict['q'].size, len(data_dict.keys())), dtype=np.float32)\n",
    "\n",
    "k = 0\n",
    "data_array_not_T = []\n",
    "for key in features_nn:\n",
    "    data_array_not_T.append(np.reshape(data_dict[key], -1))\n",
    "    del data_dict[key]\n",
    "    k += 1\n",
    "\n",
    "# Convert into np array and transpose\n",
    "data_array = np.transpose(np.array(data_array_not_T, dtype=np.float32))\n",
    "data_output = np.reshape(data_output, -1)\n",
    "\n",
    "del data_array_not_T\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6817b7b6-dbfe-4c51-ab1c-0c22bcfa9f8a",
   "metadata": {},
   "source": [
    "**Loop through SFS NNs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8a5e419-5883-477b-b068-7830b10fe3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_features(sfs_ind):\n",
    "    '''\n",
    "        Extract the relevant feature names and their order for a given SFS NN\n",
    "    '''\n",
    "    conv = {'cli': 'ciwc', 'clw': 'clwc', 'ta': 't', 'ta_z': 't_z'}\n",
    "    with open('~/workspace_icon-ml/cloud_cover_parameterization/neighborhood_based_on_seq_feat_sel_DYAMOND/saved_models/neighborhood_based_sfs_cl_area_no_features_%d.txt'%sfs_ind, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for k in range(len(lines)):\n",
    "            if lines[k].startswith('The (order of) input variables'):\n",
    "                out_line = lines[k+1][1:-2].split(' ')\n",
    "    for ind in range(len(out_line)):\n",
    "        out_line[ind] = out_line[ind][1:-1]\n",
    "        # Rename if the name is different in ERA5\n",
    "        if out_line[ind] in conv.keys():\n",
    "            out_line[ind] = conv[out_line[ind]]\n",
    "    return out_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b405d5-dbee-4f13-b205-786c94e65cbc",
   "metadata": {},
   "source": [
    "**Final cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c030268-126f-44d5-9b1f-4b1d60005684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_small_batches(model, input_data, batch_size=2**20):\n",
    "    # Using predict_on_batch on the entire dataset results in an OOM error\n",
    "    # Curiously it works best if we use predict_on_batch on small subsets of the data instead of predict(..., batch_size=...) \n",
    "    for i in range(1 + input_data.shape[0]//batch_size):\n",
    "        if i == 0:\n",
    "            a = model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            a = np.concatenate((a, model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])), axis=0)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    pred_adj = np.minimum(np.maximum(a, 0), 100) \n",
    "    \n",
    "    return pred_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a899ae5-3dfa-4428-8df7-9f604d5ff221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute only once\n",
    "VAR = np.var(data_output)\n",
    "\n",
    "# For the NNs\n",
    "custom_objects = {}\n",
    "custom_objects['leaky_relu'] = nn.leaky_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51b834-0b85-4aaa-8cdd-b9dad7197132",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sfs_ind in range(SFS_MODEL, SFS_MODEL + 1):\n",
    "    ## Get mean and std\n",
    "    nn_path = '~/workspace_icon-ml/cloud_cover_parameterization/neighborhood_based_on_seq_feat_sel_DYAMOND/saved_models'\n",
    "\n",
    "    # Select the appropriate features    \n",
    "    features_inds = []\n",
    "    features_nn = which_features(sfs_ind)\n",
    "    for k in range(sfs_ind):\n",
    "        features_inds.append(loc[features_nn[k]])\n",
    "    data_array_sfs_nn = data_array[:, features_inds]\n",
    "\n",
    "    if sfs_ind in [4,5,6,7]:\n",
    "        if sfs_ind == 4:\n",
    "            thrd_lay = 'False'\n",
    "        else:\n",
    "            thrd_lay = 'True'\n",
    "        model_name = 'hyperparameter_tests/neighborhood_based_sfs_cl_area_no_features_%d_False_%s_16'%(sfs_ind,thrd_lay)\n",
    "        if sfs_ind == 7:\n",
    "            model_name = 'hyperparameter_tests/neighborhood_based_sfs_cl_area_no_features_7_True_True_32'\n",
    "    else:\n",
    "        model_name = 'neighborhood_based_sfs_cl_area_no_features_%d'%sfs_ind\n",
    "\n",
    "    ## Get mean and std from the model-file\n",
    "    mean, std = read_mean_and_std(os.path.join(nn_path, model_name + '.txt'))\n",
    "\n",
    "    ## Scale all data using this mean and std\n",
    "    data_array_scaled = (data_array_sfs_nn - np.float32(mean))/np.float32(std)\n",
    "    \n",
    "    # Case 1\n",
    "    if model_type == 'trained':\n",
    "        for subset_exp in [2]:\n",
    "            for seed in 10*np.arange(1, 7):           \n",
    "\n",
    "                ## Load model\n",
    "                results = {} \n",
    "                parent_key = 'SFS_NN_%d_tl_%d_seed_%d'%(sfs_ind, subset_exp, seed)\n",
    "                results[parent_key] = {}  \n",
    "\n",
    "                nn_path = '~/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/models'\n",
    "\n",
    "                try:\n",
    "                    model = load_model(os.path.join(nn_path, parent_key + '.h5'), custom_objects)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                ## Evaluate model on scaled data\n",
    "                predictions = predict_on_small_batches(model, data_array_scaled)\n",
    "\n",
    "                # Mean-squared error\n",
    "                mse = np.mean((predictions[:, 0] - data_output)**2)\n",
    "                results[parent_key]['MSE'] = float(mse)\n",
    "                print(mse)\n",
    "\n",
    "                # R2-value\n",
    "                r2 = 1 - mse/VAR\n",
    "                results[parent_key]['R2'] = float(r2)\n",
    "\n",
    "                ## Write results to json-file\n",
    "                append_dict_to_json(results, '~/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/sfs_based_nn.json')\n",
    "\n",
    "    # Case 2\n",
    "    if model_type == 'original':\n",
    "        results = {} \n",
    "        parent_key = 'SFS_NN_%d_no_tl'%(sfs_ind)\n",
    "        results[parent_key] = {}  \n",
    "            \n",
    "        model = load_model(os.path.join(nn_path, model_name + '.h5'), custom_objects)\n",
    "\n",
    "        ## Evaluate model on scaled data\n",
    "        predictions = predict_on_small_batches(model, data_array_scaled)\n",
    "\n",
    "        # Mean-squared error\n",
    "        mse = np.mean((predictions[:, 0] - data_output)**2)\n",
    "        results[parent_key]['MSE'] = float(mse)\n",
    "        print(mse)\n",
    "\n",
    "        # R2-value\n",
    "        r2 = 1 - mse/VAR\n",
    "        results[parent_key]['R2'] = float(r2)\n",
    "\n",
    "        ## Write results to json-file\n",
    "        append_dict_to_json(results, '~/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/sfs_based_nn.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds",
   "language": "python",
   "name": "clouds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

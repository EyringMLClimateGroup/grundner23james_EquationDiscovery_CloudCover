{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867c0ea0-1ecb-4102-be68-0a813a79c426",
   "metadata": {},
   "source": [
    "### SR NNs\n",
    "\n",
    "Executed through ~scripts/run_era5_evalute_and_transfer_learn_8.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e6694d-259e-4260-9f57-00923dfd2eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should run with 650GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c00f5-d2b6-4cb8-9fe5-bdf4e68f2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('PDF')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import nn \n",
    "\n",
    "## Transfer learn? ##\n",
    "# tl_bool = bool(int(sys.argv[1]))\n",
    "tl_bool = True\n",
    "\n",
    "# Try 0,1,2 first!!\n",
    "subset_exp = int(sys.argv[1])\n",
    "\n",
    "# The number of samples in the TL training set. Cannot be less than 27.\n",
    "number_horizontal_locations = 10**subset_exp\n",
    "print(subset_exp)\n",
    "# How long to re-train? Setting it to 40 minutes.\n",
    "timeout = 40\n",
    "# The original LR_INIT was 4.33e-4.\n",
    "LR_INIT = 4.33e-4\n",
    "\n",
    "# Run with SEED=10,20,30\n",
    "SEED = int(sys.argv[2])\n",
    "\n",
    "sys.path.insert(0, '~/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "import my_classes\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import load_data\n",
    "from my_classes import TimeOut\n",
    "\n",
    "sys.path.insert(0, '~/workspace_icon-ml/symbolic_regression/')\n",
    "from functions import add_derivatives\n",
    "from functions import append_dict_to_json\n",
    "\n",
    "num_cells = int(sys.argv[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe5914-3d4f-47c2-bed0-bd266ad6d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PWD = '~/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e749b4-81c1-4f7d-8e4a-cd8c9ada2386",
   "metadata": {},
   "source": [
    "**Load data**\n",
    "\n",
    "-- Takes 10 seconds per day <br>\n",
    "-- 28 minutes for 172 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5346f6-c28e-4a44-b614-e8190670e478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes 10-15 minutes. 150 GB suffice here.\n",
    "order_of_vars = ['q', 'clwc', 'ciwc', 't', 'pa', 'u', 'v', 'zg', 'fr_land', 'cc']\n",
    "data_dict = load_data(source='era5', days='all', order_of_vars=order_of_vars)\n",
    "\n",
    "TIMESTEPS, VLAYERS, HFIELDS = data_dict['q'].shape\n",
    "\n",
    "data_dict['fr_land'] = np.repeat(np.expand_dims(data_dict['fr_land'], axis=1), VLAYERS, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1414a1be-faaa-4161-a9cf-73d121d86ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add magnitude of horizontal wind\n",
    "data_dict['U'] = np.sqrt(data_dict['u']**2 + data_dict['v']**2)\n",
    "del data_dict['u']\n",
    "del data_dict['v']\n",
    "\n",
    "# Add RH\n",
    "T0 = 273.15\n",
    "r = 0.00263*data_dict['pa']*data_dict['q']*np.exp((17.67*(data_dict['t']-T0))/(data_dict['t']-29.65))**(-1)\n",
    "data_dict['rh'] = r\n",
    "\n",
    "# Add ps\n",
    "ps = np.repeat(np.expand_dims(data_dict['pa'][:, -1], axis=1), VLAYERS, axis=1)\n",
    "data_dict['ps'] = ps\n",
    "\n",
    "# Removing four upper-most levels\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = data_dict[key][:, 4:].copy()\n",
    "\n",
    "# Data output\n",
    "data_output = data_dict['cc']\n",
    "del data_dict['cc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e978a4-d271-43c5-b243-b4a9d4a3c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires another 133G: Requires 9.5G = 56G/(2*3) per folder\n",
    "# Requires 15 - 20 minutes: Takes around one minute per folder. There are 14 folders\n",
    "# --> Actually it runs much longer than expected! (> 45 minutes)\n",
    "\n",
    "# Load derivatives\n",
    "for folder in os.listdir('~/bd1179_work/ERA5/hvcg_data'):\n",
    "    if folder.endswith('z'):\n",
    "        # Initialize all_npy_files with empty tensor. It's important to specify the dtype here!\n",
    "        all_npy_files = np.zeros((0, VLAYERS-4, HFIELDS), dtype=np.float32)\n",
    "        \n",
    "        # Load all filenames in the folder containing the derivatives. The filenames are sorted chronologically.\n",
    "        npy_file_names = sorted(os.listdir(os.path.join('~/bd1179_work/ERA5/hvcg_data', folder)))        \n",
    "        \n",
    "        for file in npy_file_names:\n",
    "            # Load three-hourly data and convert directly to float32\n",
    "            npy_file = np.load('~/bd1179_work/ERA5/hvcg_data/%s/%s'%(folder,file), mmap_mode='r')\n",
    "            npy_file = np.float32(npy_file[0::3].copy())\n",
    "            all_npy_files = np.concatenate((all_npy_files, npy_file), axis=0)\n",
    "        data_dict[folder] = all_npy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1828e5-02a3-4abe-891e-b32be862e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data_dict.keys():\n",
    "    print(data_dict[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0caace00-5183-4e49-af0d-d318dbb6d36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['q', 'clwc', 'ciwc', 't', 'pa', 'zg', 'fr_land', 'U', 'rh', 'ps', 'pa_z', 'pa_zz', 'ciwc_z', 'clwc_zz', 'clwc_z', 'U_zz', 'rh_z', 'q_zz', 'U_z', 'q_z', 't_z', 't_zz', 'rh_zz', 'ciwc_zz'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_features = len(data_dict.keys())\n",
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f6f21-1831-4a82-99a0-b6253fb110b3",
   "metadata": {},
   "source": [
    "**SR NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687da51-1a97-4610-b3a0-e55739d191e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_path = '~/workspace_icon-ml/cloud_cover_parameterization/neighborhood_based_SR_DYAMOND/saved_models'\n",
    "\n",
    "custom_objects = {}\n",
    "custom_objects['leaky_relu'] = nn.leaky_relu\n",
    "\n",
    "model_name = 'cross_validation_neighborhood_based_sr_cl_area_fold_2.h5'\n",
    "model = load_model(os.path.join(nn_path, model_name), custom_objects)\n",
    "\n",
    "mean, std = read_mean_and_std(os.path.join(nn_path, 'cross_validation_neighborhood_based_sr_cl_area_fold_2.txt'))\n",
    "\n",
    "# To ensure that data_array_scaled will be in float32. \n",
    "# The difference between values is < 1e-5\n",
    "mean = np.float32(mean)\n",
    "std = np.float32(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a90a0b-d8cc-4f5b-a067-d26550383de8",
   "metadata": {},
   "source": [
    "**All features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc9230d6-2120-46ce-8567-9e8de58b8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nn = ['q', 'clwc', 'ciwc', 't', 'pa', 'zg', 'fr_land', 'U', 'rh', 'ps', 'q_z', 'q_zz', 'clwc_z',\\\n",
    "            'clwc_zz', 'ciwc_z', 'ciwc_zz', 't_z', 't_zz', 'pa_z', 'pa_zz', 'U_z', 'U_zz', 'rh_z', 'rh_zz']\n",
    "\n",
    "k = 0\n",
    "loc = {}\n",
    "for feat in features_nn:\n",
    "    loc[feat] = k\n",
    "    k = k + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9c2cc-9765-4ec2-a01c-dbfa3344e754",
   "metadata": {},
   "source": [
    "**Cast dict into ndarray and reshape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98a348a6-ca0d-4b04-ba81-6169b547dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_array = np.zeros((data_dict['q'].size, len(data_dict.keys())), dtype=np.float32)\n",
    "\n",
    "k = 0\n",
    "data_array_scaled = []\n",
    "for key in features_nn:\n",
    "    data_array_scaled.append(np.reshape(data_dict[key], -1))\n",
    "    del data_dict[key]\n",
    "    k += 1\n",
    "\n",
    "# Convert into np array and transpose\n",
    "data_array_scaled = (np.transpose(np.array(data_array_scaled, dtype=np.float32)) - mean)/std\n",
    "data_output = np.reshape(data_output, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17188d44-5adc-4ab2-bf8c-bf88cc624435",
   "metadata": {},
   "source": [
    "**Pick the subset to train on. Only relevant if tl_bool is True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "213e3f23-f0f7-4932-8704-99a9e4e241bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.seed(SEED)\n",
    "subset = np.random.randint(0, HFIELDS, number_horizontal_locations)\n",
    "# Convert to regular int to make check_sum JSON serializable\n",
    "check_sum = int(np.sum(subset))\n",
    "\n",
    "# Collecting all grid cell indices for the horizontal fields given by subset\n",
    "Z = np.zeros((TIMESTEPS, 27, HFIELDS), dtype=int)\n",
    "for k in range(HFIELDS):\n",
    "    Z[:,:,k] = k\n",
    "Z_res = np.reshape(Z, -1)\n",
    "subset_inds = np.concatenate([np.where(Z_res == s)[0] for s in subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8591ec13-d715-441b-947c-d658be41f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_scaled = data_array_scaled[subset_inds[:num_cells]] #num_hours*27\n",
    "train_output = data_output[subset_inds[:num_cells]] #num_hours*27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08275486-eed2-4bca-a178-cce81ca7f8ac",
   "metadata": {},
   "source": [
    "**1) Evaluate SR NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a117e0-dd96-4630-925e-e28cc5713993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# If tl_bool, we transfer learn to a subset first before evaluating the model!\n",
    "if tl_bool:\n",
    "    parent_key = 'SFS_NN_24_tl_%d_num_cells_%d_seed_%d'%(subset_exp, num_cells, SEED)\n",
    "    results[parent_key] = {}  \n",
    "    results['number_horizontal_locations'] = number_horizontal_locations\n",
    "    \n",
    "    ## Training the model ##\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LR_INIT, epsilon=0.1),\n",
    "        loss=tf.keras.losses.MeanSquaredError()\n",
    "    )\n",
    "    \n",
    "    t0 = time.time()\n",
    "    time_callback = TimeOut(t0, timeout)\n",
    "    \n",
    "    print('Should be 3693600')\n",
    "    print(train_input_scaled.shape)\n",
    "    print(train_output.shape)\n",
    "    \n",
    "    # 20 mins per epoch\n",
    "    history = model.fit(x=train_input_scaled, y=train_output, \n",
    "                        epochs=50, verbose=2, callbacks=[time_callback])\n",
    "    \n",
    "    #Save the model\n",
    "    #Serialize model to YAML\n",
    "    model_json = model.to_json()\n",
    "    with open(os.path.join(PWD, 'results/era5_1979-2021/models', parent_key+\".json\"), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    #Serialize model and weights to a single HDF5-file\n",
    "    model.save(os.path.join(PWD, 'results/era5_1979-2021/models', parent_key+'.h5'), \"w\")\n",
    "    print('Saved model to disk')\n",
    "\n",
    "    #Plot the training history\n",
    "    # if len(history.history['loss']) > len(history.history['val_loss']):\n",
    "    #     del history.history['loss'][-1]\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid(True)\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.savefig(os.path.join(PWD, 'results/era5_1979-2021/models', parent_key+'.pdf'))\n",
    "else:\n",
    "    parent_key = 'SFS_NN_24'  \n",
    "    results[parent_key] = {}  \n",
    "    \n",
    "## Looks like we have to outsource the predictions... ##\n",
    "\n",
    "# # Reduces memory requirement by roughly 200GB    \n",
    "# del train_input_scaled, train_output\n",
    "# gc.collect()\n",
    "    \n",
    "# predictions = model.predict_on_batch(data_array_scaled)\n",
    "# predictions = np.minimum(np.maximum(predictions, 0), 100)\n",
    "\n",
    "# # Mean-squared error\n",
    "# mse = np.mean((predictions[:, 0] - data_output)**2)\n",
    "# results[parent_key]['MSE'] = float(mse)\n",
    "# print(mse)\n",
    "\n",
    "# # R2-value\n",
    "# r2 = 1 - mse/np.var(data_output)\n",
    "# results[parent_key]['R2'] = float(r2)\n",
    "\n",
    "# # # Plot results\n",
    "# # plt.hist(predictions,bins=100)\n",
    "# # plt.hist(data_output,bins=100,alpha=0.7)\n",
    "# # plt.yscale('log')\n",
    "# # plt.legend(['NN', 'ERA5'])\n",
    "# # plt.savefig('~/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/SR_NN.pdf')\n",
    "# # plt.clf()\n",
    "\n",
    "# results[parent_key]['Check_sum'] = check_sum\n",
    "\n",
    "# # Dump results\n",
    "# append_dict_to_json(results, '~/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/sr_based_nn.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c0010-c02d-4a4c-9faf-3aa5a4cc72e1",
   "metadata": {},
   "source": [
    "**Extra plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e2f73-ecbc-4b84-a700-caa2c0dc4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All predictions in one plot instead of 10 plots?\n",
    "# plt.hist(data_output,bins=100,histtype='step',color='k')\n",
    "# for k in range(0,3):\n",
    "#     plt.hist(all_preds[k],bins=100,histtype='step')\n",
    "# plt.yscale('log')\n",
    "# plt.legend(['ERA5', 'SFS_NN_1', 'SFS_NN_2', 'SFS_NN_3'])\n",
    "# plt.savefig('~/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/SFS_NN_1-3.pdf')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8cf390-e489-4bfb-afea-3653e80b56c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All predictions in one plot instead of 10 plots?\n",
    "# plt.hist(data_output,bins=100,histtype='step',color='k')\n",
    "# for k in range(3,6):\n",
    "#     plt.hist(all_preds[k],bins=100,histtype='step')\n",
    "# plt.yscale('log')\n",
    "# plt.legend(['ERA5', 'SFS_NN_4', 'SFS_NN_5', 'SFS_NN_6'])\n",
    "# plt.savefig('~/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/SFS_NN_4-6.pdf')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d46859-bdb8-4acf-b22e-6ae4055038dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All predictions in one plot instead of 10 plots?\n",
    "# plt.hist(data_output,bins=100,histtype='step',color='k')\n",
    "# for k in range(6, 10):\n",
    "#     plt.hist(all_preds[k],bins=100,histtype='step')\n",
    "# plt.yscale('log')\n",
    "# plt.legend(['ERA5', 'SFS_NN_7', 'SFS_NN_8', 'SFS_NN_9', 'SFS_NN_10'])\n",
    "# plt.savefig('~/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/SFS_NN_7-10.pdf')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e33cc-19ea-4b93-aff7-82bb2ec8e922",
   "metadata": {},
   "source": [
    "**Plot R2 vs number of features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454785b8-03ec-4052-a630-e0c891b24f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('~/workspace_icon-ml/symbolic_regression/evaluate_schemes/on_era5/results/era5_1979-2021/sfs_based_nn.json', 'r') as file:\n",
    "#     d = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753ed54-c9bb-4709-8d16-7e9cb5044181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %matplotlib inline\n",
    "# plt.plot(np.arange(1,11), [d['SFS_NN_%d'%k]['R2'] for k in range(1,11)], 'bo')\n",
    "# plt.xlabel('Number of features')\n",
    "# plt.ylabel('R2 score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds",
   "language": "python",
   "name": "clouds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

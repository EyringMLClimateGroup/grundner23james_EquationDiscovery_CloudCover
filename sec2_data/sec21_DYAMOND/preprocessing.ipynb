{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing Dyamond data\n",
    "\n",
    "1) We read the data\n",
    "2) Reshape variables so that they have equal dimensionality\n",
    "3) Reshape into data samples fit for the NN and convert into a DataFrame\n",
    "4) Downsample the data: Remove data above 21kms, remove condensate-free clouds, combat class-imbalance\n",
    "5) Split into input and output\n",
    "6) Save as npy\n",
    "\n",
    "Note: We neither scale nor split the data into training/validation/test sets already in this notebook. <br>\n",
    "The reason is that i) in order to scale we need the entire dataset but this can only be done in conjunction with the dyamond dataset. Also for cross-validation different scalings will be necessary based on different subsets of the data, ii) The split into subsets will be done by the cross-validation procedure or not at all when training the final model.\n",
    "\n",
    "*To compute the derivatives, I had to run eight duplicates of this notebook, only computing the derivatives for one variable at a time (to not run OOM). And at the end I had to piece together the resulting npy-files.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ran with 900GB\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "# import importlib\n",
    "# importlib.reload(my_classes)\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from functions import add_derivatives\n",
    "from my_classes import load_data\n",
    "\n",
    "output_path = '~/my_work/icon-ml_data/cloud_cover_parameterization/neighborhood_based_SR_DYAMOND'\n",
    "\n",
    "VERT_LAYERS = 31\n",
    "\n",
    "## Parameters for the notebook\n",
    "\n",
    "#Set a numpy seed for the permutation later on!\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Reading the data\n",
    "\n",
    "The data above 21km is capped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_of_vars_dyamond = ['hus', 'clw', 'cli', 'ta', 'pa', 'ua', 'va', 'zg', 'fr_land', 'clc', 'cl_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clc\n",
      "cl_area\n"
     ]
    }
   ],
   "source": [
    "# Load horizontally and vertically coarse-grained dyamond data\n",
    "data_dict = load_data(source='split_by_var_name', days='discard_spinup', resolution='R02B05', order_of_vars=order_of_vars_dyamond, \n",
    "                      path='~/bd1179_work/DYAMOND/hvcg_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clc (467, 31, 79342)\n",
      "cl_area (467, 31, 79342)\n"
     ]
    }
   ],
   "source": [
    "for key in data_dict.keys():\n",
    "    print(key, data_dict[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(TIME_STEPS, VERT_LAYERS, HORIZ_FIELDS) = data_dict['clc'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #Reshaping into nd-arrays of equaling shapes (don't reshape in the vertical)\n",
    "    data_dict['zg'] = np.repeat(np.expand_dims(data_dict['zg'], 0), TIME_STEPS, axis=0)\n",
    "    data_dict['fr_land'] = np.repeat(np.expand_dims(data_dict['fr_land'], 0), TIME_STEPS, axis=0)\n",
    "    data_dict['fr_land'] = np.repeat(np.expand_dims(data_dict['fr_land'], 1), VERT_LAYERS, axis=1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to float32!\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = np.float32(data_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.000015\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# Our Neural Network has trained with clc in [0, 100]!\n",
    "data_dict['clc'] = 100*data_dict['clc']\n",
    "data_dict['cl_area'] = 100*data_dict['cl_area']\n",
    "print(np.max(data_dict['clc'][:, 4:, :]))\n",
    "print(np.max(data_dict['cl_area'][:, 4:, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carry along information about the vertical layer of a grid cell. int16 is sufficient for < 1000.\n",
    "vert_layers = np.int16(np.repeat(np.expand_dims(np.arange(1, VERT_LAYERS+1), 0), TIME_STEPS, axis=0))\n",
    "vert_layers = np.repeat(np.expand_dims(vert_layers, 2), HORIZ_FIELDS, axis=2)\n",
    "vert_layers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add magnitude of horizontal wind\n",
    "data_dict['U'] = np.sqrt(data_dict['ua']**2 + data_dict['va']**2)\n",
    "del data_dict['ua']\n",
    "del data_dict['va']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add RH\n",
    "T0 = 273.15\n",
    "r = 0.00263*data_dict['pa']*data_dict['hus']*np.exp((17.67*(data_dict['ta']-T0))/(data_dict['ta']-29.65))**(-1)\n",
    "\n",
    "data_dict['rh'] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ps\n",
    "ps = np.repeat(np.expand_dims(data_dict['pa'][:, -1], axis=1), VERT_LAYERS, axis=1)\n",
    "data_dict['ps'] = ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data above 21kms\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = data_dict[key][:, 4:, :]\n",
    "\n",
    "# vert_layers = vert_layers[:, 4:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add derivatives\n",
    "\n",
    "# Required time per variable\n",
    "# On JupyterHub, sequential loop: 129.2 hours\n",
    "# Via SLURM job, sequential loop: 105 hours\n",
    "# 128 processes, parallel loop: 11.6 hours --> Actually, it only seems to take 1.1 hours\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import multiprocessing as mlp\n",
    "import gc\n",
    "\n",
    "@contextmanager\n",
    "def poolcontext(*args, **kwargs):\n",
    "    pool = mlp.Pool(*args, **kwargs)\n",
    "    yield pool\n",
    "    pool.terminate()\n",
    "    \n",
    "def add_derivatives_par(data_dict):\n",
    "    # Define variables for add_derivatives (Add 'zg' at the end)\n",
    "    base_variables = ['hus', 'clw', 'cli', 'ta', 'pa', 'U', 'rh', 'zg']\n",
    "    return add_derivatives(data_dict, base_variables)\n",
    "\n",
    "procs = 128\n",
    "with poolcontext(processes=procs) as pool:\n",
    "    # Every process received a part of data_dict\n",
    "    results = pool.map(add_derivatives_par, [{key: data_dict[key][k*TIME_STEPS//procs:(k+1)*TIME_STEPS//procs] for key in data_dict.keys()} for k in range(procs)])\n",
    "    \n",
    "data_dict = {}\n",
    "for key in results[0].keys():\n",
    "    data_dict[key] = np.concatenate([results[k][key] for k in range(procs)])\n",
    "    \n",
    "del results\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping into 1D-arrays and converting dict into a DataFrame-object\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = np.reshape(data_dict[key], -1)\n",
    "\n",
    "# vert_layers = np.reshape(vert_layers, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clc (1000423278,)\n",
      "cl_area (1000423278,)\n"
     ]
    }
   ],
   "source": [
    "for key in data_dict.keys():\n",
    "    print(key, data_dict[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000423278"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(data_dict)\n",
    "\n",
    "# Number of samples/rows\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del data_dict\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downsampling the data (minority class: clc = 0)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no nans left\n",
    "assert np.all(np.isnan(df) == False) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ensure that clc != 0 is as large as clc = 0 (which then has 294 Mio samples) and keep the original order intact\n",
    "df_noclc = df.loc[df['clc']==0]\n",
    "print(len(df_noclc))\n",
    "\n",
    "# len(downsample_indices) will be the number of noclc samples that remain\n",
    "downsample_ratio = (len(df) - len(df_noclc))/len(df_noclc)\n",
    "shuffled_indices = np.random.permutation(df_noclc.index)\n",
    "size_noclc = int(len(df_noclc)*downsample_ratio)\n",
    "\n",
    "del df_noclc\n",
    "gc.collect()\n",
    "\n",
    "downsample_indices = shuffled_indices[:size_noclc] \n",
    "\n",
    "# Concatenate df.loc[df[output_var]!=0].index and downsample_indices\n",
    "final_indices = np.concatenate((downsample_indices, df.loc[df['clc']!=0].index))\n",
    "\n",
    "del shuffled_indices, downsample_indices\n",
    "gc.collect()\n",
    "\n",
    "# Sort final_indices so that we can more or less recover the timesteps\n",
    "final_indices = np.sort(final_indices)\n",
    "\n",
    "# Label-based (loc) not positional-based\n",
    "df = df.loc[final_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774757958"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of samples after downsampling\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifies df as well\n",
    "def split_input_output(dataset):\n",
    "    output_clc = dataset['clc']\n",
    "    output_cl_area = dataset['cl_area']\n",
    "    del dataset['clc']\n",
    "    del dataset['cl_area']\n",
    "    return output_clc, output_cl_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_clc, output_cl_area = split_input_output(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "# np.save(output_path + '/cloud_cover_input_dyamond.npy', df)\n",
    "np.save(output_path + '/cloud_cover_output_dyamond.npy', output_clc)\n",
    "# np.save(output_path + '/cloud_area_output_dyamond.npy', output_cl_area)\n",
    "# np.save(output_path + '/samples_vertical_layers_dyamond.npy', vert_layers[df.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK\n",
    "test_output = np.load(output_path + '/cloud_area_output_dyamond.npy')\n",
    "\n",
    "assert np.all(np.abs(test_output - output_cl_area) < 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piece together the npy files containing the derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = ['hus', 'clw', 'cli', 'ta', 'pa', 'zg', 'fr_land', 'U', 'rh', 'ps', 'hus_z', 'hus_zz', 'clw_z', 'clw_zz', 'cli_z',\\\n",
    "                  'cli_zz', 'ta_z', 'ta_zz', 'pa_z', 'pa_zz', 'U_z', 'U_zz', 'rh_z', 'rh_zz']\n",
    "len(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cli = np.load(output_path + '/cloud_cover_input_dyamond_cli_only.npy')\n",
    "input_ta = np.load(output_path  + '/cloud_cover_input_dyamond_ta_only.npy')\n",
    "input_clw = np.load(output_path + '/cloud_cover_input_dyamond_clw_only.npy')\n",
    "input_rh = np.load(output_path  + '/cloud_cover_input_dyamond_rh_only.npy')\n",
    "input_hus = np.load(output_path + '/cloud_cover_input_dyamond_hus_only.npy')\n",
    "input_U = np.load(output_path + '/cloud_cover_input_dyamond_U_only.npy')\n",
    "input_pa = np.load(output_path + '/cloud_cover_input_dyamond_pa_only.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first ten variables should coincide\n",
    "assert np.all(input_cli[:, :10] == input_ta[:, :10])\n",
    "assert np.all(input_cli[:, :10] == input_clw[:, :10])\n",
    "assert np.all(input_cli[:, :10] == input_rh[:, :10])\n",
    "assert np.all(input_cli[:, :10] == input_hus[:, :10])\n",
    "assert np.all(input_cli[:, :10] == input_U[:, :10])\n",
    "assert np.all(input_cli[:, :10] == input_pa[:, :10])\n",
    "\n",
    "# These should be the derivatives\n",
    "assert not np.all(input_cli[:, -2:] == input_ta[:, -2:])\n",
    "assert not np.all(input_cli[:, -2:] == input_clw[:, -2:])\n",
    "assert not np.all(input_cli[:, -2:] == input_rh[:, -2:])\n",
    "assert not np.all(input_cli[:, -2:] == input_hus[:, -2:])\n",
    "assert not np.all(input_cli[:, -2:] == input_U[:, -2:])\n",
    "assert not np.all(input_cli[:, -2:] == input_pa[:, -2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_input = np.concatenate((input_hus, input_clw[:, -2:], input_cli[:, -2:], input_ta[:, -2:], input_pa[:, -2:], input_U[:, -2:], input_rh[:, -2:]), axis=1)\n",
    "np.save(output_path + '/cloud_cover_input_dyamond.npy', final_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (based on the module python3/2022.01)",
   "language": "python",
   "name": "python3_2022_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
